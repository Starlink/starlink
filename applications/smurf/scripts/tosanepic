#!/bin/tcsh

#  Usage:
#     tosanepic <path to raw scuba-2 file>
#
#     If no input file is supplied a default of /star/share/smurf/s4a20091214_00015_0002.sdf is used.

#  Description:
#     Creates a FITS file suitable for use with sanepic from a single SCUBA-2 raw data file.
#     The output FITS file is left in the current directory and has the same base=name as
#     the supplied SCUBA-2 raw file, but with file type ".fits".

#     Also creates a text file ("bolomask") containing the identifiers for the usable bolometers.
#
#     In addition, the down-sampled sampling frequency is needed by sanepic. This can be found by
#     looking in the makemap.log file for the place where it says how many down-sampled
#     samples there are and how long they last.

#     Note - this will need to be changed when makemap can create lon/lat values for whole chunks
#     rather than just individual input files.

if( $# > 0 ) then
   set full = $1
else
   set full = "/star/share/smurf/s4a20091214_00015_0002.sdf"
endif

set file = `basename $full .sdf`

rm -f cont.sdf >& /dev/null
rm -f bmask.log >& /dev/null

#  Get the required time series NDFs by running makemap.
$SMURF_DIR/makemap $full method=iter out=map msg_filter=debug config=^${STARLINK_DIR}/share/smurf/experimental/dimmconfig_export.lis | tee makemap.log

#  Create an HDS container containing a dummy 1 pixel NDF (since ndf2fits
#  container=yes seems to put the first NDF it finds into the primary
#  HDU which is not what we want).
$KAPPA_DIR/maths exp=xa type="_REAL" lbound=1 ubound=1 out=cont.dummy

#  Get a template NDF that is a copy of the cleaned data but with
#  PAD samples trimmed from start and end.
$KAPPA_DIR/nomagic ${file}_con_res_cln tmp1 0
$KAPPA_DIR/qualtobad tmp1 tmp2 PAD
$KAPPA_DIR/ndfcopy tmp2 template trimbad=yes

#  Copy the full TCS_TAI array from JCMTSTATE into a separate 1D NDF.
$HDSTOOLS_DIR/hcreate tai_days NDF
$HDSTOOLS_DIR/hcreate tai_days.data_array ARRAY
$HDSTOOLS_DIR/hcopy ${file}_con_res_cln.more.jcmtstate.tcs_tai tai_days.data_array.data

#  Convert from days to seconds.
$KAPPA_DIR/cmult tai_days 86400 tai_secs

#  Trim the 1D TCS_TAI NDF to match the 3rd (time) axis of the template. Put the result in
#  the HDS container.
$KAPPA_DIR/ndftrace template quiet
set lo = `$KAPPA_DIR/parget LBOUND'(3)' ndftrace`
set hi = `$KAPPA_DIR/parget UBOUND'(3)' ndftrace`
$KAPPA_DIR/ndfcopy tai_secs\($lo\:$hi\) cont.time

#  Find the number of time slices in the trimmed data.
@ ntime = $hi - $lo + 1

#  Extract the quality component from the trimmed cleaned data NDF
$KAPPA_DIR/ndfcopy ${file}_con_res_cln comp=qua out=qua

#  Set up arrays of input NDF names and corresponding names required by sanepic.
#  The cleaned time series data must be first.
set ndfs = `echo "qua ${file}_con_res_cln  ${file}_lon ${file}_lat"`
set names = `echo "mask signal lon lat"`

#  Process each 3D time series NDF
set n = 1
while( $n <= $#ndfs )

#  Get the name of the NDF and the corresponding sanepic name.
   set ndf = $ndfs[$n]
   set name = $names[$n]
   @ n = $n + 1

#  Use the template to trim off the padding from the 3D time series NDFs generated by makemap.
   $KAPPA_DIR/setbound $ndf like=template

# Make the NDF 2D.
   $KAPPA_DIR/reshape $ndf tmp5 shape=\[1280,$ntime\]

# Swap the axes
   $KAPPA_DIR/permaxes tmp5 perm=\[2,1\] out=tmp6

# Use the first file (which should be the quality array) to create a mask of bolometers to use
   if( !( -e bmask.log ) ) then
      $KAPPA_DIR/maths exp='"qif((ia>0),<bad>,0)"' ia=tmp6 out=tmp7
      $KAPPA_DIR/collapse tmp7 axis=p1 wlim=0 out=bmask
      $KAPPA_DIR/look bmask'(,1)' mode=all format=vlist log=bmask.log quiet
      $KAPPA_DIR/stats bmask quiet
      set ngood = `$KAPPA_DIR/parget NUMGOOD stats`

#  Create a Mapping that maps (col,bolo1) for all bolos to (col,bolo2) excluding bad bolometers.
      ./tosanepic_2.py
   endif

#  Replace bad values with zeros.
   $KAPPA_DIR/nomagic tmp6 tmp7 0.0

#  Use this Mapping (mapping.ast) to regrid the array to exclude dead bolometers, storing the new NDF
#  with the sanepic name in an output HDS container file
   $KAPPA_DIR/regrid tmp7 out=cont.$name mapping=mapping.ast method=near \
                     rebin=true lbound=\[1,1\] ubound=\[$ntime,$ngood\]

#  For safety erase everything but the DATA array so that ndf2fits does not copy unrequired things.
   $KAPPA_DIR/erase cont.$name.quality ok >& /dev/null
   $KAPPA_DIR/erase cont.$name.history ok >& /dev/null
   $KAPPA_DIR/erase cont.$name.wcs ok >& /dev/null

end

#  Convert the HDS container into a multi-extension FITS file
$CONVERT_DIR/ndf2fits container=yes in=cont out=\!${file}.fit comp=d

#  Add the channels table
./tosanepic_1.py ${file}.fit

#  Clean up
rm -f bmask.sdf bmask.log cont.sdf mapping.ast qua.sdf ${file}*.sdf tai_*.sdf template.sdf tmp?.sdf map.sdf

