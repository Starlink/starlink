\documentclass{book}
\pagestyle{myheadings}
\newcommand{\stardoccategory}  {Starlink User Note}
\newcommand{\stardocinitials}  {SUN}
\newcommand{\stardocnumber}    {98.6}
\newcommand{\stardocauthors}   {D.J.Allan \& R.J.Vallance}
\newcommand{\stardocdate}      {13 May 1995}
\newcommand{\stardoctitle}     {ASTERIX --- X-ray Data Processing System}

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\renewcommand{\_}{{\tt\char'137}}     % re-centres the underscore
\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{230mm}
\setlength{\topmargin}{-2mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}
\setcounter{tocdepth}{1}

\begin{document}
\thispagestyle{empty}
CCLRC / {\sc Rutherford Appleton Laboratory} \hfill {\bf \stardocname}\\
{\large Particle Physics \& Astronomy Research Council}\\
{\large Starlink Project\\}
{\large \stardoccategory\ \stardocnumber}

\begin{flushright}
\stardocauthors\\
School of Physics and Space Researc, University of Birmingham\\
\stardocdate
\end{flushright}
\vspace{-4mm}
\rule{\textwidth}{0.5mm}
\vspace{5mm}
\begin{center}
{\Large\bf \stardoctitle}
\end{center}
\vspace{5mm}
\setlength{\parskip}{0mm}
\tableofcontents
\setlength{\parskip}{\medskipamount}
\markright{\stardocname}
\chapter{Getting started}
The object of this section is to point the novice ASTERIX user to the
various pieces of documentation (online and offline) that will get them
started on using ASTERIX and to guide them through a few example
commands.

The first thing you should do if you haven't already done so is get a
copy of the User Guide (SUN98). You should read this before you start
trying to do anything else with ASTERIX, so that you are familiar with
ASTERIX terminology - 'event dataset', 'binned dataset' etc.

Next you will need to get hold of some test data. Three files of test
data were provided with this ASTERIX release. You should ascertain the
location of these from your system manager and copy them to your own
work directory. There is one event dataset ({\tt event\_data.sdf}) and two
binned datasets ({\tt time\_series.sdf} and {\tt image.sdf}). You can give your own
copies different names, but the {\tt .sdf} extension must be retained.

Before you start doing things with these datasets it would be worth
reading through the help sections, Shortform commands and
Macro commands to make sure you are totally familiar with the different
sorts of ASTERIX command. You should also read the section on
'Using\_ASTHELP', so that you know, for example, how to get a hardcopy
of the help. You may find it more convenient to get a hardcopy of
this section.

The simplest commands are probably the general HDS ones. If you are
not already in ICL then type {\tt icl} at the shell prompt to load it
and once loaded, type
{\tt asterix} to load ASTERIX command definitions (unless this is done
automatically at your site). Now try this command:

\begin{quote}\begin{verbatim}
ICL> hdir time_series
\end{verbatim}\end{quote}
There will be a slight delay as this section of ASTERIX is loaded and
then you will be presented with a 'directory' listing of the top level
of the dataset. In this case you gave HDIR the parameter it wanted on
the command line. At this point it would be worth looking in the help
section 'User\_interface ADAM\_parameters' to familiarise yourself with
the other ways parameters may be handled. Having done that try giving
the HDIR command without any parameters. First you should notice that
you get much quicker response because that part of ASTERIX is already
active. Second, you will see that you are now being prompted for a
parameter and that a value is being suggested - the same value you
gave last time. Press \verb+<RETURN>+ and it will take that value by default.
Now give the command:

\begin{quote}\begin{verbatim}
ICL> hdir \
\end{verbatim}\end{quote}
You will see that this time it doesn't prompt, but instead takes the
default directly. Now try:

\begin{quote}\begin{verbatim}
ICL> hdir prompt
\end{verbatim}\end{quote}
It will prompt for the object name. Give it a different one this time -
{\tt event\_data}. Now you'll find it's asking for another parameter. This
parameter gives you the option of sending the output somewhere else -
for now just take the default value. This parameter is normally hidden
and just takes its default. The only way to make these parameters
visible is to use the {\tt PROMPT} keyword. Give the HDIR command again by
itself. This time when it prompts, press the \verb+<TAB>+ key. You will see
that the default it offers you is transfered to the input buffer and
that you are able to edit it. Just add '.X\_RAW' onto the end to have
a look at one of the event lists. Finally, before we give HDIR a rest,
invoke it again without parameters and give {\tt !!} in response to the
prompt. The command will abort.

You have been guided through the use of just one command so far. It
would be appropriate now to explore some other commands and also
become familiar with the help given on commands. Have a look at the
help on HDIR ({\tt ICL> asthelp hdir}) and see how the way it's presented
fits with what you now know about the command. Explore and try some
of the other commands in the same group.

The general HDS commands that you have been exploring have no
knowledge about the datasets they are dealing with and make no
assumptions about components they find there. ASTERIX does have
other display commands, though, which do know about the standard
structures. Try the following:

\begin{quote}\begin{verbatim}
ICL> header time_series

ICL> binlist time_series slice=1:20

ICL> hist time_series
\end{verbatim}\end{quote}
These commands will require a different part of ASTERIX to be loaded
so there will be a slight delay before the first one responds. This
first command will display the contents of the standard header and
tell you the origin of the data. The second will actually list a part
of the data in tabular form and the final one will give you its
processing history.

The next thing you will probably want to do is plot out the data. First
type:

\begin{quote}\begin{verbatim}
ICL> dlist
\end{verbatim}\end{quote}
to get a list of available graphics device names known to your system.
Again this is a new part of ASTERIX, so there will be a slight delay.
Having selected the appropriate device, type:

\begin{quote}\begin{verbatim}
ICL> gdraw time_series device-name
\end{verbatim}\end{quote}
This will give you a default plot of this dataset. The graphics sub-system
is quite large and comprehensive and has its own extensive help section
which you should explore separately.

The time series plot probably took some time to draw because it is quite
large. It also has some duff data at the end where the instrument slewed
off-target. You could use GRAFIX to cut down the size of the graph
window, but GDRAW would still spend time invisibly plotting the points
outside this window. Instead it would be better to subset out the
section of data you want to look at. Type:

\begin{quote}\begin{verbatim}
ICL> binsubset time_series
\end{verbatim}\end{quote}
it will prompt for an output dataset, for which you can supply a name.
Then it will ask for the range of axis values you want to keep, and
will give you as a default the full range. Press \verb+<TAB>+ and then edit
the second number to something much smaller. You can then try drawing
this smaller dataset.

There are several commands in ASTERIX where a range or a list of
values are required to be input. Such ranges and lists are input as a
character string which is parsed to extract the numeric values. This
allows a more comprehensive syntax to be used. You have seen an example
in the BINSUBSET command.

You should by now be getting a feel for ASTERIX and the ADAM parameter
system that it uses. By looking at the list of commands in the help
topic 'Command\_summary' or in the Appendix to the user guide, you should
select some groups of commands that you are likely to want to use, explore
the help on them and try them out with the test data.

After hacking the test data about some more you are probably ready to
take some real data right through the system. The list of documents
includes reference guides for the instruments supported by
ASTERIX. You should consult these.

\chapter{Command summary}
This is a list of commands available in ASTERIX. These are loosely
grouped by their function. Shortform and macro commands are shown
with suffixes '+' and '*' respectively.


\begin{enumerate}
\item Instrument Interfaces

\begin{enumerate}
\item ROSAT commands (PSPC and HRI specific)

\begin{description}
\item[SHOWXRT]
Show observation information stored in header file
\item[XRTCONV]
Converts ROSAT FITS datasets into HDS format
\item[XRTINDEX]
Extracts header information from ROSAT FITS files
\item[XRTHOT]
Extracts hotspot/deadspot information from HRI datafiles
\item[XRTCORR]
Corrects XRT datafiles for vignetting effects etc..
\item[XRTHK]
Produce list of times based on housekeeping parameters
\item[XSORT]
Sorts ROSAT PSPC and HRI raw data into ASTERIX datasets
\item[XRTRESP]
Creates an energy response structure within a spectral NDF
\item[XRTSUB]
Subtracts an XRT background from an XRT source file
\item[XRTORB]
Create orbital information file for use in BARYCORR
\item[XSPOKES]
Defines the ribs region on a PSPC image
\item[XRTBCKBOX*]
Sorts XRT data using a radius optimisation algorithm
\item[XRTBOX*]
Allows a image region to be selected and used for sorting
\item[XRTEXPMAP]
Constructs exposure maps by dithering background images
\item[XPSSCORR]
Converts PSS XRT source counts into a flux
\item[XRAD]
Calculates the PSF radius for a PSPC source
\item[XRAD90]
Calculates the 90\% PSF radius for a PSPC source
\end{description}
\item ROSAT commands (WFC Pointed Phase specific). Some of these are
defined by typing {\tt WFCSTART} from the command line.

\begin{description}
\item[WFCAUTO*]
Provides automatic analysis of one or more RWODs
(Rosat WFC Observation Datasets)
\item[WFCBACK]
Produce background subtracted image given raw image
\item[WFCBOP*]
Performs background threshold optimisation for a WFC observation
\item[WFCCOUNTS*]
Produces statistics on a source seen in a WFC image.
\item[WFCDBM]
Maintain a database of WFC observations
\item[WFCDISK*]
Copies the contents of an RWOD (Rosat WFC Observation Dataset) to disk.
\item[WFCEXP]
Exposure correct a WFC binned dataset
\item[WFCHK]
Create time series of housekeeping parameters
\item[WFCIMAGE*]
Creates filter specific WFC counts, flux and exposure images.
\item[WFCLIGHT*]
Creates a background subtracted, exposure corrected,
light curve for a source in the current image (interactive
use only)
\item[WFCPOSX]
Get the exposure at a point in the WFC FOV
\item[WFCSLOTS]
Selects a set of filter defined observation slots
\item[WFCSLMRG]
Merges 2 sets of time slots together
\item[WFCSORT]
Sorts events into ASTERIX binned or event datasets
\item[WFCSPC*]
Creates a background subtracted, exposure corrected,
spectral dataset for a source in the current image
(interactive use only)
\item[WFCSPEC]
Creates a WFC spectral file
\item[WFCSUB*]
Subtracts a background light curve from a source light
curve. Both input light curves must be exposure corrected
before use.
\end{description}
\end{enumerate}
\item Event Processing Commands - these all work on EVENT datasets.

\begin{description}
\item[EVBIN]
Creates a binned dataset from an event dataset.
\item[EVCSUBSET]
Extracts a circular or annular region from an event
dataset. Output is an event dataset.
\item[(EVLIST]
Displays the DATA\_ARRAY component of all lists in an
event dataset.)
\item[EVMERGE]
Merges two or more event datasets.
\item[EVPHASE]
Create a phase list given an ephemeris
\item[EVPOLAR]
Polar binning of event dataset
\item[EVSIM]
Creates a simulated event dataset
\item[EVSORT]
Reorder events on basis of a list's value.
\item[EVSUBSET]
Linearly extracts a subset from an event dataset.
\end{description}
\item Binned Dataset Commands

\begin{description}
\item[ASMOOTH]
Smooth a dataset with an adaptive filter
\item[AXCENTROID]
Centroid an dataset wrt to one axis
\item[AXCONV]
Expands spaced axes for use in KAPPA etc.
\item[AXFLIP]
Reverses directions of any or all of dataset's axes
\item[AXORDER]
Re-order axes of a dataset
\item[AXSHOW]
Display list of axis attributes for binned dataset
\item[BINMERGE]
Merges up to ten datasets.
\item[BINSUBSET]
Subsets a binned dataset.
\item[CORRECT]
Applies exposure and dead time correction
\item[ENMAP]
Creates 2D image of mean energy of photons in 3d dataset
\item[INTERP]
Reconstitutes bad pixels by spline interpolation
\item[MEANDAT]
Finds mean or sum of up to 20 datasets
\item[POLYFIT]
Fits 1-d polynomial(s) to a dataset.
\item[PROJECT]
Project data along one axis.
\item[RADOPT]
Finds optimum SNR radius for a given source
\item[RATIO]
Gives the ratio of two bands on any axis in an n-d array.
\item[REBIN]
Rebins a binned dataset.
\item[SIGNIF]
Changes an input dataset to its weighted significance.
\item[SCATTERGRAM]
Produce scatter plot of one array versus another
\item[SMOOTH]
Smooths an n-d datafile with a user-selectable mask.
\item[SPLINEFIT]
Finds splines to 1D or 2D data
\item[SYSERR]
Adds a constant percentage to the variance of each point.
\item[VALIDATE]
Basic validation of binned dataset
\end{description}
\item Conversion Commands

\begin{description}
\item[ASTCONV]
Converts between old and new ASTERIX binned datasets.
\item[AST2QDP]
Convert a 1D dataset to QDP format
\item[ASTQDP]
Invoke AST2QDP and then QDP on the file created
\item[AST2XSP]
Convert spectrum from Asterix to Xspec format
\item[(EVBIN]
Creates a binned dataset from an event dataset.)
\item[EXPORT]
Outputs one or more datasets to text file
\item[HDS2TEXT]
Convert HDS objects to text (opposite of TEXT2HDS)
\item[IMPORT]
Reads a text file into an ASTERIX binned dataset
\item[TEXT2HDS]
Convert text file to HDS
\item[FITS2HDS]
Convert FITS file to HDS
\end{description}
\item Display Commands

\begin{description}
\item[ASTDEFS]
List ASTERIX customizable parameters
\item[AXSHOW]
Display list of axis attributes for binned dataset
\item[BINLIST]
Displays a 1D binned dataset.
\item[EVLIST]
Displays the DATA\_ARRAY component of all lists in an
event dataset.
\item[HEADER]
Displays header and processing information in a dataset
\item[HIST]
Displays history of a dataset.
\item[SSDUMP]
Displays contents of source search results file
\end{description}
\item Math Commands

\begin{description}
\item[ARITHMETIC]
Performs basic arithmetic (+,-,/,*) on two data objects.
\item[ADD+]
Invokes ARITHMETIC in + mode
\item[SUBTRACT+]
Invokes ARITHMETIC in - mode
\item[MULTIPLY+]
Invokes ARITHMETIC in * mode
\item[DIVIDE+]
Invokes ARITHMETIC in / mode
\item[OPERATE]
Performs operations e.g. LOG10 on a dataset.
\end{description}
\item Time Series Analysis Commands

\begin{enumerate}
\item Non-interactive analysis commands

\begin{description}
\item[ACF]
Computes a 1d auto-correlation function
\item[BARYCORR]
Applies barycentric correction to binned and event datsets
\item[CROSSCORR]
Cross correlates two 1D series.
\item[CROSSPEC]
Computes the cross spectrum of two 1D datasets
\item[DIFDAT]
Differences adjacent bins to reduce low frequency noise
\item[DYNAMICAL]
Finds power spectrum of successive segments of a time series
\item[EVPHASE]
Add a phase list to an event dataset
\item[FOLDBIN]
Folds a time-series into phase bins at a given period
\item[PHASE]
Converts a time-series into a phase series.
\item[POWER]
Finds the power spectrum of a full unweighted 1D dataset
\item[SINFIT]
Finds a periodogram of irregularly spaced 1D data.
\item[TIMSIM]
Generates simulated time series
\item[VARTEST]
Test a source/background time series pair for variability
\end{description}
\item Interactive time series editor

\begin{description}
\item[TLOAD]
Load a time series into the system for plotting and tinkering with.
\item[TCLOSE]
Release the time series and close down the system.
\item[TPLOT]
Plot the time series
\item[TCHOP]
Chop up the display into separate plots of contiguous data.
\item[TWHOLE]
Use the whole time series including gaps.
\item[TSLICE]
Select a time slice to keep or discard.
\item[TZAP]
Remove individual points.
\item[TSELECT]
Select a subset of time segments for display.
\item[TSAVE]
Save the data in their current status.
\item[TONOFF]
Write out on-off times in MJD for currently selected good data.
\end{description}
\end{enumerate}
\item Image Processing Commands

\begin{enumerate}
\item Interactive

\begin{description}
\item[ISTART]
Start up image processing system
\item[INEW]
Load new image
\item[ICLOSE]
Close down image processing system
\item[IDISPLAY]
Display current image
\item[IBOX]
Define rectangular section of image
\item[IWHOLE]
Select whole image
\item[IZOOM*]
Zoom in on section of image
\item[IUNZOOM*]
Redisplay whole image
\item[ISCALE]
Rescale image
\item[ICONTOUR]
Contour current image
\item[IGRID]
Puts grid over image in specified coords
\item[ICOLOUR]
Change colour table
\item[IPLOT]
Display current 1D plot
\item[ILIMITS]
Changes axis limits on 1D plot
\item[ISTYLE]
Set plotting style of 1D plot
\item[ITITLE]
Change title of current image or plot
\item[IZONES]
Changes zones on display surface
\item[ICLEAR]
Clear current plotting zone
\item[ISTATS]
Gives basic statistics on pre-selected region of image
\item[IBOXSTATS+]
Select region and give statistics
\item[IMARK]
Marks sources on image
\item[IREMOVE]
Remove sources from image
\item[IPEAKS]
Find and mark peaks above threshold
\item[INOISE]
Adds gaussian noise to image
\item[IZAP]
Removes individual pixels from image
\item[IPATCH]
Patches bad quality pixels in image
\item[IPOLYFILL]
Fill data area defined by user selected polygon
\item[IPOLYFILLQ+]
Fill quality area defined by user selected polygon
\item[IBLUR]
Blur image by Gaussian or Box filter
\item[IBROWSE]
Look at data values around selected point
\item[IPEEKV+]
Look at variances
\item[IPEEKQ+]
Look at quality
\item[IHIST]
Histogram pixels inside current box
\item[IRADIAL]
Produces radial plot
\item[IPSF]
Puts PSF profile on 1D plot
\item[IAZIMUTH]
Produces azimuthal distribution
\item[ISLICE]
Takes a 1-d slice from an image
\item[IPREVIOUS]
Go back to previous image
\item[ISAVE]
Save current image to file
\item[ISAVE1D]
Save current 1D data to file
\item[ICURRENT]
Get current positions etc
\end{description}
\item Non-interactive

\begin{description}
\item[IMOSAIC]
Merges several non-congruent images
\item[IPOLAR]
Produce a polar surface brightness profile of an image
\item[IXCONV]
2D convolution of a pair of images
\item[IDTODMS*]
Convert decimal degrees to dd:mm:ss
\item[IDMSTOD*]
Convert dd:mm:ss to decimal degrees
\item[IDTOHMS*]
Convert decimal degrees to hh:mm:ss
\item[IHMSTOD*]
Convert hh:mm:ss to decimal degrees
\end{description}
\end{enumerate}
\item Parameter Commands

\begin{description}
\item[GLOBAL]
Displays the current values of all global parameters.
\end{description}
\item Spectral analysis commands

\begin{description}
\item[FREEZE]
Freezes parameter(s) in spectral model.
\item[(IGNORE+]
Allows spectral channels to be excluded from fitting.)
\item[(RESTORE+]
Reinstates spectral channels for fitting.)
\item[SBG]
Associate background file with a spectrum
\item[SBIN]
Rebin spectrum to even out counts in each bin.
\item[SDATA]
Define datasets to be fitted.
\item[SEDIT]
Interactively edit a spectral model description.
\item[SERROR]
Evaluates confidence region for spectral model parameters.
\item[SFIT]
Fits spectral model to one or more datasets.
\item[SFLUX]
Evaluates flux from defined model over energy band.
\item[SGRID]
Construct n-D grid of fit statistic/reoptimised
parameters against grid parameters
\item[SMODEL]
Allows user definition of muti-component spectral model.
\item[SPLOT]
Plots data, fits and residuals.
\item[SSHOW]
Display current model parameters to any ascii device
\item[THAW]
Frees spectral model parameter (after FREEZEing).
\end{description}
\item Statistical analysis commands

\begin{description}
\item[BINSUM]
Integrates a dataset - use for cumulative distributions
\item[COMPARE]
Compares two datafiles or a file and a model.
\item[FREQUENCY]
Produces a histogram of values in a data array.
\item[KSTAT]
Calculates the Kendall K statistic for a 1-d dataset.
\item[STATISTIX]
Finds mean,standard deviation etc.. of a data array
\end{description}
\item Quality processing commands

\begin{description}
\item[IGNORE+]
Invokes QUALITY in IGNORE mode: sets temporary bad
quality bit
\item[QUALITY]
General quality manipulation application.
\item[RESTORE+]
Invokes QUALITY in RESTORE mode: clears temporary bad
quality bit. (Only availible in ICL)
\item[SETQUAL+]
Invokes QUALITY in SET mode: sets quality to specified
value.
\item[MAGIC]
Sets magic values
\item[MASK]
Sets quality mask value
\end{description}
\item HDS editor commands

\begin{description}
\item[HCOPY]
Recursively copies an HDS object
\item[HCREATE]
Creates an HDS object in a file
\item[HDELETE]
Deletes an HDS object
\item[HDIR]
Displays the components of an HDS file or structure
\item[HDISPLAY]
Displays the contents of an HDS primitive
\item[HFILL]
Fills a primitive object with one value
\item[HGET]
Get HDS object attributes to environment
\item[HMODIFY]
Change the value(s) in an existing object.
\item[HREAD]
Reads from an ASCII (or binary) file to an HDS object
\item[HRENAME]
Renames an HDS object
\item[HRESET]
Set values in HDS primitive to 'undefined'
\item[HRESHAPE]
Alters size of dimensions in an HDS primitive array
\item[HRETYPE]
Change type of a structured object
\item[HTAB]
Simultaneously displays several HDS primitive vectors
\item[HTRACE]
Displays full HDS tree structure
\item[HWRITE]
Writes an HDS primitive to an ASCII (or binary) file
\end{description}
\item Source Searching Commands

\begin{description}
\item[BSUB]
Background subtraction program
\item[COVERAGE]
Convert PSS sensivity map to coverage function
\item[CREPSF]
Create dataset containing psf description
\item[EVSIM]
Create simulated event datasets
\item[IMSIM]
Create simulated image datasets
\item[PSS]
Search a binned dataset for sources
\item[SPCONVOL]
Convolve a spatial response with a source model
\item[SPRESP]
Attach a spatial response to a dataset
\item[SSCARIN]
Export source search results to binary SCAR catalogue
\item[SSDUMP]
Dumps source search results to an ascii file
\item[SSGET]
Extracts source search results data items to environment
\item[SSMERGE]
Merge two or more source results together
\end{description}
\item GRAFIX display commands

\begin{description}
\item[DOPEN]
Open a device for interactive plotting by any subsystem.
\item[DCLOSE]
Force closure of current device.
\item[DERASE]
Clear the display surface.
\item[DLIST]
List valid device names.
\item[DSHOW]
Show information about current device.
\item[GDRAW]
Draw 1D or 2D data, or plot data from a multiple dataset.
\item[GMULTI]
Creates and manipulates multiple datasets.
\item[GSET]
Sets plotting attributes for a dataset or an interactive sub-system.
\item[GLOAD]
Load and hold open a dataset for interactive plotting.
\item[GCLOSE]
Release a loaded dataset.
\item[G\_POS]
Specifiy position of plotting window for primitive plotting.
\item[G\_WINDOW]
Define world coordinate limits of plotting window.
\item[G\_AXES]
Plot axes.
\item[G\_MOVE]
Move to a given position without plotting.
\item[G\_PLOT]
Plot from current position to specified position.
\item[G\_LINE]
Draw a line through a given series of points.
\item[G\_MARK]
Plot markers at specified points.
\item[G\_LABEL]
Put labels on plot.
\item[G\_TEXT]
Put text at arbitrary position.
\item[G\_CURS]
Return cursor position in world coordinates.
\end{description}
\end{enumerate}
\chapter{Shortform commands}
Many commands have switches or options which determine the way they
behave. These switches are parameters entered by keyword on the
command line. This can sometimes make the overall command quite
long, so shorter forms of popular combinations have been
predefined using the ICL 'defstring' facility. An example is:

\begin{quote}\begin{verbatim}
ICL> defstring div(ide) arithmetic oper="/"
\end{verbatim}\end{quote}
When the command DIVIDE (may be abbreviated to DIV) is given, ICL
expands this into {\tt arithmetic oper="/"} and thereby invokes
ARITHMETIC with the option OPER="/". Any other quantities entered
on the command will be appended to the expanded string before it
is interpreted. So a full example might be:

\begin{quote}\begin{verbatim}
ICL> divide img 2.0 newimg
\end{verbatim}\end{quote}
which expands to {\tt arithmetic oper="/" img 2.0 newimg}.

Where shortform commands have been defined a section is included
in the help for the full command. Shortform commands also have
their own entry in the relevant command group and to distinguish
them from primitive ASTERIX commands they have the suffix '+'.
You may of course define your own and in this way ASTERIX can be
customised to suit the way in which you normally use it.

\chapter{Macro commands}
The ASTERIX package contains several higher level commands which
combine primitive commands in useful sequences. These are
implemented as ICL procedures. A simple example is the ZOOM
command in GRAFIX. This combines the primitive commands CURSOR
and AXES to change the displayed limits of a plot interactively.
Some macro commands have parameters and it is important to realise
that these are parameters to ICL procedures, not ADAM parameters.
They must {\em always} be supplied on the command line. Details of the
parameters required by a macro command are given in the help for
that command. Macro commands are distinguished in the help system
by having the suffix '*'.

\chapter{User interface}
This section describes various aspects of the way ASTERIX
interfaces with the user.

\section{ADAM parameters}
ASTERIX makes extensive use of the ADAM parameter system for
inputting values to applications and to a lesser extent for
passing values between applications. The user interface to the
parameter system is the command line and prompting. Parameters
may take their values from a number sources not all of which are
always visible to the user.

\subsection{Command line input}
Command line input can be used to specify the value of parameters
required by an application. These always override values from any
other source, for example hidden default values in the interface
file.

In addition, there is a special keyword PROMPT which may be
placed on the command line. This causes {\em all} parameters used by
the application, including hidden defaults, to be prompted for.

Parameters are entered on the command line by POSITION or by
KEYWORD with the following rules and syntax:


\begin{enumerate}
\item Logical values: {\tt KEYWORD} sets value to true,
{\tt NOKEYWORD} sets value to false

\item Other values: KEYWORD=value,
or just value if entered by POSITION.

\item Keywords may be placed anywhere on the command line
and are not counted when assessing the position of a
parameter.

\item Non-scalar values must be enclosed in [ ] eg [1 2 3 4]

\item Character strings need only be put in quotes when
they contain spaces, when case needs to be preserved, or
where there may be ambiguity with an HDS object name.

\item An HDS object name may be preceded by @ if there is
ambiguity but this is not normally needed.

\end{enumerate}
\subsection{Default values}
Many parameters are given default values. Sometimes these are
picked up automatically unless overridden on the command line.
Alternatively the user may be presented with the default in the
prompt line and may confirm it by typing \verb+<RETURN>+ or enter
another value.

Automatic default values and whether an application uses globals
can be found from the appropriate ASTHELP module, by looking at
the 'Parameters' subtopic.

\subsection{Global parameters}
Some parameters are stored in global parameters which are
accessible to all applications. These are used for example to
store the current analysis dataset. Applications will normally
pick this up and suggest it as a default. Some applications may
still use this as a hidden default, which needs to be overridden
on the command line.

Current values of all global parameters may be displayed by
typing the command

\begin{quote}\begin{verbatim}
ICL> global
\end{verbatim}\end{quote}
The value of global parameters can be set using the ICL built-in
command SETGLOBAL.

\subsection{Parameters Help Subtopic}
All commands which require user defined input have a Parameters
subtopic as a part of the ASTHELP on the command. This informs
the user of the various properties of each parameter. Along with
a description of what the parameter is for, the KEYWORD, and
command line POSITION are given as is the FORTRAN data type of
the expected response (eg REAL, INTEGER, CHARACTER, etc...) as a
guide to the type of input required from the user.

Some parameters have been given 'hidden defaults' which will be
picked up automatically by the parameter system, without
prompting the user for confirmation. The value of these hidden
defaults is also given in the parameters help subtopic.

\section{Lists and ranges}
Some applications require the user to specify various ranges,
BINSUBSET is an obvious example; EVBIN when irregularly binning
an axis is another.

Responses to these prompts should be made in the standard ASTERIX
way, described here.

Basically, the user types in the lower and upper bounds for each
range required. There are, however, special characters and syntax
rules to bear in mind.

\begin{itemize}
\item Delimiters. There are 3 basic delimiters:

\begin{enumerate}
\item Between the lower and upper bounds of a given range,
this is represented by a colon ':'
e.g. 10:20
i.e. lower bound = 10, upper bound = 20
\item Between two separate ranges,
this is represented by {\em either} a semi-colon ';' {\em or} by a
space ' '
e.g. 10:20 20:30 or 10:20;20:30
\item Between ranges for different dimensions,
this is represented by a comma ','
(Not many applications use this facility.)
e.g. 10:20 20:30,5:6 11:12
\end{enumerate}
\item Special Characters
There are 2 further characters which have special meaning:


\begin{enumerate}
\item The asterisk '*' may be used to specify the extreme
values. Thus if it is used as a lower bound it represents
the minimum value of the range, whereas if used as the
upper bound it represents the maximum value of the range.
e.g. If the maximum value is 1000 and the minimum value
is 0, then:
*:200 300:400 500:*
is equivalent to
0:200;300:400;500:1000
and to
0:200 300:400 500:1000
\item The ampersand '\&' may be used to specify that the input
will continue. Place it at the end of a line of input and
type the return key. You will be prompted to continue
your input.
e.g.
\begin{quote}\begin{verbatim}
       Enter ranges > 10:20 30:40 & <RETURN>
       Continuation > 50:60 <RETURN>
       \end{verbatim}\end{quote}
Is the same as:
\begin{quote}\begin{verbatim}
       Enter Ranges > 10:20;30:40;50:60 <RETURN>
       \end{verbatim}\end{quote}
\end{enumerate}
\item Other features.
There are three other features to help you:

\begin{enumerate}
\item All extra spaces are ignored, eg.
\begin{quote}\begin{verbatim}
        10 : 20   20 : 30; 30:40
        \end{verbatim}\end{quote}
Is the same as:
\begin{quote}\begin{verbatim}
        10:20;20:30;30:40
        \end{verbatim}\end{quote}
\item Implicit lower bounds. If no lower bound is specified,
then it will be assumed to be the same as the previous
upper bound, eg.

\begin{quote}\begin{verbatim}
        10:20:30:40:50 70:80
        \end{verbatim}\end{quote}
Is the same as:
\begin{quote}\begin{verbatim}
        10:20 20:30 30:40 40:50 70:80
        \end{verbatim}\end{quote}
\item Implicit upper bounds. If no upper bound is specified,
then it will be assumed to be the same as the lower
bound.
e.g.
10 20:30 40
Is the same as:
10:10 20:30 40:40
\end{enumerate}
\end{itemize}
Notes


\begin{enumerate}
\item These range specifications are read in as a CHARACTER
string. You should {\em not}, however, have to enclose them in
single speech marks.
\item When {\em binning} lower bounds are inclusive, but upper
bounds are exclusive, thus consecutive ranges are OK.
The use of implicit upper bounds would, however, have no
meaning in this context!
\end{enumerate}
\section{PSF System}
The psf system is used by Asterix commands which have to access
PSF information in an instrument independent manner. Generally
the user will have a dataset produced by an instrument interface,
such as the EXOSAT or XRT interface, and this will direct the psf
system to the appropriate information.

The psf system displays a menu of options available, controllable
using the PSF prompt. For example,

\begin{quote}\begin{verbatim}
ICL > PSS IMAGE ...

PSF system options :

  ANAL         ASCA          EXOLE         RESPFILE      PWFC
  TABULAR      WFC           XRT_HRI       XRT_PSPC

  CSPEC(psf/model,spec[,nbin])
  POLAR(psf,rbin[,abin])       RECT(psf,xbin[,ybin])

PSF - Choose PSF to use for source model /'ANAL'/ > xrt_pspc
\end{verbatim}\end{quote}
The menu is in two parts - the first lists those psfs which can
be invoked by name (ANAL to XRT\_PSPC) - the second shows allowed
psf model specifications. The latter are used with applications
which psf require a spatially varying psf, or one with energy
dependency.

\subsection{ANALytic psfs}
The ANAL option allows selection of an analytic model. The names of
the option required is entered using the MASK parameter - those
currently supported are,


\begin{description}
\item[TOPHAT]
Value constant out to a specified radius
\item[GAUSSIAN]
Specified by the FWHM of the gaussian
\item[TRIANGLE]
Defined by the full width at zero response
\item[FLAT\_TRI]
Defined by the full width at zero response and the
full width at half-maximum.
\item[KING]
King profile, specified by core radius and index
\item[LORENTZ]
Specified by half-width at half maximum
\end{description}
The user is then prompted for the exact data required to specify the
shape of the profile using the AUX parameter - the prompt for this
parameter changes with the MASK selected. All spatial information
should be specified in terms of axis units.

\subsection{Documentation}
A guide to using the the psf system in conjunction with ASTERIX
source searching applications can be found in USER\_004 in the
ASTERIX documentation directory \$AST\_DOCS (UNIX). A programmers
guide can be found in PROG\_011 in the same directory.

\subsection{Instrument psfs}
These PSF routines are provided for datasets from a specific
instrument. The name of the routine is the same as the instrument
detector combination. Those currently available are:


\begin{description}
\item[ASCA]
The ASCA SIS and GIS
\item[EXOLE]
The EXOSAT LE
\item[PWFC*]
The WFC in pointed mode
\item[WFC]
The WFC in survey mode
\item[XRT\_HRI]
The ROSAT XRT, HRI detector
\item[XRT\_PSPC*!]
The ROSAT XRT, PSPC 1 and 2 detectors
\end{description}
* These routines vary with position across the field of view

! These routines support energy modelling using the CSPEC model

\subsubsection{ASCA}
The ASCA SIS psf is taken from a manual tracing of Figure 5.3c in the
European AO. The GIS psf is that same psf convolved with a Gaussian
of $FWHM = 1/2 * sqrt(5.9/E)$ arcmin.

\subsubsection{EXOLE}
This routine models the on-axis PSF of the EXOSAT LE with an
analytic function. The data depends neither on time nor energy.
No further information is required by this psf routine from the
user.

\subsubsection{PWFC and WFC}
These routines interrogate the online calibration database for
the WFC. This is pointed to by a logical name CAL\_WFC\_MASTER.
No further information is required by this psf routine from the
user.

Filter, detector id and other information are extracted from a
dataset to correctly index the calibration database.

\subsubsection{XRT PSPC}
Four alternative psfs are supplied for the XRT PSPC - the user
selects them by name using the MASK prompt. The options are,

\begin{description}
\item[VARPROFILE]
An analytic fit to enclosed psf energy radii distributed to
US observers. This model varies with both off-axis angle and
energy and should be regarded as the current best model for
the PSPC. However, the model is relatively slow to calculate,
and use of the spatial modelling POLAR option is recommended.
\item[VARGAUSS]
A gaussian fit to the combined telescope and detector
response. The width of the gaussian is a function of
both off-axis angle and energy. This model has the merit of
being very fast even when used in a spatially varying manner.
\item[ONAXIS\_2]
A 2 component fit to the energy integrated on-axis psf
(derived by Jon Mittaz).
\item[ONAXIS\_3]
The definitive 3-component fit to the on-axis response of the
PSPC, as defined in the OGIP Calibration Memo CAL/ROS/92-001,
due to Turner and George.

The model is a combination of three, physically well understood
terms,


\begin{enumerate}
\item A gaussian for the intrinsic PSPC resolution due to the
inherent statistics of the primary electron generation.
Theoretically the gaussian Sigma is proportional to
1/SQRT(Energy)
\item An exponential function due to the finite penetration
depth of the X-rays in the counter gas combined with the
8.5 degree cone angle. The PSPC is focussed for 1 keV; the
'chromatic aberration' is largest for large energies
\item A Lorentzian function for the mirror scattering which breaks
into a different power law slope at larger energies.
Theoretically the scattering fraction should increase like
the square of the energy, if the grazing angle remains
constant. Due to the diffraction laws, the shape parameters
should be proporional to 1/Energy.
\end{enumerate}
In principle these three components should be folded with each
other, however, their angular domains are reasonably well
separated that a simple addition is accurate enough. The
detailed PSF parameters and their energy dependence have been
determined using the PANTER telescope calibration data of both
PSPC-A and PSPC-C at the monochromatic energies 0.28, 0.93,
1.49 and 1.70 keV. At lower pulse heights than channel 15
(0.15 keV) additional 'ghost images' appear in the PSPC for
which no analytical fit is possible. These events should be
avoided as far as possible in PSF modelling.

\end{description}
The XRT\_PSPC option prompts the user for a mean photon energy
to take account of the energy variation. Unless very small
patches of an image are being analysed, it is recommended that
this option is used in conjunction with a POLAR psf model.

\subsubsection{XRT HRI}
The HRI model is an analytic fit to the in-flight reponse,
performed by MPE/GSFC/SAO. The functional form is,

\begin{quote}\begin{verbatim}
PSF = A1*EXP(-0.5*(R/S1)**2) + A2*EXP(-0.5*(R/S2)**2) + A3*EXP(-R/S3)
\end{verbatim}\end{quote}
where the radial distance R from the psf centre is in arcseconds.
The constants A and S have values,
\begin{quote}\begin{verbatim}
   A1 = 0.9638       S1 = 2.1858"
   A2 = 0.1798       S2 = 4.0419"
   A3 = 0.001168     S3 = 31.69"
\end{verbatim}\end{quote}
This model provides a reasonable description of the psf out to
radii of 100", but does not model the azimuthal asymmetry
present within R12". The model will be updated when this effect
can be parameterised.

This model does not vary with position in the field of view.

\subsection{RESPFILE option}
The RESPFILE option is the one which deals with spatial respones
created by SPRESP. If the default offered at the PSF prompt
is 'RESPFILE' this means that the dataset has an attached spatial
response. Accepting this default means that the psf system will
use the information in the response - supplying an alternative
ignores the response.

If the RESPFILE option is selected for a dataset which has no
energy axis, but the response does have a defined energy axis,
the the AUX parameter is used to select an energy value. This
situation might arise where you have attached a response to a
spectral image, but have then PROJECTed this down to a 2D
image to do some (for example) image processing.

\subsection{TABULAR psfs}
This option reads in data from a 2D binned dataset written by
the CREPSF application.

\subsection{Model psfs}
The 3 psf model options are CSPEC,POLAR and RECT. The first allows
the spectral properties of a source to fed in to the construction
of the psf, the latter two control the granularity of psf access in
the spatial dimensions.

\subsubsection{Energy Models}
The CSPEC option requires a counts spectrum in addition to any
other psf option, ie. a name or a spatial model. The psf returned
to the application is a sum of the psf as a function of energy
weighted by the number of counts in each spectral bin. Thus, the
user may tailor a psf to a specific source to achieve more
accurate parameterisation or increase in sensitivity.

\subsubsection{Spatial Models}
Spatial models are useful when using programs which are performing
intensive access of the psf system. The psf system intercepts an
application's psf request and selects that model psf in which the
requested position lies. The effect is to access a psf in the model
bin at most once, which has enormous savings in cpu time. Model
grid positions which are never accessed do not cause psf's to be
calculated, saving more time.

The psf model grid may be specified in two ways - a POLAR grid
and a RECTangular grid. The syntax for each specification is
shown in the subtopics below.

The selection of an appropriate grid inevitably requires some
knowledge of the instrument being modelled. The spatially
varying psfs is the system are,


\begin{description}
\item[PWFC]
The WFC spatial reponse is effectively constant
within a degree off-axis, degrading by a factor
of 2 at 1.6 degrees and by 3 at the edge of the
field ($\sim$ 2.5 degrees). The psf also varies
azimuthally. A suitable model might be,
{\tt POLAR(PWFC,30:60:70:80:100:120:140:160,8)}
giving better sampling where the psf deteriorates
most rapidly.
\item[XRT\_PSPC]
The model used is radially dependent only. The
specification {\tt POLAR(XRT\_PSPC,0.02)}
gives radial bins about an arcminute wide.
The inaccuracies introduced by this approximation
are much smaller than the errors due simply to
uncertainties in the analytic form of the psf.
\end{description}
\subsubsection{CSPEC model}
The CSPEC option constructs a psf which is a sum of the named
psf evaluated at different energies, weighted by the counts in
a user supplied spectrum. The syntax is, CSPEC($psf$/$model$,$spec$[,$nbin$]).

The first argument may be any energy dependent psf, eg. XRT\_PSPC
or XRT\_HRI or a POLAR/RECT model using one of these. The second
argument is a spectrum in the same data units as the dataset
being processed. When using PSS in Cash statistic mode this means
a raw counts spectrum should be used.

The third, optional argument is the number of spectral bins into
which the spectrum is divided - the default is one. The psf system
defines those bounds in energy space such that the $1/nbin$ of the
counts fall into each bin. When nbin is unity, the system finds
the mean photon energy for the spectrum supplied.

\subsubsection{POLAR model}
The POLAR option permits a polar model of the specified psf
to be constructed. The syntax is, POLAR($psf$,$rbin$[,$abin$])
where $psf$ is the name of an instrument psf (eg. XRT\_PSPC).

The value $rbin$ tells the psf system how to construct the radial bins. A
single value simply specifies a bin width. The psf system auto-
matically finds the number of radial bins in the dataset. An
alternative specification is to supply radial bin boundaries
separated by colons. In both methods of specifying $rbin$ the
values should be in the datasets axis units.

Optionally $abin$ can be used to specify the number of azimuthal
bins. If omitted, the default is one. Some examples are,

Radial bins every 10 arcmin starting from zero. Only one azimuthal bin.
\begin{quote}\begin{verbatim}
POLAR(PWFC,10.0)
\end{verbatim}\end{quote}
5 arcmin radial bins, and 5 azimuthal bins
\begin{quote}\begin{verbatim}
POLAR(PWFC,5.0,5)
\end{verbatim}\end{quote}
Defines radial bins 0$\rightarrow$5, 5$\rightarrow$10, 10$\rightarrow$30, 30$\rightarrow$60 and 60$\rightarrow$$\infty$.
\begin{quote}\begin{verbatim}
POLAR(PWFC,5:10:30:60)
\end{verbatim}\end{quote}
Note that regardless of the number of azimuthal bins selected,
the psf system only creates one bin from on-axis to the first
radial bin boundary. This is done to eliminate any chance of
discontinuities in the psf data precisely on-axis, where there
is greatest likelihood of there being an interesting source.

\subsubsection{RECT model}
The RECT option permits a polar model of the specified psf
to be constructed. The syntax is, RECT($psf$, $xbin$[,$ybin$])
where $psf$ is the name of an instrument psf (eg. XRT\_PSPC).

$xbin$ controls the placement of psf model bin boundaries in the
x-axis direction. It can be specified as either a single value
indicating bin width, or as a bin-boundary specification (values
separated by colons). $ybin$ may also be supplied. If omitted,
the same bin boundaries are used in the y-axis as in the x-axis.
Some examples are,

Bins 10 arcmin wide in both x and y axes.
\begin{quote}\begin{verbatim}
RECT(PWFC,10.0)
\end{verbatim}\end{quote}
Bin boundaries are -$\infty$ $\rightarrow$ -60, -60 $\rightarrow$ -30 and so on to 60 $\rightarrow$$\infty$
\begin{quote}\begin{verbatim}
RECT(PWFC,-60:-30:-10:0:10:30:60)
\end{verbatim}\end{quote}
\section{Text Output}
Most Asterix applications which output data in ascii form do so using
a DEV parameter. The value taken by this character string parameter
determines the destination of the output data. There are five special
values for this parameter:


\begin{description}
\item[TERMINAL, CONSOLE]
Either of these directs output to the terminal (or standard output
in UNIX speak). This is often the default choice. Using this
option means the output can be piped to
further UNIX commands, eg. {\tt more}
\item[PRINTER]
The system default print queue. The output
is spooled directly and leaves no work file
behind. See the 'Spooling' subtopic for
much more info on controlling spooling.
\item[NEWFILE, OLDFILE]
If NEWFILE is specified, then output is sent
to 'ast\_print.lis' (but this can be overridden by
particular applications, spectral
fitting uses 'fit.op'). If OLDFILE is
specified then ASTERIX attempts to open an
existing text file for append access.
\end{description}
All of the above key words can be abbreviated to as few as one letter.

\subsection{Spooling}
ASTERIX applications create printable output files in a number of
forms. Each of these forms has a name,


\begin{description}
\item[LIST]
Plain ascii text
\item[FORTRAN]
Fortran carriage control
\item[PS]
Postscript
\end{description}
The system commands used to send these outputs to a printer are
constructed by getting the value of an environment variable called
AST\_$form$\_SPOOL and tacking the filename (which may be a temporary
filename on some systems) on the end. The value of the variable
might be a system command, or a command which you have defined
yourself. Consider how the Fortran form is handled by ASTERIX. UNIX
does not print Fortran carriage control files properly directly, so
AST\_FORTRAN\_SPOOL is defined in the ASTERIX start up,

\begin{quote}\begin{verbatim}
csh> setenv AST_FORTRAN_SPOOL /star/asterix/bin/ast_fortran_spooler
\end{verbatim}\end{quote}
This command is just a very simple csh script,

\begin{quote}\begin{verbatim}
#!/bin/csh
fpr $1 | lpr
\end{verbatim}\end{quote}
The UNIX utility {\tt fpr} converts Fortran carriage control to normal
ascii - the output is then piped to the {\tt lpr} command which sends
output to the default printer. Note that the execution bit of such
scripts must be set using {\tt chmod} before ASTERIX can use them.

Another example - you have a printer with a queue
called {\tt psa} and you want to send ascii output to it. If the printer
can take ascii directly you might use,

\begin{quote}\begin{verbatim}
csh> setenv AST_LIST_SPOOL 'lpr -Ppsa -h'
\end{verbatim}\end{quote}
If it has Postscript queue then we might want to do a bit of
formatting using {\tt a2ps} first, in which case we write a script, eg.

\begin{quote}\begin{verbatim}
#!/bin/csh
a2ps $1 -nn -p | lpr -Pps -h
\end{verbatim}\end{quote}
uses {\tt a2ps} to in portrait mode, with line numbers suppressed.

If you use any of these facilities frequently then customise
your ASTERIX setup to define them when you start up ASTERIX. The
current settings can be displayed using the commands,

\begin{quote}\begin{verbatim}
csh> astdefs spool
csh> astdefs all
\end{verbatim}\end{quote}
\section{RA DEC Formats}
Some ASTERIX applications require input of right ascension and
declination celestial positions. The input format of such positions
can take any of the forms below.

\begin{verbatim}
  Decimal degrees :    235.789

  Radians         :    2.56879rad (r is sufficient)

  hms/dms         :    12:34:24.5

  hms/dms         :    12 34 24.5

  hms             :    12h34m24.5s

  dms             :    -12d45m20.5s

  dm              :    -12d45.33
\end{verbatim}
\section{Customising ASTERIX}
Many aspects of ASTERIX's operation can be customised at both the
site and user level. This is generally done by supplying extra
start up procedures which ASTERIX executes when you type 'aststart'
to start a shell or ICL session. These can override the values of
built in ASTERIX settings, or define new commands, etc.

A utility called 'astdefs' is provided which displays the important
ASTERIX control parameters, eg.

\begin{quote}\begin{verbatim}
csh> astdefs
\end{verbatim}\end{quote}
gives the output,
\begin{verbatim}
    ASTERIX defaults for user : dja

    Installation

      ASTERIX Version           : 1.7-1
      Installed root directory  : /star/asterix

    Optional Data

      FTOOLS calibration files  : unknown

    Local and user start up procedures

      Site shell start up       : /star/asterix/../ast_local/startup
      Site ICL start up         : /star/asterix/../ast_local/startup.icl
      User shell start up       : UNDEFINED
      User ICL start up         : UNDEFINED

    Logging Info

      Current logging state     : OFF
    \end{verbatim}
A fuller listing is available by adding an 'all' argument to the
{\tt astdefs} command.

\subsection{FTOOLS}
Certain parts of ASTERIX can make use of calibration files supplied
by GSFC for the analysis of ASCA data. To allow ASTERIX to use these
files you must ensure that the environment variable AST\_FTOOLS\_CALDB
points to FTOOLs 'caldb' directory, eg.
\begin{quote}\begin{verbatim}
setenv AST_FTOOLS_CALDB /base/ftools/ftools/caldb
\end{verbatim}\end{quote}
\subsection{Shell Startup}
Shell startup scripts usually set the values of environment
variables (eg. to alter printer spooling behaviour) or define
aliases or new command shortforms. A csh script to set the default
Postscript spooling and to define a new PSS shortform might look
like this.

\begin{quote}\begin{verbatim}
#! csh script - this is source'd by the ASTERIX start up
#
setenv AST_PS_SPOOL 'lpr -Ppsx -h'
#
alias xrtpss 'pss psf="polar(xrt_pspc,0.02)" mask=varp'
\end{verbatim}\end{quote}
This script can be defined using,

\begin{quote}\begin{verbatim}
csh> setenv AST_USER $HOME/mystuff.csh
\end{verbatim}\end{quote}
Be sure to make this definition before your {\tt aststart} command
is invoked.

User shell scripts are also the most sensible place to put the
definition of user ICL startup scripts.

\subsection{ICL Startup}
To customise your personal ICL startup create a file of ICL commands
and assign the environment variable AST\_USER\_ICL to that file, eg.
\begin{quote}\begin{verbatim}
csh> setenv AST_USER_ICL $HOME/mystuff.icl
\end{verbatim}\end{quote}
The most sensible place to add this definition is in your user shell
startup file.

Useful things to put in your ICL start-up are definitions of shell
commands that you tend to use inside ICL, ICL procedures or command
shortforms. An example is shown below,

\begin{quote}\begin{verbatim}
{+ mystuff.icl - My personal ICL startup
{
{ Shortform for XRT source searching
{
defstring xrtpss pss psf="polar(xrt_pspc,0.02)" mask=varp
{
proc zoomsub outname
  izoom
  x1=0.0
  x2=0.0
  y1=0.0
  y2=0.0
  file="                    "
  icurrent suppress x1=(x1) x2=(x2) y1=(y1) y2=(y2) name=(file)
  binsubset inp=(file) axes="1 2" axis1=((x1):(x2)) ~
             axis2=((y1):(y2)) out=(outname) \
  iwhole
endproc
\end{verbatim}\end{quote}
This procedure allows the user to specify an area of the current
image using the cursor which is then subsetted using the BINSUBSET
application.

\subsection{Site Startups}
ASTERIX supports both site shell and site ICL start up procedures.
These are defined by setting the AST\_LOCAL and AST\_LOCAL\_ICL
environment variables respectively.

\subsection{WWW}
Supplying 'www' or 'all' to the 'astdefs' command will produce output
similar to that below,

\begin{quote}\begin{verbatim}
World Wide Web

WWW viewer command        : Mosaic
Asterix WWW home page     : http://www.sr.bham.ac.uk:8080/...
\end{verbatim}\end{quote}
The alteration to ASTERIX's use of WWW you are most likely to want
to make is to change the WWW viewer. ASTERIX uses Mosaic by default
but this can be changed using
\begin{quote}\begin{verbatim}
csh> setenv AST_WWW_VIEWER executable_name
\end{verbatim}\end{quote}
When you use WWW inside ASTERIX (eg. by typing 'xasthelp'), the
viewer specified in this variable will be used. A useful viewer
you might consider using is the 'lynx' ascii viewer - useful for
low bandwidth network connections.

The other variable ASTERIX uses for WWW is the address of the
ASTERIX home page, which is stored in the variable AST\_WWW\_HOMPAG.
You should not alter this unless you know what you're doing!

\chapter{Using ASTHELP}
ASTHELP provides a comprehensive online HELP system for ASTERIX.

Details of all available commands, including information on all
of their parameters, are included.

ASTHELP can be used hierarchically, working down from the
top level headings to the section of interest, or by going
directly to a section by specifying a predefined keyword.
All ASTERIX commands are predefined keywords, so invoking it
with

\begin{quote}\begin{verbatim}
ICL> asthelp command_name
\end{verbatim}\end{quote}
will go straight to the section for that command. Once inside
ASTHELP, keywords can be activated by preceding them with \verb+\+
eg. \verb+\+line\_style

You may also append a path to a keyword to take you onwards
to a sub-section for that keyword eg. \verb+\+COMMAND PARAM will
take you straight to the parameter section for that command.
Defined keywords are indicated in help text by being displayed
in different intensity. Unambiguous abbreviations of keywords
may be used. At present there aren't very many but in future
more will be defined so that ASTHELP has a more hypertext feel
about it.

Other commands available are:

\begin{verbatim}
                \              goes back to top level
             <RETURN>          goes back one level
                -              go back to previous display ***
                ?              display current level
              Ctrl-Z           exit from ASTHELP
\end{verbatim}
*** if you've made a jump through hypertext to get more
information on a keyword then this is intended to
get you back to your starting point - navigation
isn't always precise and a final adjustment under
impulse power may be required
The above commands and \verb+\+KEYWORD may also be given at the
{\tt more...} prompt except that \verb+<RETURN>+ continues the display.

The full syntax of the {\tt asthelp} command is,
\begin{quote}\begin{verbatim}
asthelp [-p|-f filename [-q]] keywords
\end{verbatim}\end{quote}
where the the {\tt -p} switch causes output to be sent to the
printer and the {\tt -f} $filename$ switch sends output
to the named file. With either of these options the {\tt -q}
switch can be used to suppress screen output. which may be
useful if you wish to make hardcopy of a large section.

Also useful for copying large sections is the ellipsis
feature eg.

\begin{quote}\begin{verbatim}
csh> asthelp image... -f im.lis -q
\end{verbatim}\end{quote}
will write to file the whole help section for image processing.

\chapter{Worked Examples}
This section is intended to give a quick guide on how to do
a specific task in Asterix.

\section{Phase resolved spectral fitting}
How to perform spectral fitting on data from a given phase range
of an orbital period.


\begin{enumerate}
\item Use a sorting routine to create a 2-d, time x PH\_channel
binned dataset. e.g. XSORT.

\begin{verbatim}
  \end{verbatim}
\item Exposure correct the file to convert it into counts/second
e.g. for the XRT.

\begin{quote}\begin{verbatim}
  ICL> XRTCORR
  XRTCORR version 1.4-6
  INP - Input dataset name /@MID_SPEC/ > TIMPH
  OUT - Output dataset name /@TIMPHC/ >
  RTNAME - Rootname for calibration files /'[HIS.2142]X2142'/ >
  Warning: insufficient event rate data to calculate dead time correction
           for 19 timebins
  Correction for these bins has been set to 1.0
  Warning: bad eventrate data for 9 timebins
  Correction for these bins has been set to 1.0
  Mean dead time correction : 1.001657
  Mean vignetting correction : 58.66176
  Mean scattering correction : 1.007486
  \end{verbatim}\end{quote}
\item Fold the data at the known orbital period.

\begin{quote}\begin{verbatim}
  ICL> FOLDBIN
  FOLDBIN Version 1.2-2
  INP - Enter name of input file /@TIMPHC/ >
  Data is 2 dimensional
  OUT - Enter name of output file > TIMPHCFOLD
  TIME UNITS are seconds
  PERIOD - Enter period for folding > 4128.1
  EPOCH - Enter epoch of phase zero /6775.0817013889/ > 6774.002
  BINS - Enter number of phase bins /8/ > 5
  WEIGHT - Are weighted means required? /NO/ >
  ** VARB is zero - CHISQ may not be correct in output file **
  \end{verbatim}\end{quote}
\item Having converted the time axis into 5 phase bins, we now want to
extract the central 20 \% of the orbit.

\begin{quote}\begin{verbatim}
  ICL> BINSUBSET
  BINSUBSET Version 1.2-0
  INP - Input dataset /@TIMPHC/ > TIMPHCFOLD
  OUT - Output dataset > MIDSPEC
  The axes present are:
   1 Phase
   2 Corrected PHA channel
  AXES - Axis numbers to select on > 1
  The range for axis 1 is 0.1 to 0.9
  AXIS1 - Axis 1 ranges to subset /'0.1:0.9'/ > 0.4:0.6
  \end{verbatim}\end{quote}
\item We now have a single spectrum to perform spectral fitting on.
Create a detector response structure for this datafile. e.g.
for the ROSAT-XRT use XRTRESP.

\begin{quote}\begin{verbatim}
  ICL> XRTRESP
  XRTRESP Version 1.3-4
  INP - Name of source file /@MIDSPEC/ >
  RESPFILE - Name of instrument response matrix file /@X2142_DRPM/ >
  \end{verbatim}\end{quote}
\item Set up a model for the spectral fit

\begin{quote}\begin{verbatim}
  ICL> SMODEL
  SMODEL Version 1.0-1
  Input model file :- DISK$SCRATCH:[ASTERIX88]MOD.SDF;1

  A composite model can be synthesised using + - * ( ) and
  any of the following primitive models:

  AG     galactic interstellar absorption             multiplicative
  AB     intrinsic absorption                         multiplicative
  BR     thermal bremsstrahlung (cosmic)              additive
  PL     power law                                    additive
  BB     black body                                   additive
  WN     Wien                                         additive
  BH     hydrogen bremsstrahlung                      additive
  BP     pseudo-bremsstrahlung                        additive
  PC     cutoff power law                             additive
  CS     Chapline-Stevens Comptonised brems.          additive
  ST     Sunyaev-Titarchuk Comptonisation             additive
  RZ     Raymond-Smith plasma (metals combined)       additive
  LG     Gaussian line                                additive
  LL     Lorentzian line                              additive

 SPEC - Model specification > BR*AG

                      COMPONENT  1
                     **************
Name: thermal bremsstrahlung (cosmic)          Keyword: BR
 PARAMETER  1 - em10                       (1e60cm**-3/(10kpc)**2)
                value,min,max =   1.0000       1.00000E-04    10.000
VALS - New values /'unchanged'/ >
 PARAMETER  2 - temperature                (keV)
                value,min,max =   5.0000       0.10000        20.000
VALS - New values /'unchanged'/ >

                      COMPONENT  2
                     **************
Name: galactic interstellar absorption         Keyword: AG
 PARAMETER  1 - hydrogen column density    (1.0E21 /cm**2)
                value,min,max =   1.0000       0.00000E+00    20.000
VALS - New values /'unchanged'/ > 0.28
  \end{verbatim}\end{quote}
g) Freeze the hydrogen column density and perform the fit.
\begin{quote}\begin{verbatim}
  ICL> FREEZE 3
       * hydrogen column density frozen

  ICL> SFIT
  SFIT Version 1.4-1
  ...
  \end{verbatim}\end{quote}
\item Having finished processing the data - review the history
of the final file.

\begin{quote}\begin{verbatim}
  ICL> hist midspec
  HIST Version 1.8-1

  History of  : /datan5/asterix/midspec.sdf
                SPEC
  Created     : 9-AUG-91 14:25:16
  Update mode : NORMAL
  Contains    : 5 records

  Record 1:

    Creator: XSORT Version 1.3-5
    Date:    9-AUG-91 14:25:17

  Record 2:

    Creator: XRTCORR version 1.4-6
    Date:    9-AUG-91 15:31:44
    Text:    Input dataset  1:
                DISK$SCRATCH:[ASTERIX88]TIMPH.SDF;1

  Record 3:

    Creator: FOLDBIN Version 1.2-2
    Date:    9-AUG-91 15:36:49
    Text:    Input file :
                DISK$SCRATCH:[ASTERIX88]TIMPHC.SDF;3
             Folded output file :
                DISK$SCRATCH:[ASTERIX88]TIMPHCFOLD.SDF;2
             The data has been folded into a period of 4128.1 seconds
             The epoch of phase zero was 6774.002

  Record 4:

    Creator: BINSUBSET Version 1.2-0
    Date:    9-AUG-91 15:39:14
    Text:    Input dataset  1:
               DISK$SCRATCH:[ASTERIX88]TIMPHCFOLD.SDF;2

  Record 5:

    Creator: XRTRESP Version 1.3-4
    Date:    9-AUG-91 15:42:23
    Text:    Set up detector response structure in datafile
  \end{verbatim}\end{quote}
\end{enumerate}
\chapter{Documentation}
ASTERIX offline documentation is to be found in directory
\$AST\_DOCS. There are two catagories of document:
\begin{verbatim}
        USER_xxx   for general users of ASTERIX
        PROG_xxx   for programmers of ASTERIX applications
\end{verbatim}
The following is a list of documents currently available:
\begin{verbatim}
   USER_001 *      ASTERIX user guide (this has become the Starlink
                   Document SUN98)
   USER_002        Processing Exosat raw data using ASTERIX
   USER_004        A guide to ASTERIX source searching
   USER_005        The Rosat XRT interface in ASTERIX
   USER_006 *      A guide to time series analysis within ASTERIX

   PROG_001        Programming Standards
   PROG_002        Data structures: specifications and conventions
   PROG_003        The BDA_ binned data access routines
   PROG_004        The DYN_ dynamic memory routines
   PROG_005        The USI_ user interface routines
   PROG_006        The HIST_ history routines
   PROG_007        The GCB_ GRAFIX control block routines
   PROG_008        Notes on writing ASTHELP
   PROG_009        ASTERIX directory structure
   PROG_010        The AXIS_ axis identification routines
   PROG_011        The PSF_ system routines
   PROG_012        Introduction to ASTERIX programming
   PROG_013        The SSO_ source search results access routines

   * Indicates TeX file - use DVI file with (say) DVIPS
   \end{verbatim}
\chapter{This release}
Asterix version 1.8-0 is the second major release of Asterix for UNIX,
and the first release which is not supported on VMS.

There are changes to most ASTERIX sub-systems and to the graphics and
image processing in particular. These are detailed
further in the sub-topics along with other notable developments.

\section{ROSAT}
\begin{itemize}
\item One new detector map has been added to XRTEXPMAP's repertoire.
\item HRI vignetting and deadtime corrections included in XRTCORR and XPSSCORR.
\item HRI response matrix can be attached to a dataset using
XRTRESP using the keyword HRI on the command line.

\item Some bug fixes
\end{itemize}
\section{ARD}
The use of ARD (Ascii Regional Description) has been rationalised
within ASTERIX. Previously an old, development version was being
used, the code having been frozen into the ASTERIX libraries. We
are now using the official, released Starlink version. A
consequence of this is that ARD files previously created with
ASTERIX will be incompatible as the syntax is slightly different.
Some minor editing will rectify this, the main difference being
the removal of the COMPOSITE keyword. The syntax is described in
SUN 183. Commands which have been changed to use the new syntax
are XSPOKES, REGIONS and QUALITY (ARDQUAL). There is also a new
image processing command, IREGION, which incorporates the
functionality of the first two of these, and much more - more
details of that in the following section.

\section{Graphics}
\begin{itemize}
\item Previously regions of interest could only be specified as circles
or boxes. Now you can specify virtually any shape you like
using a new command, IREGION. This can be driven by ARD input, from
a file or entered at the keyboard; by cursor input, defining a
variety of shapes; or by parameter input. Shapes can be built up
incrementally through logical combinations. The current shape can be
displayed on the image and an ARD description of the shape can be
exported to a text file for subsequent input to other commands, such
as event sorting. (One of the shapes built in is the ROSAT XRT
support structure, so the full functionality of the command XSPOKES
is included.) Where appropriate other image processing commands
(eg ISTATS) will operate only on the current region. IEXCLUDE has
been changed slightly to allow the current region to be input. So
if you want to exclude a complex region (by setting QUALITY bad), the
best method is to use IREGION to define the region, then IEXCLUDE
that region, then reselect the whole image using IREGION again.
(The QUALITY array can now be visualised using a new option in
IPIXEL.)

Image Processing has always had the concept of 'current position' - the
position always presented as default when a command requires positional
input. That concept has now been extended to a list of positions.
The command IPOS now has an option which allows a list of RA/DECs to
be entered, directly at the keyboard, as a text file, or as a PSS
results file. Other options of IPOS then allow the current position
to be selected from that list. The command IMARK, which marks positions
onto the displayed image, now has an additional option to use the
internal list. Another slight change to IMARK is that when you use
the NUMBER option to write a numeric label next to each mark, that
numbering will now be 'remembered' and will be reproduced if the image
is redisplayed. Of course if you are marking from the internal list
then the numeric labels will coincide with the position in the list.

The other change to Image Processing is that IPATCH now has a PASTE
mode that allows holes to be patched by pasting from another part of
the image. It does this completely blind, so it is up to the user
to ensure the validity of this.

There are just a couple of minor changes to the general graphics
command GSET. When specifying the title lines to go above a plot
you can now enter multiple lines with one invocation. Also there
is a SUPPRESS option which can be used together with the switches,
LABEL, AXES and TITLE, to suppress even the defaults for these.

\item The old graphics commands (DRAW + ancillaries) are no longer in this
release. The {\tt new\_graphics} and {\tt old\_graphics}
commands have been withdrawn.

\end{itemize}
\section{Spectral Fitting}
\begin{itemize}
\item New command TIE allows simple equality constraints to be defined
for a spectral fit.
\end{itemize}
\section{Time Series Analysis}
\begin{itemize}
\item There is a new command, TONOFF, which writes a file of on-off times
in MJD, which can be used as input to XSORT. It can be used as part
of the interactive Time Series Editing system or standalone. In the
former case it takes on-off times from the currently selected time
segments, and in the latter it uses the QUALITY array and gives
on-off times for periods of good QUALITY. This can be used if you
want to produce a spectrum, say, of the quiet and busy periods of
a variable object. You would use the Time Series stuff to pick out
the periods of interest, use TONOFF to write the file of on-off times,
then sort a spectrum using the on-off times as input.
\end{itemize}
\section{Utilities}
\begin{itemize}
\item The old ASTQDP program has been replaced by a format conversion
program called AST2QDP which converts HDS files to QDP format.
A procedure called ASTQDP wraps up this format convertor to run
QDP using the converted file. This separation of the two facilities
makes installation more reliable, and allows the user to make
use of their own site's, or even their own personal copy of QDP.
\item The AST2XSP program has been updated to write out the new style
FITS spectral files and response matrices. These files are much
more amenable to user editing than the old format, and being FITS
are portable across all architectures on which FITS is supported.
\item A two-dimensional image convolution program has been added, called
IXCONV. Performs both cyclic and non-cyclic image convolution.
\item IMOSAIC now correctly handles image mosaicing near the celestial
poles.
\end{itemize}
\section{Documentation}
\begin{itemize}
\item A new version of SUN98 is included in this release
\end{itemize}
\chapter{Binned dataset commands}
These are general applications working on binned datasets, which
do not fall into any of the specific catagories.

\section{ASMOOTH}
Smooths the input N-dimensional dataset with a mask whose width
varies to achieve constant signal to noise at user defined data
levels.

This application is most useful for smoothing image data containing
diffuse emission, where a wide mask at low data levels smooths the
diffuse emission sufficiently, but a progressively narrower mask
as the data value increases means point sources are not blurred out.

\subsection{Input Output data}
Input: A single binned dataset

Output: A single binned dataset

\subsection{Background Subtracted Data}
ASMOOTH treats data slices which lie below zero in a special way.
These usually occur in background subtracted or patched data. The
assumption is made that the negative regions are unphysical, and
so are smoothed completely flat. The normalisation of these slices
is kept such that the flux is conserved in the total output image.

\subsection{Biasing}
If the minimum data value in the dataset is less than or equal to
zero, and the user selects log spacing of the data layers then
there are obviously problems. ASMOOTH solves this by adding a bias
value to the data minimum and maximum before calculating the
logarithmically spaced data levels. Once calculated the bias value
is subtracted off.

The default for the bias level is $|dmin| + 1\% * dmax$. The value
must clearly be at least as big as $|dmin|$, but if exactly that value
is supplied then there are an infinite number of logarithmic
decades between $(dmin+bias)$ and $dmax$. ASMOOTH adds on another 1\%
of the data range, which will reduces the dynamic range in the
data to two decades. By adding on smaller percentages of the data
range in response to the BIAS prompt (ie. values less than the
default but larger than $|dmin|$) a larger dynamic range will be
retained.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Input binned dataset to be smoothed.
                            May be primitive
   -GLOBAL.BINDS (Default binned dataset)
 OUT       2    UNIV        Output dataset
   ->GLOBAL.BINDS (Default binned dataset)
 SDIMS     -    INTEGER     Array of those dimensions to be smoothed.
                            Can contain only 1 or 2 axis numbers at
                            present.
   Default = 1
 SNR       -    REAL        The target signal to noise. Controls how
                            much smoothing is done at each data level
   Default = 3
 FILTER    -    INTEGER     The mask form. 1=gaussian, 2=tophat,
                            3=cosine.
   Default = 1
 OPT       -    INTEGER     Option for setting data levels at which
                            input is smoothed. If 1, values are
                            linearly spaced between min and max in data,
                            logarithmically if OPT=2, or at arbitrary
                            user supplied levels if OPT=3.
   Default = 2
 NLEV      -    INTEGER     Number of levels for OPT=1,2.
   Default = 5
 BIAS      -    REAL        See sub-topic "Biasing"
   Default = $|dmin| + 1% * dmax$
 LEVS           REAL        Array of data levels at which smoothing is
                            performed. Only used if OPT=3.
 WSTART    -    INTEGER     Initial guess for mask width.
   Hidden default = 3
 WMAXSZ    -    INTEGER     Maximum half width of mask in pixels. The
   Hidden default = 30      mask size for a given SNR is limited by
                            this quantity - it can be made as large
                            as you like.

\end{verbatim}\subsection{Masks}
Three mask types are currently supported by ASMOOTH. The mask is
chosen using the FILTER parameter, which takes the following values


\begin{description}
\item[Gaussian]
The extent of the mask used is sufficient
to enclose at least 99\% of the power, regardless of
its FWHW.
\item[Top Hat]
The mask extent always encloses 100\% of
the mask power.
\item[Cosine]
The mask extent always encloses 100\% of
the mask power.
\end{description}
\subsection{Examples}
Smooth IMAGE to create SMIMAGE, using logarithmically spaced
data levels, a gaussian filter and aim for a signal to noise
ratio of 5.

\begin{quote}\begin{verbatim}
ICL> ASMOOTH IMAGE SMIMAGE OPT=2 NLEV=5 SNR=5 \
\end{verbatim}\end{quote}
Smooth at the data levels 2,4,8,16 and 32.

\begin{quote}\begin{verbatim}
ICL> ASMOOTH IMAGE SMIMAGE OPT=3 LEVS=[2,4,8,16,32] SNR=5 \
\end{verbatim}\end{quote}
\section{AXCENTROID}
Centroids the data in the input dataset with respect to one of its
axes, ie. finds the mean axis value on the selected axis weighted by
the data. If the input is one-dimensional then the centroid is printed
to the terminal, otherwise an output dataset is created with one
dimension fewer than the input.

Quality is supported, but variance is ignored at present.

\subsection{Input Output data}
Input: A single binned dataset

Output: A single binned dataset

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Input dataset, must be non-scalar but
                            may be primitive
   -GLOBAL.BINDS (Default binned dataset)
 OUT       2    UNIV        Output dataset. Produced if INP has
                            a dimensionality > 1
   ->GLOBAL.BINDS (Default binned dataset)
 AXIS      -    INTEGER     Axis for centroiding

\end{verbatim}\subsection{Example}
A one dimensional dataset,

\begin{quote}\begin{verbatim}
ICL> axcentroid gx6
AXCENTROID Version 1.7-1
Centroid wrt axis 1 is 11.46023
\end{verbatim}\end{quote}
A two dimensional dataset. On a spectral series, centroiding on the
spectral axis produces the mean spectral channel as a function of
time.

\begin{quote}\begin{verbatim}
ICL> axcentroid gx_spec_series
AXCENTROID Version 1.7-1
AXIS - Axis to centroid on > 1
OUT - Output dataset > gx_mchan_t
\end{verbatim}\end{quote}
\section{AXFLIP}
AXFLIP reverses the direction of a dataset about any or all of its
axes. Datasets up to a limit of 7 dimensions are handled.

The user is prompted for the axes to flip, except in the case of
1 dimensional inputs.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn  Type       Description
 -------    ----  ----       -----------
 INP          1   CHARACTER  Input object.
    - GLOBAL.BINDS (the current binned dataset)
 SELAX        2   INTEGER[]  Axes to be flipped.
 OUT          3   CHARACTER  Output object.
    -> GLOBAL.BINDS (the current binned dataset)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> axflip
AXFLIP Version 1.2-0
INP - Input object /@DBB/ > /data/asterix/dbb
Axes present in dataset are:

 1 X_CORR
 2 Y_CORR

SELAX - Axis numbers to flip - separate by spaces > 1 2
OUT - Output for flipped data > /data/asterix/fdbb
\end{verbatim}\end{quote}
\section{AXORDER}
AXORDER reorders the axes of a binned dataset. The user is prompted
for the order of the axes in the output dataset only if the input
dimensionality is greater than 2.

In the special case of the input being one dimensional, AXORDER
swaps the data array and axis values. The quality array is unchanged,
and variances and axis widths are transformed. Note that unless the
data values increase or decrease monotonically, AXORDER can generate
output which will cause many Asterix programs using range specifi-
cations to fail.

AXORDER will not work on primitive objects.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn  Type       Description
 -------    ----  ----       -----------
 INP          1   CHARACTER  Input object.
    - GLOBAL.BINDS (the current binned dataset)
 OUT          2   CHARACTER  Output object.
    -> GLOBAL.BINDS (the current binned dataset)
 SELAX        3   INTEGER[]  New order of axes

\end{verbatim}\subsection{Examples}
Swapping axes of a cube from X,Y,E to E,X,Y
\begin{quote}\begin{verbatim}
ICL> axorder cube newcube
AXORDER Version 1.8-0
Axes present in dataset are:

 1 X_CORR
 2 Y_CORR
 3 ENERGY

SELAX - Axis numbers to flip - separate by spaces > 3 1 2
\end{verbatim}\end{quote}
\section{BINMERGE}
BINMERGE merges 2 to 10 binned datasets. All must have the same
dimensionality.

Specify each input dataset in response to the prompt INP=
finishing with a NULL (!).

Data arrays may be n-dimensional, and are merged along their
axes. The arrays must be of the same dimensionality, but the
length of each dimension may vary. For the results to be
meaningful all datasets must have their axes in the same order.

At present the program looks for BASE\_TAI to determine time
offsets between the datasets and checks that axes have the same
units in all datasets.

The merged dataset should be immediately REBINed to to combine
overlapping regions, and sort out the axis values.

This routine should be run only on {\em corrected} datasets as it does
not modify LIVE\_TIME components.

Header data is carried over from the first dataset only.

\subsection{Input output data}
Input: up to 10 binned datasets

Output: A single binned dataset

\subsection{Method}
Each of the user-supplied list of datasets is checked against the
first to see that its structure is similar. The output dataset is
formed using the total lengths of the axes found in the input
datasets.

The datasets are merged in the order in which they are entered.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn    Type       Description
 -------  ----    ----       -----------
 INP1       -     CHARACTER  Input dataset(s)
 INP2       -         "         "      "
 INP3       -         "         "      "
 INP4       -         "         "      "
 INP5       -         "         "      "
 OUT        1     CHARACTER  Output dataset.
    -> GLOBAL.BINDS (the current binned dataset)
 CONT       -     LOGICAL    Continue after error?

\end{verbatim}\section{BINSUBSET}
This takes a subset of a binned dataset, as specified by one or
more ranges along selected axes, and writes that subset into
a new dataset. By default it keeps the data within the specified
ranges, but by including the keyword NOKEEP on the command line
it can be made to exclude them instead. Default behaviour is for
the ranges to be specified in axis values, however, by using the
INDEX keyword, array index values may be used. If any axis is reduced
to a single bin, then by default that axis is excluded from the
output and the dimensionality is reduced by one. This behaviour
can be switched off by including the keyword NOSLICE on the
command line.

This application should be used {\em after} any corrections have been
applied to the binned dataset.

\subsection{Algorithm}
BINSUBSET does {\em not} "slice up" input bins - each input bin is
transferred to the output dataset on an all-or-nothing basis.
An input bin is included in the output if the range specified
falls anywhere within that bin.

\subsection{Input output data}
Input: 1D to 7D binned dataset

Output: modified binned dataset

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn  Type       Description
 -------    ----  ----       -----------
 INP          1   CHARACTER  Input object.
    - GLOBAL.BINDS (the current binned dataset)
 OUT          2   CHARACTER  Output object.
    -> GLOBAL.BINDS (the current binned dataset)
 KEEP         -   LOGICAL    Keep the specified ranges?
    Hidden default = YES
 SLICE        -   LOGICAL    Take slice if axis reduced to one bin
    Hidden default = YES
 INDEX        -   LOGICAL    Use index values
    Hidden default = NO
 AXES         3   CHARACTER  Axis numbers to be selected on
 AXIS1        -   CHARACTER  Axis 1 range(s)
 AXIS2        -   CHARACTER  Axis 2 range(s)
 AXIS3        -   CHARACTER  Axis 3 range(s)
 AXIS4        -   CHARACTER  Axis 4 range(s)
 AXIS5        -   CHARACTER  Axis 5 range(s)
 AXIS6        -   CHARACTER  Axis 6 range(s)
 AXIS7        -   CHARACTER  Axis 7 range(s)

\end{verbatim}\subsection{Examples}
Take subset of DS1 and put into DS2, giving axis ranges in response
to prompts
\begin{quote}\begin{verbatim}
> BINSUB DS1 DS2
\end{verbatim}\end{quote}
Same but discarding specified data
\begin{quote}\begin{verbatim}
> BINSUB DS1 DS2 NOKEEP
\end{verbatim}\end{quote}
Ranges to be specified by bin index
\begin{quote}\begin{verbatim}
> BINSUB DS1 DS2 INDEX
\end{verbatim}\end{quote}
\section{CORRECT}
Applies exposure and dead time corrections to binned datasets. These
corrections are performed using only standard instrument independent
structures within your dataset (if present) and so should work on
data from any source.

CORRECT gives the option of adding a variance array to its output if
one is not present in its input. This enables (for example) swift
progression from a simple binned spectrum to spectral fitting.

Note that if Asterix has an instrument interface for the dataset you
are processing then you should check to see if there is a specialised
application available - this is likely to do a better job than CORRECT.
This is the case for all the EXOSAT, WFC and XRT interfaces.

\subsection{Input output data}
Input: 1D to 7D binned dataset

Output: modified binned dataset

\subsection{Algorithm}
Exposure correction is performed by dividing the values in the input
data by an exposure time. CORRECT attempts to get an exposure time from
three sources, in order,

\begin{itemize}
\item The EFF\_EXPOSURE component of the input dataset header
\item The EXPOSURE\_TIME component of the input dataset header
\item From the user using the TEXP parameter
\end{itemize}
Dead time correction is only performed if a valid LIVE\_TIME structure
is defined in the dataset. This contains 3 components, the ON, OFF and
DURATION. The first two list the start and stop times during which the
detector was exposed to a "source". The last component is the time for
which the detector would actually have been able to register a count.
DURATION(i) is generally less than OFF(i)-ON(i) for photon counting
detectors due to dead time in the counting electronics. CORRECT applies
dead time correction slightly differently depending on whether the
input dataset contains a time axis.

If no time axis is present, the sum of (OFF-ON)/DURATION provides a
dead time correction factor which is the same for every bin. The
presence of a time axis causes this correction to be applied in for
each time bin individually.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn  Type       Description
 -------    ----  ----       -----------
 INP          1   CHARACTER  Input object.
    - GLOBAL.BINDS (the current binned dataset)
 OUT          2   CHARACTER  Output object.
    -> GLOBAL.BINDS (the current binned dataset)
 OVER         -   LOGICAL    Overwrite the input dataset?
    Hidden default = NO
 POISS        -   LOGICAL    Create variance in output dataset by
                             copying input data?
    Default = YES
 TEXP         -   REAL       Exposure time for the dataset. Only
                             used if one is not present in the
                             input dataset.

\end{verbatim}\section{DTREND+}
Invokes POLYFIT in DTREND mode
\section{ENMAP}
Produces an image, colour coded by the mean energy of the photons
in each pixel. i.e. a pixel which has primarily low energy photons
incident on it, will be assigned a low value, whereas a pixel containing
on average higher energy photons will have a larger value. When the
image produced by ENMAP is displayed, it can help to bring sources
out of the background or distinguish between sources with a different
spectrum. To help improve the signal to noise it is sometimes useful
to set a lower threshold so that background pixels get set to zero.

\subsection{Input output}
The input file should be an Asterix 3-d file containing an energy
or pulse height channel axis. It doesn't matter what the other two axes
are, but typically they would be the X and Y axis.

The output file is a 2-d Asterix file, containing the two axes which
weren't the energy axis ( typically the X and Y axes ), with data values
scaled by the average value of the energy in each pixel.

\subsection{Method}
First of all a 3-d datafile should be created with a sort program,
e.g. XSORT. Usually a spectral image, containing the X,Y and pulse
height channels (corrected PHA channel for the XRT-PSPC) should be
created.

ENMAP takes this file and finds the mean pulse height channel in each
image pixel. It then produces an output file containing an image colour
coded with the mean photon energy, or more correctly, pulse height
channel.

\subsection{XRT Hints}
With the ROSAT PSPC, the best result seems to be obtained by initially
producing a spectral image with X,Y dimensions of 256 or 512 and
a coarsely binned spectral axis e.g. with 5 or 6 bins.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Name of input 3-d file
 OUT       2    UNIV        Name of output 2-d dataset
    -> GLOBAL.BINDS (the current binned dataset)
 THRESH         REAL        Counts threshold below which pixels
                            are set to zero
 ENAXIS         INTEGER     Position of energy/PH channel axis
                            (only used if the program can't understand
                             the axis labels)

\end{verbatim}\subsection{Examples}
To produce an energy map of a spectral image and ignore pixels with
less than three counts.

\begin{quote}\begin{verbatim}
ICL> ENMAP SPECIM ENIM THRESH=3.0
\end{verbatim}\end{quote}
\section{INTERP}
Corrects bad data values by fitting a spline to good values and
interpolating.

\subsection{Edges}
The spline method used falls over when bad points are at the edge
of the field. So when a bad quality point is found outside the
strip of good points being fitted, the bad pixel is set to the
value of the nearest GOOD pixel in that strip. This is reported
at run time by an error message for each strip where it occurs.

\subsection{Method}
Takes an n-dimensional array of data and fits a spline to points
which are of "good" quality.

The spline is 'tied' to the data at points called knots. The more
knots chosen, then the more closely the spline fits the good data
points.

The number of knots is determined by the user by stating the
number of bins required between knots.

The "bad" points are then reconstituted using the spline
coefficients. The quality of these points is set to good while
the errors are set to the average variance of the "Good" points.

The spline can be fitted in any particular direction as required.
The above procedure is performed for each 1d slice of the
data\_array along the dimension chosen.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    CHARACTER  Input datafile name
 OUT       2    CHARACTER  Output datafile name
   -> GLOBAL.BINDS (the current binned dataset)
 APP_DIM   3    INTEGER    Dimension over which the spline should
                           be fitted.
 LOWER     4    REAL       The lower edge of the range for a
                           parameter.
 UPPER     5    REAL       The upper edge of the range for a
                           parameter.
 KWIDTH    6    INTEGER    Number of data bins between knots in
                           the spline.

\end{verbatim}\subsection{Examples}
Interpolating over the time dimension in an image series. Using
the full image.

\begin{quote}\begin{verbatim}
ICL> INTERP
Input datafile > IMAGE3D
Output datafile > IMAGE3DINT
1.  X POSITION
2.  Y POSITION
3.  TIME
Dimension to be interpolated > 3
1.  X POSITION
Lower bound for this parameter/-0.2/ >
Upper bound for this parameter/0.2/ >
2.  Y POSITION
Lower bound for this parameter/-0.2/ >
Upper bound for this parameter/0.2/ >
Number of bins between knots > 8
\end{verbatim}\end{quote}
\section{MEANDAT}
Finds the average of up to 20 datafiles using a weighted or
unweighted mean. It can also be used to find the sum of several files.
NB: These files should be fully instrument corrected, any attempt to
instrument correct the final file will be wrong.
\subsection{Method}
The program will work on files with up to seven dimensions.
The output array contains the mean of the input arrays. Each input
element may be weighted.
Weighting can either be by the reciprocal of the variance, a user
supplied value for each file (e.g. the total exposure time) or
the mean can be unweighted. If an input element has bad quality then
it will be ignored. An output element will be good unless none of the
corresponding input elements were good. An output element which has
been produced from fewer good pixels than its neighbours will be
apparent by its larger variance. An output variance array is only
produced if all the input files contain variance arrays. Quality
checking is only carried out if all files contain quality, otherwise
{\em none} of the files are quality checked and an output quality array is
not produced.

\subsubsection{Summing files}
A weighted or unweighted sum of the files, may be achieved by
setting 'AVERAGE=NO' on the command line. If you produce a weighted
sum then the program DOES NOT perform any normalisation.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 NFILES    1    INTEGER     Number of files to average
 FILE1     2    UNIV        Name of first input dataset
    - GLOBAL.BINDS (the current binned dataset)
 FILE2     3    UNIV        Name of second input dataset
   .
   .
 FILE20    21   UNIV        Name of 20th input dataset
 WTMETH         CHARACTER   Type of weighting to be applied
                            'U'=user supplied weighting
                            'V'=reciprical of variance weighting
                            'N'=No weighting
 WEIGHT         REAL        Weighting factor for a given file
                            (only if WTMETH='U' selected)
 AVERAGE        Logical     By default the files are averaged.
                            However a sum of the files may be obtained
                            by setting 'AVERAGE=NO' on the command line
 OUTPUT         CHARACTER   Name of output data set
    -> GLOBAL.BINDS (the current binned dataset)

\end{verbatim}\subsection{Examples}
To find the average of two files using a mean weighted by the variance

\begin{quote}\begin{verbatim}
ICL> MEANDAT 2 FILE1 FILE2 OUTPUT=OUT_FILE
WTMETH - Weighting method (U,V,N) > V
\end{verbatim}\end{quote}
To find the average using weighting values of your own

\begin{quote}\begin{verbatim}
  ICL> MEANDAT 2 FILE1 FILE2 OUTPUT=OUT_FILE
  WTMETH - Weighting method (U,V,N) > U
  Weight - Enter weighting for file 1 > 3.0
  Weight - Enter weighting for file 2 > 5.0
\end{verbatim}\end{quote}
To find the average weighting each file by its total exposure time
\begin{quote}\begin{verbatim}
ICL> MEANDAT 2 FILE1 FILE2 OUTPUT=OUT_FILE
WTMETH - Weighting method (U,V,N) > U
Weight - Enter weighting for file 1 >
             FILE1.MORE.ASTERIX.HEADER.EXPOSURE_TIME
Weight - Enter weighting for file 2 >
             FILE2.MORE.ASTERIX.HEADER.EXPOSURE_TIME
\end{verbatim}\end{quote}
To find the sum of these two files

\begin{quote}\begin{verbatim}
ICL> MEANDAT 2 FILE1 FILE2 AVERAGE=NO
\end{verbatim}\end{quote}
\section{MKCOLIM+}
Combines three different energy images to produce an output image
containing energy information which may be used with the colour
table set up in the 'I' routines by ICOLOUR RGB. This is designed to
be used with the PSPC, where it is recommended that images with the
pulse height ranges, 8(11)-40, 41-100 and 101-200 are used as the
input files. The routine is not PSPC specific however, and it may
be used on any instrument capable of producing energy resolved images.

\subsection{Method}
It is a wrap-up of MEANDAT. The low energy images is weighted by 1.0,
the medium energy by 6.0 and the high energy by 36.0. The resultant
values key into a colour table defined by ICOLOUR RGB (or GSET COL RGB)
and make no real sense unless this colour table has been used.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 FILE1          UNIV        Name of low energy image
    - GLOBAL.BINDS (the current binned dataset)
 FILE2          UNIV        Name of medium energy image
 FILE3          UNIV        Name of high energy image
 OUTPUT         CHARACTER   Name of output data set
    -> GLOBAL.BINDS (the current binned dataset)

\end{verbatim}\section{POLYFIT}
Fits a 1 dimensional polynomial to a data set.

If the data set is 1 dimensional then the polynomial coefficients
are displayed.

If the data is more than one dimensional, then a series of 1
dimensional polynomial fits is made. The fits are made along the
first axis in the dataset, whilst the other axes are looped over.

VARIANCE (errors**2), and QUALITY are taken into account if present.

Polynomials of degree 0 to 10 inclusive can be calculated.

The program output is a binned dataset containing either the fit
to the data points (FIT = TRUE, default), or the original data
minus the fit to it (DTREND = TRUE).

Either mode can be done in place by using the OVERWRITE
parameter, otherwise a new output object will be created.

\subsection{Input Output Data}
Input: HDS structure containing N-dimensional DATA\_ARRAY

Output: HDS structure containing N-dimensional DATA\_ARRAY

\subsection{Shortforms}
DTREND = POLYFIT DTREND
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Name of input dataset
 OUT       2    CHARACTER   Name of output data set
    -> GLOBAL.BINDS (the current binned dataset)
 DEGREE    3    INTEGER     Degree of required polynomial fit
 FIT       -    LOGICAL     FIT mode required?
    Hidden default = YES
 DTREND    -    LOGICAL     DTREND mode required?
    Hidden default = NO
 OVERWRITE -    LOGICAL     Overwrite input dataset?
    Hidden default = NO

\end{verbatim}\section{PROJECT}
Allows one axis of a dataset of two or more dimensions to be
collapsed by projecting along it.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Name of input dataset
 OUT       2    CHARACTER   Name of output data set
    -> GLOBAL.BINDS (the current binned dataset)
 AXIS      3    INTEGER     Axis to be collapsed

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> PROJECT image out_ds 2     - projects along axis 2
\end{verbatim}\end{quote}
\section{RADOPT}
Calculates the radius of a circular box around a source which
will maximise the signal to noise of the source. It currently
works for the ROSAT XRT and WFC experiments.

NB: It only works in pointed mode - NOT for survey data.

\subsection{Method}
The routine needs to know the total number of counts from the
source, the counts per unit area from the background, the
off-axis position of the source and the instrument used.

The signal to noise ratio is defined as:

\begin{quote}\begin{verbatim}
S/N = Ns * FRAC(r) / SQRT( Ns * FRAC(r) + Nb * 2 * PI * r**2 )
\end{verbatim}\end{quote}
Where, $Ns$ is the total no. of source counts, $Nb$ is
the background flux per unit area, $r $is the box radius and
$FRAC(r)$ is the fraction
of source counts which are contained within a circle of radius
$r$.
$FRAC(r)$ is a function of the detector involved e.g. WFC, XRT and
the distance of the source from the optical axis.

RADOPT finds the maximum of this function by trying a series of
increasing radii until a maximum is found.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INSTR        1      CHAR       Instrument name  e.g. 'WFC' or 'XRT'
 SCOUNT       2      REAL       Total counts from source
 BCOUNT       3      REAL       Counts from background per square arcmin
 AZIMUTH      4      REAL       Off-axis azimuth (arcmin)
 ELEV         5      REAL       Off-axis elevation (arcmin)
 ENERGY              REAL       Mean energy of source photons (keV)
 DMJD                DOUBLE     Modified Julian date of observation
                                         (WFC only)
 IFILT               INTEGER    WFC filter number
 DISP                LOGICAL    Display the result ?
    Default = YES
 RADIUS     output   REAL       Radius of circle giving best S/N ratio
                                          (arcmin)
 MAXFRAC    output   REAL       Fraction of counts within best radius

\end{verbatim}\subsubsection{BCOUNT}
The number of counts in the background per square arcmin. This
can usually be found by using ICIRCSTATS around a background
region of the image containing the source.

\subsubsection{DMJD}
The calibration data for the WFC is indexed by date. DMJD is the
modified Julian date of the observation - this can be found by
looking at the header in an Asterix file of the observation in
the object MORE.ASTERIX.HEADER.BASE\_MJD.

\subsubsection{ENERGY}
The mean photon energy from the source in keV. This is set as a
dynamic default by the program according to the following recipe.

\begin{verbatim}
XRT - energy = 0.2 keV

WFC - S1 = 0.15 keV
    - S2 = 0.09 keV
    - P1 = 0.065 keV
    - P2 = 0.02 keV
\end{verbatim}
These values can be overridden on the command line.

\subsubsection{IFILT}
The filter number in a WFC observation. The filters are defined
as follows:

\begin{verbatim}
 S1A = 4
 S1B = 8
 S2A = 2
 S2B = 6
 P1  = 3
 P2  = 7
\end{verbatim}
\subsubsection{SCOUNT}
The total number of counts from the source in this observation.
This can usually be found accurately enough by using a large box
around the source in ICIRCSTATS and performing a simple background
subtraction.

\subsection{Examples}
To find the optimum radius for sorting data from an on-axis WFC source
observed with the S1A filter :
\begin{quote}\begin{verbatim}
ICL> RADOPT INSTR='WFC' AZIMUTH=0.0 ELEV=0.0 IFILT=4
\end{verbatim}\end{quote}
\section{RATIO}
Finds the ratio of two bands on any axis of an arbitrarily
dimensioned array. This could be used in the following ways:-
\begin{itemize}
\item 1D array, ratioed on ENERGY gives a hardness index for a
spectrum.
\item 2D array, say TIME vs ENERGY. Ratio along TIME to give
relative intensity or along ENERGY to give a time series
of hardness values.
\item 3D array, say TIME vs X,Y. Ratio on TIME to give
relative intensity of two images.
\item 3D array, say ENERGY vs X,Y. Ratio on ENERGY to get
hardness map.
\item 4D array, ENERGY vs TIME vs X,Y. Could get time sequence
of hardness maps ; a relative intensity of two spectral
image ranges and so on.
\end{itemize}
\subsection{Floating Point Divide by 0}
When two ranges of a data object are divided, it is quite
possible that division by zero would occur. In this situation,
RATIO sets the 2nd quality bit in the corresponding pixel of
the output. See under "Quality\_Commands" for more information
about manipulating quality data.

\subsection{Input Output}
Input : Binned dataset, primitive data object.

Output: Binned dataset

\subsection{Method}
Consider Fig (1) below. This is the setup if a 2D array was being
ratioed along its first dimension. N6 and N7 indicate the sizes
of the array in the 7D system.

\begin{verbatim}
    Fig 1:
                +-----------------------------------+  ^
        Row1->  |   |abcdef|            |ghij|      |  |
                |   |      |            |    |      |  N7
                |   |      |            |    |      |  |
                +-----------------------------------+  v
                   /      /            /    /
                 LOW1   HIGH1        LOW2  HIGH2
 \end{verbatim}
The data values in the two ranges are summed, row by row, eg. in
ROW1 the data are summed X = ( a+...+f ) and Y = ( g+...+j ) to
produce arrays of dimensionality one less than the input (Fig 2).
\begin{verbatim}
    Fig 2:          +-+                 +-+         +-+
                    |X|                 |Y|   Y/X   |Z|
                    | |                 | |    =>   | |
                    | |                 | |         | |
                    +-+                 +-+         +-+
  \end{verbatim}
Then these arrays are divided to produce the final output. The
output array is also multiplied by RANGE1 / RANGE2 (the widths of
the ranges selected in the units of the axis) - there might be
too much noise in one range so you can increase the width of the
range to average out the variations. This ensures that the value
of the ratioing process does not depend on small changes in the
width of the range.

\subsection{Normalisation}
To perform the process described under the method sub-topic, RATIO
first removes any axis normalisation present on the axis selected
for RATIOing using the AXIS parameter. Once the two bands have been
collapsed (see Fig 1), then the data may optionally be renormalised
by specifying the NORMAL parameter true.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn  Type       Description
 -------    ----  ----       -----------
 INP         1    CHARACTER  Input dataset. ASTERIX
                             binned dataset or primitive.
    - GLOBAL.BINDS (the current binned dataset)
 AXIS        2    INTEGER    The axis to ratio.
 BANDS       3    CHARACTER  Bounds of two bands on axis selected.
                             These bounds are input according to
                             the convention on range specifications
                             defined under 5_User_Interface 2_Lists
                             and_Ranges.
 OUT         4    CHARACTER  Output binned dataset name.
    -> GLOBAL.BINDS (the current binned dataset)
 NORMAL      -    LOGICAL    Normalise output data?
    Default = NO
 WEIGHT      -    LOGICAL    Controls whether RATIO uses variance if
                             present to create weighted or mean average
                             over each band.
    Hidden Default = NO

\end{verbatim}\subsection{Examples}
RATIO a spectrum in two bands,

\begin{quote}\begin{verbatim}
  ICL> RATIO SPECTRUM
  RATIO Version 1.7-0
  OUT - Output dataset > rat
  Input has valid QUALITY
  Axis  Label              Units             Range

    1   Corrected PHA chan channel no        From 7 to 240

  RATIO will divide two ranges on this  axis - the second by the first.
  BANDS - Two ranges to divide > 50:100 150:220
  NORMAL - Normalise output data /NO/ >
  Unnormalised ratio is 1.053244 +- 0.1195229
\end{verbatim}\end{quote}
Same as above but renormalise,

\begin{quote}\begin{verbatim}
  ICL> RATIO SPECTRUM
  RATIO Version 1.7-0
  OUT - Output dataset > rat
  Input has valid QUALITY
  Axis  Label              Units             Range
    1   Corrected PHA chan channel no        From 7 to 240
  RATIO will divide two ranges on this axis - the second by the first.
  BANDS - Two ranges to divide > 50:100 150:220
  NORMAL - Normalise output data /NO/ > YES
  Normalised ratio is 0.7523173 +- 0.1195229
\end{verbatim}\end{quote}
\section{REBIN}
Performs rebinning of any binned dataset up to 7-D. It has
several operating modes which are selected by the OPT parameter.
In the simplest and default mode a binning ratio is given
for each axis and REBIN groups bins together in this ratio
regardless of whether the input axes are regularly binned or
are irregular. Other modes bin any input data into regular
output bins determined either by the bin width or the total
number of output bins for each axis. More complex modes
allow the user to specify the boundaries for every bin using
the standard format for ranges or by cloning the new axis
values from another dataset. The latter method may be
particularly useful for datasets with large numbers of points
where it would be inpractical to type in the bin bounds at the
prompt. Instead the axis values and widths could be put into
a text file and then IMPORTed to a dummy dataset that REBIN
could then clone the output axes from. Data normalisation with
respect to each axis remains unchanged between input and output.
For datasets of dimension > 1 the user has the option of selecting
which axes are to be rebinned. When using the clone option there
is a restriction on this in that either all or just one axis may
be selected.

The behaviour of the modes specified by OPT is,

\begin{enumerate}
\item Specify a rebinning ratio for each axis (>=1)
eg. if the ratio is 4, then 4 input bins are
collected into each output bin
\item Specify the number of bins for each output
axis, the input data will be distributed into
this number of regular output bins, which may
be greater or smaller than the number input
\item Specify a single, uniform bin width for each
output axis
\item Specify the bounds for each bin of each axis
\item Clone the output axes from another dataset
\end{enumerate}
See also the help on SASSBIN for binning specific to
ROSAT XRT data.

\subsection{Algorithm}
In the simplest mode (OPT=1) data are simply accummulated in
the ratio specified with the outer bin boundaries of each
group retained between input and output. In other modes
REBIN assumes that the input data are uniformly distributed in
each input bin, and donates an appropriate fraction of it to
output bins that overlap. Output bins at the edges which are
are not completely overlapped are not filled. The QUALITY of
each output bin is the logical OR of each contributing input bin.
Any output bin which has received no data has its QUALITY set
to 'Missing-data'.

Where data are normalised with respect to a given axis (as
indicated by the AXIS(n).NORMALISED flag), they are unnormalised
(eg. counts/s becomes counts) before being assigned to an output bin,
and that bin then renormalised when filled.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 INP         1   CHARACTER  Input dataset
    - GLOBAL.BINDS
 OUT         2   CHARACTER  Output dataset.
    -> GLOBAL.BINDS (the current binned dataset)
 OPT         -   INTEGER    Rebinning mode
 AXES        -   CHARACTER  List of axes to rebin
 CLONE       -   CHARACTER  Dataset to clone axes from
 RAT1        -   INTEGER    Rebinning ratio ($>=1$)
 NBIN1       -   INTEGER    Number of bins in this axis
 WID1        -   REAL       Width of (regular) output bins.
 BOUNDS1     -   CHARACTER  Boundaries of irregular output bins.
    .
    .
  RAT7
  NBIN7
  WID7
  BOUNDS7

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> REBIN ds_1 ds_2               - rebin ds_1 into ds_2 prompting
                                     for mode etc.

ICL> REBIN ds_1 ds_2 OPT=1 RAT1=2  - reduce number of bins by half

ICL> REBIN ds_1 ds_2 RAT1=2 \      - same as above

ICL> REBIN ds_1 ds_2 OPT=2         - output to be specified by number
                                     of bins for each axis (prompted for)

ICL> REBIN cube_1 cube_2 OPT=5 AXES=3 CLONE=1D_dataset
                                  - rebin only 3rd axis of cube using
                                    a 1D clone
\end{verbatim}\end{quote}
\section{SCATTERGRAM}
SCATTERGRAM provides, as the name suggests, a plot of one
set of data values against another. Two input objects are
required, either of which may be structured datasets or
primitive data objects. Each input object must have the
same number of values (but not necessarily the same
dimensionality).

Up to 10000 items may be handled. If the input data objects
have more than this, a subset can be selected. The spacing
of items in the input dataset is also selectable : by
default, 1 is used, but any positive value is allowed.

Error data, if present in a structured, is propagated.

NOTE : This routine does not actually plot the output,
but merely prepares a file for plotting with the
graphics commands.
\subsection{Input output data}
Input: 1D to 7D binned dataset or primitive data object
Output: 1D binned dataset
\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 INP1        1   CHARACTER  First input dataset
 INP2        2   CHARACTER  Second input dataset
 SUBSET      3   INTEGER    2 or 3 numbers indicating which
                            input items are to be used.
 OUT         4   CHARACTER  Output dataset
   -> GLOBAL.BINDS (the current binned dataset)
 MARKER      -   LOGICAL    Set output graphics style to
                            markers.
   Hidden default Y

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
    ICL> SCAT
    First input dataset > X.X_RAW
    Second input dataset > X.Y_RAW
    There are 65536 items
    Subset and spacing of items to be used /1,10000,1/ > <RETURN>
    Output dataset > SCATTER1

    ICL> SCATT
    First input dataset > 1,2,3,4,5
    Second input dataset > IMAGE1.AXIS1_DATA
    There are 5 items
    Subset and spacing of items to be used /1,5,1/ > <RETURN>
    Output dataset > SCATTER2

    ICL> SCATTER
    First input dataset > IMAGE1.DATA_ARRAY
    Second input dataset > IMAGE2.DATA_ARRAY
    There are 16384 items
    Subset and spacing of items to be used /1,10000,1/ > 1,16384,2
    Output dataset > SCATTER3
\end{verbatim}\end{quote}
\section{SIGNIF}
Changes the input binned dataset to its significance, weighting
it with the square root of input variance array where present,
or otherwise with a user supplied weights array.

The default output object is a file with the same name as the
input with a higher version number.

\subsection{Input Output data}
Input : Takes any binned dataset and a
primitive array of weight data. These
two must have the same dimensions.

Output : A binned dataset of the same size as
the input. If OVER is specified then
the input is overwritten - otherwise
a new output file is created ( the
default ).

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   CHARACTER Input dataset.
 OUT         2   CHARACTER Output dataset. Only used if OVER is
                           false.
 WEIGHT_DATA 3   REAL      Significance data. Must be primitive
                           object.
 OVER        -   LOGICAL   Overwrite input file?
    Hidden default = FALSE

\end{verbatim}\section{SLICE}
Lists those values in a binned dataset which lie in the specified
data range. Works on datasets of any dimensionality. The destination
of the output is controlled using the standard ASTERIX mechanism
for Text Output which allows output to terminal, file or
hardcopy device.

\subsection{Input Output data}
Input : Takes any binned dataset or primitive array
\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   CHARACTER Input dataset.
 MIN         2   REAL      Lower bound of data range
 MAX         3   REAL      Upper bound of data range
 DEV         -   CHAR        Text Output device.
      Default = 'TERMINAL'

\end{verbatim}\section{SMOOTH}
The program cross-correlates data from a structure of arbitrary
dimensionality (up to a maximum of 4) with a one dimensional,
selectable, mask. Linear correlation is performed in one of the
data dimensions whilst the remaining dimensions are allowed to
run over their full ranges. Optionally this process may be
repeated over a second data dimension (eg an image can be
smoothed in both directions).

Any gaps in the data (i.e. points with quality set bad) are
filled by linearly interpolated values before correlation.
Optionally the gaps may be reinstated afterwards.

The output is the cross-correlated data; where variances are
present these are combined in the usual way to give an estimate
of the statistical fluctuation in the smoothed data, but are
{\em not} used in weighting the smoothing function itself.

\subsection{Ends of Dataset}
To ensure that the smoothing function is applied uniformly over
the whole data array, it is treated differently at the edges.
For instance, at pixel one, half of the smoothing function will
lie outside the array. The ends of the dataset are temporarily
extended by half of the width of the smoothing window while
being smoothed using one of the following user selectable
techniques (controlled by the value of the ENDS parameter),

\begin{description}
\item[ENDAVERAGE]
Extend each end with a local average over n pixels at
the end of each strip.
\item[REGIONAVERAGE]
Extend each end with an average over two user specified
pixel ranges.
\item[ONESPECIFY]
Extend each end with one value
\item[TWOSPECIFY]
Extend each end with a separate value
\item[ENDVALUE]
Extend each end with the first and last values in the
strip.
\end{description}
\subsection{Input \& Outpu}
The input file should be the name of an Asterix file or a primitive
array within such a file. By default the smoothed
result will be copied to a second file along with all the header
information from the first file. The file can be smoothed in situ by
specifying the "OVER" switch on the command line. If a primitive array
is being smoothed then the "OVER" switch {\em must} be used.

\subsection{Masks}
The program provides a choice of the following standard masks,
all of which are normalized to 1.

\subsubsection{COSine}
A cosine mask of 2.2 times the selected width elements, normalised to 1.
\begin{verbatim}
       M(N) = COS ( (1.1 * WIDTH - N) * 2 * PI / WIDTH ) + 1.0
\end{verbatim}
where N = 1.. 2.2*WIDTH.

The whole mask is normalised to 1.

\subsubsection{GAUsian}
A gaussian mask. The user selects the width of the mask which
is the standard deviation of the gaussian (i.e. 1 sigma) and the
length of the mask, which is the total length of the mask.

\begin{verbatim}
       M(N) = EXP (- (LMASK/2 - N)**2 / (2 * WIDTH**2))
\end{verbatim}
where N = 1..LMASK, WIDTH=1 SIGMA, LMASK is the total mask length.

The whole mask is normalised to 1.

\subsubsection{LAPlacian}
A four element filter with values 0.5, 1.0, -0.5, 0.0. This
changes the units of the output to 'Curvature'.

\subsubsection{TOP hat}
A top hat mask normalised by the number of elements in the mask.

\subsubsection{User Defined}
A text (ASCII) file containing a user-defined mask may be used.
Enter the name of the file in responce to the MSK\_MASK prompt.

The file should contain:
\begin{verbatim}
    Line 1   : The number of values in mask.

    Line 2...: The value of each mask element (in free format).
\end{verbatim}
\subsection{Quality}
Pixels with bad quality are always ignored in the smoothing operation
but end up being filled with an interpolated smoothed value. The value
of bad quality pixels can be set to zero in the output file by setting
MSK\_DO=YES. The quality value of a pixel remains the same in the
input and output files. If SMOOTH has been used to interpolate bad
quality pixels then they may be set good using the application
SETQUAL.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn  Type       Description
 -------    ----  ----       -----------
 INFILE       1   CHARACTER  Input dataset
 OUTFILE      2   CHARACTER  Output dataset
 MSK_IDIM     3   INTEGER    Dimension to smooth first
 MSK_IDIM1    4   INTEGER    Optional dimension to smooth next
                             '0' for none)
 MSK_DO       5   CHARACTER  'Y' to reinstate gaps in data  or
                             'N' to leave as interpolated values.
 MSK_MASK     6   CHARACTER  Type of mask (TOP hat, LAPlacian,
                             COSine bell, GAUssian or user file
                             if none of these). Enter first three
                             letters of predefined masks.
 MSK_WIDTH    7   INTEGER    Width of top hat and cosine masks,
                             standard deviation of the Gaussian mask.
                             (Specified in bins, must be = 256).
 MSK_NEWUNITS 8   CHARACTER  Units of user file (eg GRADIENT for
                             gradient mask)
 MSK_LGAU         INTEGER    Total length of Gaussian mask.
 ENDS             CHARACTER  How to process the ends of the dataset?
                             (see ENDS_OF_DATASET entry)
 OVER             LOGICAL    Should the input file be overwritten ?

\end{verbatim}\subsection{Examples}
To correlate a time series with a gaussian mask.
\begin{quote}\begin{verbatim}
ICL> smooth time time_smooth
First dimension to smooth over > 1
Should gaps in the data be set to zero (Y/N) > Y
Mask type > GAU
Mask width > 8
\end{verbatim}\end{quote}
To smooth an image in both dimensions with a Cosine bell
\begin{quote}\begin{verbatim}
ICL> smooth image image_smooth
First dimension to smooth over > 1
Second dimension to smooth ("0" if not wanted) > 2
Should gaps in the data be set to zero (Y/N) > N
Mask type > COS
Mask width > 10
\end{verbatim}\end{quote}
To smooth the variance array of a time series file, overwriting
the original file.
\begin{quote}\begin{verbatim}
ICL> SMOOTH OVER TIME.VARIANCE
\end{verbatim}\end{quote}
\section{SPLINEFIT}
This fits a cubic B-spline to one or two dimension data. It
automatically choses the knot positions and coefficients.

The output is a binned dataset of the same dimensionality as
the input file, with the spline fit instead of data. There are
no bad quality points in the output, all have been fitted by
the spline. The positions of the knots, and the values of their
coefficients are tabulated in the MORE structure of the file.

\subsection{Parameters}
\begin{verbatim}
  Keyword  Posn     Type     Description
  -------  ----     ----     -----------
  INP        1    CHARACTER  Input object
  OUT        2    CHARACTER  Output object
  SQ_RES     3    DOUBLE     Accuracy factor

\end{verbatim}\subsubsection{SQ RES}
The choice of SQ\_RES determines the accuracy of the fit. The
default value given is determined from the variances of the
data. If a tighter fit is required, this value should be
reduced.

\section{SYSERR}
Adds constant percentage error (say E\%) to the input dataset.

New\_var = Old\_var + (E/100 * Data\_Value)**2
In the case where the input dataset had no variance, the result
is as above with Old\_var set to zero.

The default output object is a file with the same name as the
input with a higher version number.

\subsection{Input Output data}
Input : Takes any binned dataset
Output : A binned dataset of the same size as
the input. If OVER is specified then
the input is overwritten - otherwise
a new output file is created ( the
default ).
\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 INP          1  CHARACTER  Input binned dataset
 ERROR        2  REAL       Percentage error to be added
 OUT          3  CHARACTER  Output dataset
   -> GLOBAL.BINDS (the current binned dataset)
 OVER         -  LOGICAL    If NO ( the default ) then a new dataset
                            will be created on output.

\end{verbatim}\subsection{Examples}
Example 1 : Adding an error of 13.45\% to a dataset with no
existing variance. The OVER parameter is used to
force overwriting.

\begin{quote}\begin{verbatim}
ICL> SYSERR orig2 13.45 over
SYSERR Version 1.0-1
Input dataset is DISK$USER1:[DJA.ASTERIX]ORIG2.SDF;1
Data object is of type SIMP
No variance array present - new array created.
\end{verbatim}\end{quote}
Example 2 : Adding an error of 6.7\% to the same dataset as above,
but creating a new output file.
\begin{quote}\begin{verbatim}
ICL> SYSERR ORIG2 6.7 NEWDATA
SYSERR Version 1.0-1
Input dataset is DISK$USER1:[DJA.ASTERIX]ORIG2.SDF;1
Output dataset :-
  File: DISK$USER1:[DJA.ASTERIX]NEWDATA.SDF;1
Data object is of type SIMP
\end{verbatim}\end{quote}
\section{VALIDATE}
Performs basic validation of a binned dataset. Ancilliary
components such as VARIANCE and QUALITY are checked for
dimensional consistency with the DATA\_ARRAY. Checks are
also made to see if the data are stored in the most compact
form, eg. regular axes stored as a spaced array. Redundent
items such as QUALITY all set to good are removed as are non
legal objects in the top level. VARIANCE is checked for
negative values and where these are found the QUALITY is
set to temporary bad (most significant bit of QUALITY byte
is set). The command RESTORE can be used to reset this if
the user wishes to correct the condition and use the affected
data points. Everything below the MORE level is copied
unchecked and unchanged.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 INP          1  CHARACTER  Input  dataset
 OUT          2  CHARACTER  Output dataset
   -> GLOBAL.BINDS (the current binned dataset)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> VALIDATE input_ds valid_ds
\end{verbatim}\end{quote}
\chapter{Parameter \& History commands}
HIST and HISTMODE manipulate history. GLOBAL and SETRANGES
alter the global parameters.

NOTE : Neither GLOBAL nor SETRANGES is available from the shell.

\section{GLOBAL}
This can be used to display the current values of the global
parameters. GLOBAL does not have any parameters.

GLOBAL is only available under ICL.

\section{HISTMODE}
Alters or displays the history update mode associated with
a dataset. History is written by most ASTERIX applications,
but there are some circumstances in which it is desirable
to control the level of output. There are 4 such levels,
name DISABLED, QUIET, NORMAL and VERBOSE. ASTERIX only uses
the first 3 of these, NORMAL being the default behaviour.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   UNIV      The input container dataset.
            GLOBAL.BINDS (Default binned dataset)
 DISABLED    -   LOGICAL   Set update mode to DISABLED
    Hidden default = NO
 QUIET       -   LOGICAL   Set update mode to QUIET
    Hidden default = NO
 NORMAL      -   LOGICAL   Set update mode to NORMAL
    Hidden default = NO
 VERBOSE     -   LOGICAL   Set update mode to VERBOSE
    Hidden default = NO

\end{verbatim}\subsection{Update Modes}
The exact behaviour if the 4 different modes is,
DISABLED - No changes are made to the history by
any ASTERIX application
QUIET - Only program version ids, time and cpu
stamps are written
NORMAL - Normal level of history update.
Applications attempt to write all
information required to recreate output
VERBOSE - Same as NORMAL for ASTERIX applications,
but other Starlink s/w may perform a
higher level of update than NORMAL.
\subsection{Examples}
Specifying no keywords means the mode is printed

\begin{quote}\begin{verbatim}
ICL> HISTMODE SPECTRUM
HISTMODE Version 1.7-0
History update mode is NORMAL
\end{verbatim}\end{quote}
Name the mode to change it,

\begin{quote}\begin{verbatim}
ICL> HISTMODE SPECTRUM VERBOSE

ICL> HISTMODE SPECTRUM
HISTMODE Version 1.7-0
History update mode is VERBOSE
\end{verbatim}\end{quote}
\section{HIST}
Displays the history information within an event or binned
dataset.

HIST uses the DEV parameter to direct text output.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   UNIV      The input container dataset.
            -GLOBAL.BINDS (Default binned dataset)
 DEV         2   CHAR      text output device. Can take values of
                           (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                           or (N)EWFILE, otherwise is interpreted
                           as a filename.
    Default 'TERMINAL'
 LINES       3   INT       Maximum number of lines of text per record
    Hidden default = 200

\end{verbatim}\subsection{Examples}
List the history in a file to the screen,
\begin{quote}\begin{verbatim}
HIST Version 1.8-1

History of  : /datan5/asterix/spec.sdf
              SPEC
Created     : 25-NOV-92 15:39:36
Update mode : NORMAL
Contains    : 19 records

Record 1:

  Creator: XSORT Version 1.6-1
  Date:    25-NOV-92 15:39:36
  Host:    unknown

...

Record 19:

  Creator: ARITHMETIC Version 1.8-1
  Date:    24-Apr-95 09:48:59
  Host:    xun9
  Text:    Operation: /
           Dataset was overwritten
           Input 1 : File : /datan5/asterix/spec.sdf
           Input 2 : File : /base/asterix/adam/arithmetic.sdf
                     Object : ARITHMETIC.INP2
             value = 1.02023

\end{verbatim}\end{quote}
As above but send the output to the printer and limit the text to 5 lines
per record,
\begin{quote}\begin{verbatim}
> hist spec dev=printer lines=5 \
\end{verbatim}\end{quote}
\section{SETRANGES}
SETRANGES sets up ranges in one domain according to value ranges
in a parallel domain.

It is best understood by an example of its use,
\begin{verbatim}
      value object:     3,5,8,6,0,0,3,4,7,2
      range object:     1,2,3,4,5,6,7,8,9,10
      Limits:           4,7
\end{verbatim}
resulting in range values 2,2,4,4,8,9.

The resulting vector of ranges is written to the global RANGES
parameter from where it can be picked up by, for example, BINSUBSET.

In practice SETRANGES might be used on a time series to find all
the values of time (the range object) when the counts (the
value object) fall within certain limits (the LIMITS).

Another example is that one could find all the times
when Quality is set to any given
value or range of values (LIMITS).

NOTE: The value limits specified are {\em inclusive} (see
example above). It is therefore possible to
select slices where the free parameter has a
single value rather than a range of value by
entering the same value as both upper and lower
limit of LIMITS.
\subsection{Input Output Data}
Input: two primitive objects

Output: parameter holding RANGES array

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn  Type        Description
 -------    ----  ----        -----------

 VALUE_OBJECT 1   CHARACTER   Array of data from which range of
                              values will be selected.
 RANGE_OBJECT 2   CHARACTER   Array of data corresponding to data in
                              Value_object.
 LIMITS       3   REAL        Array(size 2) containing lower and
                              upper bounds of the values of
                              Value_object being sought.
 GLOBAL       4   REAL        Vector of range values written to global
                              parameter.
    -> GLOBAL.RANGES

\end{verbatim}\subsection{Data}
RANGE\_OBJECT and VALUE\_OBJECT must be 1D arrays of primitive data
types. They can be entered from the terminal if desired. The two
arrays must be of equal length.

\subsection{Deficiencies}
Note the program can only output up to 500 slices of data. If
there are more than 500 sets of start and end times found in the
RANGE\_OBJECT the program stops after putting the 500 sets into
RANGES and outputs a warning message.

\subsection{Errors}
If the RANGE\_OBJECT and VALUE\_OBJECT are of different lengths
then a warning is printed out and the program stops.

\chapter{Event dataset commands}
These are commands which require event datasets as their input
file(s), or create them from scratch.

\section{EVBIN}
Converts EVENT datasets, produced by instrument specific
software, into BINNED datasets for use with ASTERIX applications.

The user must select which LIST's within the EVENT dataset are to
be binned, each LIST selected becoming a dimension of the output
binned dataset (maximum of 7). Separate entries using a space
e.g. '1 2 3'.

Each axis may be either regularly or irregularly binned.

There are 3 parameters OPT1, OPT2 and OPT3 which are used to reduce
the number of questions asked of the user. See the OPTIONS
subtopic.

\subsection{Input Output}
Input: EVENT dataset

Output: BINNED dataset, of up to 7 dimensions.
eg TIME\_SERIES, IMAGE, SPECTRUM, etc...

\subsection{Options}
There are three parameters, OPT1, OPT2 and OPT3 designed to reduce
the number of prompts, and thus make input easier.

\begin{description}
\item[OPT1]
If YES then you will be asked whether each axis is to
be regularly or irregularly binned.
If NO then all axes will be regularly binned.
The hidden default is NO.
\item[OPT2]
If YES then you will be asked how each regular output
axis is to be specified.
If NO they will all be specified in the same way,
determined by OPT3.
The hidden default is NO.
\item[OPT3]
If YES then all regular axes must be specified by the
number of output bins.
If NO then all regular axes must be specified by
their bin width.
The hidden default is NO.
\end{description}

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type       Description
 -------  ----   ----       -----------
 INP         1   UNIV       Input event dataset.
            -GLOBAL.EVDS   (Default event dataset)
 OUT         2   UNIV       Name of the output binned dataset.
            ->GLOBAL.BINDS  (Default binned dataset)
 LISTS       3   CHAR       Index numbers of the lists to become output
                            dimensions. Separate entries
                            with spaces e.g. 1 2 3
 OPT1        -   LOGICAL    Allow irregular binning?
    Hidden default = NO
 OPT2        -   LOGICAL    Individually select how regular axes
                            are to be specified?
    Hidden default = NO
 OPT3        -   LOGICAL    Specify all regular axes by the No of
                            bins?
    Hidden default = NO

 The following 2 parameters only apply if a QUALITY list has been
 selected.
 QVAL        -   INTEGER    Events with QUALITY greater than QVAL
                            will be treated as BAD QUALITY data.
    Hidden default = 0
 QKEEP       -   LOGICAL    Write BAD QUALITY data to output
                            dataset?
    Hidden default = NO

 The following parameters are repeated for each output dimension
 (i.e. n = 1..7)
 REGn        -   LOGICAL    Is the nth list selected to be regularly
                            binned? Only asked if OPT1=YES
 USEBINSIZEn -   LOGICAL    Specify binning of the nth list by
                            bin width? Only asked if OPT2=YES,
                            and is regularly binned.
 BASEn       -   REAL       Lower bound of lowest bin. By default the
                            data minimum, but can be set to any value.
 BINSIZEn    -   REAL       Binsize for the nth list. Only asked if
                            regularly binned, and
                            binning specified by bin width.
 NBINSn      -   INTEGER    No of bins for the nth list. Only asked if
                            regularly binned, and
                            binning specified by No of bins.
 RANGESn     -   CHARACTER  Bin ranges for the nth list.
                            Only asked if irregularly binned.

\end{verbatim}\subsection{Quality Lists}
Some instrument interfaces produce a QUALITY list in their event
datasets, containing information on the validity of the events.

Quality lists will be ignored by EVBIN unless specifically
selected in response to the LISTS prompt.

They are an exception to the general rule and will NOT become a
dimension of the output dataset. The output dataset may, however,
contain a QUALITY component.

There are two parameters associated with the handling of QUALITY
lists:

\begin{description}
\item[QVAL]
Events with a QUALITY value greater than QVAL are
treated as bad quality events. The hidden default
value is 0.
\item[QKEEP]
If YES then any bad quality events are written to the
output dataset. In addition, a QUALITY component is
created in the output dataset, and the quality of bins
containing one or more bad quality events is set to
bad (i.e. 1).
If NO, then the bad quality events are simply
thrown away. No QUALITY component is created in the
output dataset.
The hidden default value is NO.
\end{description}
\section{EVCSUBSET}
This produces one EVENT dataset from another, having selected
EVENTS on the basis of their X and Y positions.

Data can be selected to lie within either a circular or annular
region around a user defined central point. The user is asked
which type of 'aperture' is required (Annular or circular), the
(X,Y) coords of the central position, the outer radius of the
region (for a circular region this is just the radius of the
'aperture') and, for an annular aperture, the inner radius.
The units used are those of the X and Y LISTs in the event
dataset.

\subsection{Input output data}
Input: Event dataset
Must have X and Y Lists (Either X\_RAW and Y\_RAW or
X\_CORR and Y\_CORR. If both present, uses the latter.

Output: New event dataset

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn    Type      Description
 -------   ----    ----      -----------
 INP         1     UNIV      Input Dataset
              -GLOBAL.EVDS (Default event dataset)
 OUT         2     UNIV      Dataset for output
              ->GLOBAL.EVDS (Default event dataset)
 ANNULUS     -     LOGICAL   YES = annular, NO = circular region
        Hidden default = NO
 XCENT       3     REAL      x-coord of centre of region
 YCENT       4     REAL      y-coord of centre of region
 OUTER       5     REAL      Outer radius of region
 INNER       6     REAL      Inner radius of annular region

\end{verbatim}\subsection{Examples}
Select circular region from EVDS1 centred on
(234.5,506.7) and with radius 25 and put selected
events in dataset SOURCE,
\begin{quote}\begin{verbatim}
ICL> EVCSUB EVDS1 SOURCE 234.5 506.7 25
\end{verbatim}\end{quote}
Select an annular region of width 10 (=35-25)
around the outside of previous region and put
events in dataset BCGND,
\begin{quote}\begin{verbatim}
ICL> EVCSUB EVDS1 BCGND 234.5 506.7 ANNULUS 35 25
\end{verbatim}\end{quote}
\section{EVLIST}
Displays the DATA\_ARRAY component of all the LISTS in an event
dataset.

\subsection{Input output data}
Input: Event dataset
Output: to user-specified device
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type      Description
 -------  ----   ----      -----------
 INP        1    UNIV      Input dataset.
           GLOBAL.EVDS (Defult event dataset)
 SUBSET     2    INTEGER   Subset of items to be printed.
 DEV        -    CHAR      Select output device (P = printer,
                           N = New file, O = Old file,
                           Anything else = filename).
    Hidden default 'TERMINAL'

\end{verbatim}\section{EVMERGE}
EVMERGE concatenates two or more event datasets. Only those lists
which are common to all the inputs appear in the output.

Specify each input dataset in turn escaping with a '!'.

EVMERGE reports discrepancies between the 'headers' of the input
datasets but it is YOUR RESPONSIBILITY to determine whether these
will affect the integrity of the resultant dataset.

EVMERGE propagates LIVE\_TIME information and sorts this into order
of increasing ON time. The user should decide if the resultant
LIVE\_TIME component is sensible or not. If unsure, bin and
exposure correct the datasets before merging.
\subsection{Input output data}
Input: 1 to 10 event datasets
Output: Event dataset
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn     Type     Description
 -------  ----     ----     -----------
 INP1..10   -    UNIV       Input datasets.
         Default='!' for no more datasets
 OUT        1    UNIV       Output dataset.
            -> GLOBAL.EVDS (Default event dataset)

\end{verbatim}\subsection{Method}
Each of the user-supplied list of datasets is checked against the
first to see that its structure is similar. The output dataset is
formed using the total lengths of the lists found in the input
datasets.

Datasets with different pointing directions can be merged - the
output file will have the orientation of the first image. Only the
X\_CORR and Y\_CORR lists are affected (if present).

Most header components (and the instrument response) are copied
from the first input dataset. The exceptions are:


\begin{description}
\item[BASE\_MJD,BASE\_UTC,BASE\_DATE,BASE\_TAI :]
These 4 header parameters, and all other timing information in
the input datasets are adjusted to the origin defined by the
dataset with the earliest BASE\_TAI.
\item[EXPOSURE\_TIME, OBS\_LENGTH :]
Are calculated from the sums of the values in the input datasets.
Only written to output if present in all inputs.
\item[LIVE\_TIME:]
Live time slots from the inputs are merged and sorted into time
order. Any duplicate slots are removed.
\end{description}
\section{EVPOLAR}
Converts EVENT datasets, produced by instrument specific software,
into polar BINNED datasets.

The user must select which LIST's within the EVENT dataset are to
be used in the rectangular to polar conversion. A pole is specified
in the same units as these lists. The direction of increase of the
first list defines zero of azimuth, and the direction of increase of
the second list defines +90 degrees.

The polar bins are then defined. Only regular azimuthal bins are
allowed, but either regular or irregular radial bins are permitted.

The output dataset may optionally be normalised to unit bin area.

\subsection{Input Output}
Input: EVENT dataset

Output: BINNED dataset, of 1 or 2 dimensions, of type POLAR

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type       Description
 -------  ----   ----       -----------
 INP         1   UNIV       Input event dataset.
             -GLOBAL.EVDS (Default event dataset)
 OUT         2   UNIV       Name of the output binned dataset.
            ->GLOBAL.BINDS (Default binned dataset)
 LISTS       3   CHAR       Index numbers of the lists to use in the
                            polaring. Separate by spaces, eg. 1 2
 X0, Y0      -   REAL       The coordinates of the pole in the same
                            units as the selected axes.
       Defaults 0,0
 The following 2 parameters only apply if a QUALITY list is present
 in the input.
 QVAL        -   INTEGER    Events with QUALITY greater than QVAL
                            will be treated as BAD QUALITY data.
     Hidden default = 0
 QKEEP       -   LOGICAL    Write BAD QUALITY data to output
                            dataset?
    Hidden default = NO

 ABINSIZE    -   REAL       The azimuthal bin size in degrees.
         Default 360
 REG         -   LOGICAL    Specify regular radial bins?
    Default YES
 RBINSIZE    -   REAL       Radial bin size if REG is YES. In same
                            unit as input lists
 NRAD        -   INTEGER    Number of radial bins. The first bin
                            has a lower bound of zero. Only used if
                            REG is YES.
 RRANGE      -   CHAR       Used if REG is NO. A list of radial
                            bin boundaries. See ASTHELP USER_INTERFACE
                            LIST_AND_RANGES for syntax.
 NORM        -   LOGICAL    Normalise output array - ie. divide by
                            bin area.
     Default YES

\end{verbatim}\subsection{Quality Lists}
Some instrument interfaces produce a QUALITY list in their event
datasets, containing information on the validity of the events.

There are two parameters associated with the handling of QUALITY
lists:


\begin{description}
\item[QVAL]
Events with a QUALITY value greater than QVAL are
treated as bad quality events. The hidden default
value is 0.
\item[QKEEP]
If YES then any bad quality events are written to the
output dataset. In addition, a QUALITY component is
created in the output dataset, and the quality of bins
containing one or more bad quality events is set to
bad (i.e. 1).
If NO, then the bad quality events are simply
thrown away. No QUALITY component is created in the
output dataset.
The hidden default value is NO.
\end{description}
\subsection{Examples}
Example 1 : Regular radial bins
\begin{quote}\begin{verbatim}
 ICL> evpolar ev
 EVPOLAR Version 1.4-0
 The available lists are :
  1  X_CORR
  2  Y_CORR
 Select 1 or 2 lists to bin using index numbers above, eg. 1 2.
 The direction of zero azimuth is that of increasing list 1 value.
 LISTS - Index numbers > 1 2
 X0 - X axis value of pole /0/ > 5
 Y0 - Y axis value of pole /0/ > 5
 ABINSIZE - Azimuthal binsize (degrees) /360/ > 90
 There will be 4 azimuthal bins in the output
 REG - Radial axis to be regularly spaced /YES/ >
 Radial distance varies from 0 to 94.14188 arcmin
 RBINSIZE - Radial binsize > 10
 NRAD - Number of radial bins /9/ >
 OUT - Enter name of the output dataset > pol1
 2497 events were binned out of 2514 input.
 NORM - Normalise output array /YES/ >
\end{verbatim}\end{quote}
Example 2 : Irregular radial bins
\begin{quote}\begin{verbatim}
 ICL> evpolar ev
 EVPOLAR Version 1.4-0
 The available lists are :
  1  X_CORR
  2  Y_CORR
 Select 1 or 2 lists to bin using index numbers above, eg. 1 2.
 The direction of zero azimuth is that of increasing list 1 value.
 LISTS - Index numbers > 1 2
 X0 - X axis value of pole /0/ > 5
 Y0 - Y axis value of pole /0/ > 5
 ABINSIZE - Azimuthal binsize (degrees) /360/ >
 There will be 1 azimuthal bins in the output
 REG - Radial axis to be regularly spaced /YES/ > n
 Radial distance varies from 0 to 94.14188 arcmin
 You must specify INCREASING ranges e.g. 0:10:20
 RRANGE - Enter irregular bin ranges > 5:10:20:40
 OUT - Enter name of the output dataset > pol2
 718 events were binned out of 2514 input.
 NORM - Normalise output array /YES/ > n
\end{verbatim}\end{quote}
\section{EVSIM}
Generates simulated event datasets using a psf of the users choice.
Two methods of defining the size of the simulated field and the
background are available,
\begin{itemize}
\item The background can be flat (including zero) and the field size
and pixel quantum defined explicitly.
\item Any 2-dimensional dataset can be read in as a model. The model
is normalised to create a cumulative probability distribution
which is then used to generate a simulated background. The
field size is the same as that of the input dataset.
\end{itemize}
After defining the background, as many as 50 sources may be added.
The total number of events is limited only by the users page file
quota.

NOTE : EVSIM only generates position lists - no timing information
is produced. A time series simulator is available called
TIMSIM.

\subsection{Flat Background}
The background photons are distributed randomly across the image
using the standard fortran random number generator - seeded using
the system clock.

\subsection{Background Model}
A two dimensional dataset is read in. After normalisation to unity,
the dataset is summed to create a cumulative probability distribution.
Each background photon has a random number generated between zero
and one which defines a bin in the distribution - this becomes the
photon's position.

\subsection{Sources}
If sources are to be added to the output dataset, EVSIM associates
a psf with the dataset using the PSF system (see "User Interface" for
more details). The psf may be assumed to be constant across the
field (the default), or may vary by specifying the PSFCON keyword
NO. The user supplies a position and count for each source.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn     Type     Description
 -------  ----     ----     -----------
 OUT        1    CHAR       Dataset into which simulated
                            events will be put
            -> GLOBAL.EVDS (Default event dataset)
 MODEL      -    UNIV       A background model
      Default !
 FIELDSIZE  -    REAL       The full width of the field in arcmin
          Default 60.0
 PIXSIZE    -    REAL       The quantum for the event lists values
          Default 1.0
 SEED       -    CHAR       The seed for the random number generator.
                            If specified, EVSIM will not poisson
                            deviate the source and background counts.
                            This provides a method of re-creating a
                            previously constructed dataset.
     Hidden default !
 BACK       -    INTEGER    Number of background counts
     Default !
 SOURCEC    -    INTEGER    A list of source counts - separate by
                            spaces or commas.
        Default !
 SOURCEP    -    REAL       A list of source positions - enter
                            x,y consecutively separated by spaces
                            or commas.
        Default 0,0
 PSFCON     -    LOGICAL    Constant psf across field
      Default YES
 PSF..      -    CHAR       The psf to use to generate the data.
                            See under "User Interface" for more
                            information about the psf system.

 NFILE      -    INTEGER    Number of files to create.
      Hidden default 1
 FFILE      -    INTEGER    Index of first file.
      Hidden default 1
 SRADIUS    -    INTEGER    Half width of psf (in pixels).
        Default 15
 WIDTHS     -    REAL       Width of gaussian for psf.
        Default 0

\end{verbatim}\subsection{Example}
The example below was produced using a gaussian point response,
for two sources of strengths 20 and 40 counts, centred at (-10,-10)
and (15,20) respectively.
\begin{quote}\begin{verbatim}
ICL> EVSIM
  EVSIM Version 1.2-0
  OUT - Name of output event dataset > EV
  FIELDSIZE - Width of field in arcminutes /60/ >
  PIXSIZE - Pixel size in arcminutes /0.5/ >
  BACK - Number of background counts > 100
  SOURCEC - Number of counts per source > 20 40
  SOURCEP - Source positions /0,0/ > -10,-10,15,20
  There will be 94 background counts
  Source no. 1 has 19 counts
  Source no. 2 has 36 counts
  The PSF system can provide :

    ANAL              EXOLE             PWFC              WFC

  PSF - Choose PSF for simulated sources /'ANALYTIC'/ >
  No axis data present - using pixels
  PSF definitions available

  GAUSSIAN  - Gaussian response   TRIANGLE     - Triangular response
  TOPHAT    - Tophat function     FLAT_TRI     - Triangle with flat top

  MASK - Name of profile to use (select from above) /'GAUSSIAN'/ >
  Dataset pixels are 0.5 arcmin square
  AUX - Gaussian FWHM in dataset pixels > 5
\end{verbatim}\end{quote}
The event dataset is now binned up using EVBIN
\begin{quote}\begin{verbatim}
ICL> EVBIN
  EVBIN Version 1.2-0
  INP - Enter name of input dataset /@EV/ >
  The available lists are:
   1  X_CORR
   2  Y_CORR

  Select the lists to be binned, by entering the index numbers.
  E.g. 1 2 3
  LISTS - Index numbers > 1 2

   X_CORR axis:
  The X_CORR data range is -30.25 to 30.25
  The intrinsic width is 0.5.
  BINSIZE1 - Enter binsize /0.5/ >
  This will give 121 X_CORR bins
   Y_CORR axis:
  The Y_CORR data range is -30.25 to 30.25
  The intrinsic width is 0.5.
  BINSIZE2 - Enter binsize /0.5/ >
  This will give 121 Y_CORR bins
  OUT - Enter name of the output dataset > IM
  A total of 149 events were binned
\end{verbatim}\end{quote}
\section{EVSORT}
This produces one EVENT dataset from another, having sorted the
events into either ascending or descending order by the value in
one of the lists.
\subsection{Input output data}
Input: Event dataset

Output: New event dataset

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn     Type     Description
 -------  ----     ----     -----------
 INP        1      UNIV     Dataset from which selection will be
                            made.
            -GLOBAL.EVDS (Current event dataset)
 OUT        2      UNIV     Dataset into which selected items
                            will be put.
            ->GLOBAL.EVDS  (Current event dataset)
 SLIST      3      CHAR     Name of list to be used in sort.
                            RAW_TIMETAG is the default supplied
                            if present.
 ASCEND     -      LOGICAL  Sort into ascending numerical order?
                            Specify false to sort into descending
                            order.
   Default = YES

\end{verbatim}\section{EVSUBSET}
This produces one EVENT dataset from another, having applied some
constraint to the data.

Two forms of constraint are implemented. The first is application
of allowed ranges in value of user selected LIST's present in the
original EVENT dataset. Up to 10 LISTS can simultaneously have
ranges applied. The second form of constraint (selected using the
INDEX keyword) is simply to select events on their ordinal position
within the event list.

In either form of constraint multiple ranges may be specified.
Multiple ranges must be placed in increasing numerical order e.g.
the start value of range 2 must be greater than the end value of
range 1.

The user may specify whether the range(s) specified for each
LIST are to be kept, or to be thrown away. By default the
ranges entered are to be kept. Up to 500 ranges may be specified
for each selected LIST.

The following rules apply to the ranges from which data will be
copied to the output: the start value is inclusive; and the end
value exclusive.

Data falling within a specified range of each of the selected
lists are written to the output event dataset.

\subsection{Input output data}
Input: Event dataset
Output: New event dataset
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn     Type     Description
 -------  ----     ----     -----------
 INP        1      UNIV     Dataset from which selection will be
                            made.
            -GLOBAL.EVDS (Current event dataset)
 OUT        2      UNIV     Dataset into which selected items
                            will be put.
            ->GLOBAL.EVDS  (Current event dataset)
 LISTS      3      CHAR     Indices of LISTS to have ranges
                            applied.
 INDEX      -      LOGICAL  Operate by chosing event number
                            rather than list value.
    Hidden default = NO
 EVENTS     -      CHAR     Range specification describing the
                            ordinal values of the events to be
                            retained in the output dataset.

  The following parameters are repeated for n = 1..10, ie once
  for each of the 10 simultaneously selectable LISTs.
 KEEPn      -    LOGICAL    Keep specified ranges?
    Hidden default = YES
 RANGESn  n + 3    CHARACTER  Range specification(s)

\end{verbatim}\chapter{Quality processing commands}
These commands handle the representation of data quality. The default
way of specifying data quality within ASTERIX is by explicit QUALITY
values separate from the data values. These QUALITY values are stored
in an unsigned byte in accordance with SGP38. The meaning of each bit
is given in the section 'Quality\_values' below. The default behaviour
in ASTERIX is for data to be considered bad if any QUALITY bit is set.
It is possible to alter this behaviour by applying a MASK to the QUALITY.
See the section 'Masking\_quality' below. In general data points with bad
QUALITY are ignored by ASTERIX. However, in some cases this is not
possible because ASTERIX is using external software which is unaware of
the QUALITY values. Two dimensional graphics is an example. In this
case it is possible to exclude bad data by using the magic value
representation. See the section on 'Quality\_handling' in GRAFIX help
and the section 'Magic\_values' below. Other packages (eg. KAPPA) use
the magic value representation by default and the 'Magic\_values' section
explains how to transport datasets between ASTERIX and such packages.

\section{Quality values}
QUALITY is stored in an 8-bit unsigned byte, where each bit gives a
separate description of the quality of the associated data value. The
meaning of each bit in ASTERIX is as follows:

\begin{verbatim}
     Bit  Meaning
      0    bad           -  unspecific bad quality
      1    missing       -  no data value at this point
      2    arith         -  arithmetic fault, eg. / by zero
      3       *** not currently used ***
      4       *** not currently used ***
      5       *** not currently used ***
      6    patched       -  indicates data patched/interpolated
      7    ignore        -  used to temporarily exclude points
\end{verbatim}
\section{Masking quality}
To determine whether a data point has bad quality, its QUALITY value
is compared with a MASK and if any bit is set in both QUALITY and MASK
then the overall quality is considered bad. By default the MASK is taken
as 11111111, so any bit set in QUALITY will give bad overall quality.
It is possible to attach a different MASK to a dataset (eg. by using
the MASK command) and for this MASK to then be used to determine
quality. For example, if some data points have been temporarily
excluded by using the IGNORE variant of the QUALITY command, they could
be switched back in by setting the MASK to 01111111, rather than by
resetting all the QUALITY values with RESTORE. Setting the MASK to
00000000 will give all points good quality.

\section{Magic values}
The magic value representation of data quality actually replaces the
bad data value with a specific constant (which is machine dependent).

If you wish to export an ASTERIX dataset to a package like KAPPA which
only handles magic values then you should first operate on the dataset
with the command MAGIC.

In order to import a dataset containing magic values from a package
such as KAPPA, you should use QUALITY in SET mode (ie. SETQUAL) with
the selection keyword MAGIC.

\section{CQUALITY}
CQUALITY sets or modifies quality in circular regions of a dataset
with 2 or more dimensions. The centres and radii of these regions
apply to the two spatial dimensions of the dataset. CQUALITY assumes
that the units of the two spatial axes are identical.

It must be invoked with either a logical keyword on the command line
to specify the mode of operation, or using the PROMPT keyword. The
modes available are:
\begin{verbatim}
  CQUALITY IGNORE  - Sets the tempory bad quality bit,     (=CIGNORE)
                     leaving the rest unaltered.
  CQUALITY RESTORE - Clears the temporary bad quality bit, (=CRESTORE)
                     restoring the original quality value.
  CQUALITY SET     - Sets quality to a specified value.    (=CSETQUAL)
  CQUALITY NOT     - Complements existing quality values.
  CQUALITY AND     - ANDs existing quality with specified
                     value.
  CQUALITY OR      - ORs existing quality with specified
                     value.
  CQUALITY EOR     - EORs existing quality with specified
                     value.
\end{verbatim}
\subsection{Axis Ordering}
CQUALITY will work most efficiently if the axis ordering is X,Y...,
but any order is acceptable. See the "User\_Interface" topic,
"Axis\_Ordering" subtopic for more information.

\subsection{OverWriting}
Like most applications the default value of the OVER
parameter is FALSE.

You must specify OVER on the command line if you wish to
overwrite the original dataset.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Input dataset.
          GLOBAL.BINDS (Default binned dataset)
 OUT       2    UNIV       Output dataset
           ->GLOBAL.BINDS (Default binned dataset)
 CX        -    REAL       X centre(s) of circular regions
 CY        -    REAL       Y centre(s) of circular regions
 CR        -    REAL       Radius/radii of circular regions.
 OVER      -    LOGICAL    Overwrite input dataset?
    Hidden default = NO
 QSEL      -    LOGICAL    Specify existing quality value?
    Hidden default = NO
 MODQUAL   -    CHAR       Existing quality value to alter.
                           (asked for if QSEL = YES)
 QVAL      -    CHAR       Specified quality value (for SET, AND,
                           NAND, OR, and EOR modes)
  ONE and ONLY ONE of the following modes MUST be selected:
 IGNORE    -    LOGICAL    Ignore mode?
    Hidden default = NO
 RESTORE   -    LOGICAL    Restore mode?
    Hidden default = NO
 SET       -    LOGICAL    Set mode?
    Hidden default = NO
 AND       -    LOGICAL    AND mode?
    Hidden default = NO
 OR        -    LOGICAL    OR mode?
    Hidden default = NO
 EOR       -    LOGICAL    EOR mode?
    Hidden default = NO
 NOT       -    LOGICAL    NOT mode?
    Hidden default = NO

\end{verbatim}\subsection{ShortForms}
Three shortform commands have been defined, with particular
modes specified.

\begin{description}
\item[CIGNORE]
Invoke CQUALITY in IGNORE mode, and
overwrite the input dataset.
\item[CRESTORE]
Invoke CQUALITY in RESTORE mode, and
overwrite the input dataset.
\item[CSETQUAL]
Invoke CQUALITY in SET mode.
\end{description}
All keywords are the same as for CQUALITY, but obviously you don't
define a mode!

Because CIGNORE and CRESTORE leave the quality value unaltered (apart
from bit 8 which is only used for this purpose in ASTERIX) they are
invoked to OVERwrite the input dataset.

\subsection{Examples}
Ignore two regions of a dataset at (0,0) and (10,10), both of radius 5.
\begin{quote}\begin{verbatim}
 > CIGNORE CUBE
 CQUALITY Version 1.4-0
 OUT - Output dataset > CUBEQ
 Specify circular regions in ARCMIN
 CX - X centre(s) of circular regions > 0,10    ;  Use axis units here
 CY - Y centre(s) of circular regions > 0,10
 CR - Radius of circular regions > 5
 15040 points had quality modified
\end{verbatim}\end{quote}
Restore quality in the (10,10) circle
\begin{quote}\begin{verbatim}
ICL> CRESTORE NBLOBQ
CQUALITY Version 1.4-0
Specify circular regions in ARCMIN
CX - X centre(s) of circular regions > 10
CY - Y centre(s) of circular regions > 10
CR - Radius of circular regions > 5
7520 points had quality modified
\end{verbatim}\end{quote}
Set quality in a dataset whose axes are not ordered X,Y, but T,X,Y.
\begin{quote}\begin{verbatim}
ICL> CSETQUAL CCF CCFQ
CQUALITY Version 1.4-0
Changing axis order to X,Y...
Specify circular regions in ARCMIN
CX - X centre(s) of circular regions > 0
CY - Y centre(s) of circular regions > 0
CR - Radius of circular regions > 5
QVAL - New quality value > ARITH
Restoring axis order...
7520 points had quality modified
\end{verbatim}\end{quote}
\section{CIGNORE+}
CIGNORE is a shortform variant of CQUALITY, used to set the
temporary bad quality data flag of points within a binned
dataset (most significant bit of the QUALITY byte).

\begin{quote}\begin{verbatim}
CIGNORE = CQUALITY IGNORE OVER
\end{verbatim}\end{quote}
As with CQUALITY the user is able to select circular regions and
specify a particular existing quality value to choose the points
in the dataset to be effected.

See CQUALITY for full details of the parameters and their keywords.

\section{CRESTORE+}
CRESTORE is a shortform variant of CQUALITY, used to clear the
temporary bad quality data flag of points within a binned
dataset (most significant bit of the QUALITY byte).
\begin{quote}\begin{verbatim}
CRESTORE = CQUALITY RESTORE OVER
\end{verbatim}\end{quote}
As with CQUALITY the user is able to select circular regions and
specify a particular existing quality value to choose the points
in the dataset to be effected.

See CQUALITY for full details of the parameters and their keywords.

\section{CSETQUAL+}
CSETQUAL is a shortform variant of CQUALITY, used to set
quality to a particular value.
\begin{quote}\begin{verbatim}
CSETQUAL = CQUALITY SET
\end{verbatim}\end{quote}
As with CQUALITY the user is able to select circular regions and
specify a particular existing quality value to choose the points
in the dataset to be effected.

See CQUALITY for full details of the parameters and their keywords.

\section{MAGIC}
This command sets a magic value in the data for points with bad
QUALITY. Both the default magic value and quality mask may optionally
be overidden. By default a new dataset is created, but the magic
values can optionally be written into the existing one.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Input dataset.
           - GLOBAL.BINDS(Default binned dataset)
 OUT       2    UNIV       Output dataset.
           -> GLOBAL.BINDS(Default binned dataset)
 OVER      -    LOGICAL    Overwrite input dataset?
      Hidden default = NO
 MAGIC     -    REAL       Magic value to use
      Hidden default = !
 MASK      -    CHAR       Quality mask
      Hidden default = mask currently attached to dataset
                     or 11111111 if not present

\end{verbatim}\subsection{Examples}
Copy dataset and write standard magic values into output
\begin{quote}\begin{verbatim}
ICL> MAGIC input output
\end{verbatim}\end{quote}
Write magic values into existing dataset
\begin{quote}\begin{verbatim}
ICL> MAGIC input OVER
\end{verbatim}\end{quote}
Use specified magic value and mask - datasets will be prompted for
\begin{quote}\begin{verbatim}
ICL> MAGIC MAGIC=9999999.9 MASK=00000001
\end{verbatim}\end{quote}
\section{MASK}
This command attaches the specified mask to the given dataset. This
will then be the mask that ASTERIX uses to determine bad quality.
Note that leading zeros are not required when specifying the mask.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Input dataset.
           GLOBAL.BINDS(Default binned dataset)
 MASK      2    CHAR       Mask value.

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 ICL> MASK dataset 00000001
\end{verbatim}\end{quote}
\section{QUALITY}
This is the general quality manipulation command. The user interface
of QUALITY is firstly concerned with selecting those points in the
input dataset whose quality is to be manipulated, and then with the
selection operation of the operation to be performed.

Selection of points is achieved by specifying zero or more of the
following keywords,


\begin{description}
\item[AXSEL]
Selection on basis of axis value ranges
\item[FSEL]
Selection on spatial axes using info in ARD file
\item[DATSEL]
Selection on basis of data value
\item[MAGIC]
Specialised version of DATSEL which locates data values
containing a "magic" value
\item[QSEL]
Select on basis of existing quality value
\end{description}
If none of these options are chosen then all points are selected. If
more than one is specified then the selections of points are combined
in the AND sense (ie. only those points included by all selectors are
picked).

QUALITY supports the following operations on the selected points. Note
that an operation must be selected on the command line, otherwise the
error "No QUALITY operation selected" will appear.

\begin{verbatim}
   QUALITY IGNORE  - Sets the tempory bad quality bit,      (=IGNORE)
                     leaving the rest unaltered.
   QUALITY RESTORE - Clears the temporary bad quality bit,  (=RESTORE)
                     restoring the original quality value.
   QUALITY SET     - Sets quality to a specified value.     (=SETQUAL)
   QUALITY NOT     - Complements existing quality values.
   QUALITY AND     - ANDs existing quality with specified
                     value.
   QUALITY OR      - ORs existing quality with specified
                     value.
   QUALITY EOR     - EORs existing quality with specified
                     value.
\end{verbatim}
\subsection{OverWriting}
Like most applications the default value of the OVER
parameter is FALSE.

You must specify OVER on the command line if you wish to
overwrite the original dataset.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Input dataset.
          GLOBAL.BINDS (Default binned dataset)
 AUXIN     -    UNIV       Auxilliary input dataset. Optionally
                           used in DATSEL and MAGIC modes
    Hidden default = !
 OUT       2    UNIV       Output dataset
           ->GLOBAL.BINDS (Default binned dataset)
 OVER      -    LOGICAL    Overwrite input dataset?
    Hidden default = NO
 AXSEL     -    LOGICAL    Select on axis ranges for data?
    Hidden default = YES
 DATSEL    -    LOGICAL    Select on data value ranges to alter?
    Hidden default = YES
 QSEL      -    LOGICAL    Specify existing quality value to alter?
    Hidden default = NO
 MAGIC     -    LOGICAL    Select on magic data values.
    Hidden default = NO
 AXES      -    CHAR       Axis numbers to apply ranges on
 AXIS1     -    CHAR       Ranges for axis 1
 AXIS2     -    CHAR       Ranges for axis 2
 AXIS3     -    CHAR       Ranges for axis 3
 AXIS4     -    CHAR       Ranges for axis 4
 AXIS5     -    CHAR       Ranges for axis 5
 AXIS6     -    CHAR       Ranges for axis 6
 AXIS7     -    CHAR       Ranges for axis 7
 DATA      -    CHAR       Data value ranges.
 MODQUAL   -    CHAR       Existing quality value to alter.
                           (asked for if QSEL = YES)
 QVAL      -    CHAR       Specified quality value (for SET, AND,
                           NAND, OR, and EOR modes)
 FSEL      -    LOGICAL    Use a spatial file ?
    Hidden default = NO
 ARDFILE   -    CHAR       Name of spatial text file
         -GLOBAL.ARDFILE (Default spatial description file)

  ONE and ONLY ONE of the following modes MUST be selected:
 IGNORE    -    LOGICAL    Ignore mode?
    Hidden default = NO
 RESTORE   -    LOGICAL    Restore mode?
    Hidden default = NO
 SET       -    LOGICAL    Set mode?
    Hidden default = NO
 AND       -    LOGICAL    AND mode?
    Hidden default = NO
 OR        -    LOGICAL    OR mode?
    Hidden default = NO
 EOR       -    LOGICAL    EOR mode?
    Hidden default = NO
 NOT       -    LOGICAL    NOT mode?
    Hidden default = NO
 XDIM      -    INTEGER    Which axis is x-axis?
    Default = 1
 YDIM      -    INTEGER    Which axis is y-axis?
    Default = 2

\end{verbatim}\subsection{Auxilliary Input}
In DATSEL and MAGIC modes QUALITY chooses those pixels which will
have their quality value altered by looking at the data array in
the same dataset. The AUXIN parameter enables QUALITY to change
the quality values in one dataset based on the data array values
in another. The data array specified by AUXIN (it may be either
primitive or structured) must be the same size as the quality
array in the INP dataset.

\subsection{Examples}
Set quality values within dataset (ie overwrites existing values) -
will prompt for axis and data ranges
\begin{quote}\begin{verbatim}
> SETQUAL dataset OVER
\end{verbatim}\end{quote}
Set quality for points with magic values
\begin{quote}\begin{verbatim}
> SETQUAL MAGIC
\end{verbatim}\end{quote}
Set 'ignore' bit in dataset for specified data range
\begin{quote}\begin{verbatim}
> IGNORE dataset OVER DATSEL NOAXSEL
\end{verbatim}\end{quote}
Reset all 'ignore' bits within dataset no axis or data value selection
\begin{quote}\begin{verbatim}
> RESTORE dataset OVER NODATSEL NOAXSEL
\end{verbatim}\end{quote}
Mask all quality values with a given value
\begin{quote}\begin{verbatim}
> QUALITY AND NODATSEL NOAXSEL
\end{verbatim}\end{quote}
Set 'ignore' bit in dataset for those pixels where the equivalent
pixels in exp\_map lie within the specified data range
\begin{quote}\begin{verbatim}
> IGNORE dataset DATSEL NOAXSEL AUXIN=exp_map
\end{verbatim}\end{quote}
\subsection{Quality values}
When the application asks you for a quality value it expects to
receive either,


\begin{enumerate}
\item A name of a quality bit pattern,
\begin{verbatim}
     Name      Bit pattern        Name      Bit pattern

     GOOD      00000000           PATCHED   01000000
     BAD       00000010           ARITH     00000100
     MISSING   00000001           IGNORE    10000000
\end{verbatim}
\item A binary number, up to 8 bits long. This is read in as a
character string, and converted to a numeric value later.

The binary number does not HAVE to be 8 bits long i.e.
\begin{verbatim}
                   11 = 00000011
\end{verbatim}
\end{enumerate}
\subsection{Range Selection}
There are 5 ways in which you can specify the QUALITY values to
be altered, controlled by the following logical keywords:


\begin{description}
\item[AXSEL]
If true (default), you are asked to specify axis
ranges within which quality values will be altered.
\item[DATSEL]
If true (default), you are asked to specify data
value ranges within which quality values will be
altered.
\item[FSEL]
If true, you are prompted for an ARD file name
which will select all points with in the spatial
areas described
\item[QSEL]
If true, you are asked to specify ONE existing
QUALITY value to alter. Default is false.
\item[MAGIC]
Cause a quality value to be set for all points
where the data contains a magic value
\end{description}
The axis and data ranges obey the usual range rules; i.e. bins
with center values greater than or equal to lower bounds and less
than upper bounds are included. If, however, the last upper bound
equals the max value of the range then it is included. Where data
and axis ranges are specified, only QUALITY for points which come
within all specified ranges are operated on.

\subsection{ShortForms}
Four shortform commands have been defined, with particular
modes specified.

ARDQUAL = QUALITY IGNORE=Y FSEL=Y
i.e. invoke QUALITY in IGNORE mode, and
use a spatial file to define the ignored region
IGNORE = QUALITY IGNORE
i.e. invoke QUALITY in IGNORE mode
RESTORE = QUALITY RESTORE
i.e. invoke QUALITY in RESTORE mode
SETQUAL = QUALITY SET
i.e. invoke QUALITY in SET mode.
All keywords are the same as for QUALITY, but obviously you don't
define a mode!

\subsection{Spatial files}
It is possible to use a spatial file (as produced by REGIONS or WELLARD)
to define a 2-d region on an n-d datafile. This file may be used to
set the QUALITY of the pixels in the region by setting FSEL=YES on
the QUALITY command line. A shortform command, ARDQUAL, is available
which basically performs an IGNORE operation on the pixels within
the region specified.

\section{ARDQUAL+}
ARDQUAL is a shortform variant of QUALITY, used to set the
temporary bad quality data flag of points within a binned
dataset (most significant bit of the QUALITY byte), using a
spatial file to define the region of interest.

\begin{verbatim}
               ARDQUAL = QUALITY IGNORE FSEL=Y
\end{verbatim}
With this command, the pixels to be changed are defined by the
spatial file. The user isn't prompted for axis ranges or data
values to change.

See QUALITY for full details of the parameters and their keywords.

\section{IGNORE+}
Ignore is a shortform variant of QUALITY, used to set the
temporary bad quality data flag of points within a binned
dataset (most significant bit of the QUALITY byte).
\begin{verbatim}
               IGNORE = QUALITY IGNORE OVER
\end{verbatim}
As with QUALITY the user is able to select axis ranges; data
value ranges; and to specify a particular existing quality value,
to choose the points in the dataset to be effected.

See QUALITY for full details of the parameters and their keywords.

\section{RESTORE+}
Restore is a short form variant of QUALITY, used to clear the
temporary bad quality data flag of points within a binned
dataset (most significant bit of the QUALITY byte).

\begin{verbatim}
               RESTORE = QUALITY RESTORE OVER
\end{verbatim}
As with QUALITY the user is able to select axis ranges; data
value ranges; and to specify a particular existing quality value,
to choose the points in the dataset to be effected.

See QUALITY for full details of the parameters and their keywords.

\section{SETQUAL+}
Setqual is a shortform variant of QUALITY, used to set
quality to a particular value.
\begin{verbatim}
               SETQUAL = QUALITY SET
\end{verbatim}
As with QUALITY the user is able to select axis ranges; data
value ranges; and to specify a particular existing quality value,
to choose the points in the dataset to be effected.

See QUALITY for full details of the parameters and their keywords.

\chapter{Format conversion commands}
These applications are concerned with converting data from one
dataset type to another.

\section{ASTCONV}
Converts datasets between the old 'Wright-Giddings' STARLINK
standard format and ASTERIX binned datasets (in the new
STARLINK NDF format).

The direction of conversion is decided according to the input
dataset specified.

If an output of the same name is specified then a new version
of the file is created.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   CHARACTER The input dataset to be converted
 OUT         2   CHARACTER Dataset to receive conversion
 OLD         -   LOGICAL   Default FALSE. Is the input file the old
                           Asterix format?
 NEW         -   LOGICAL   Default TRUE. Is input the new Asterix format?

\end{verbatim}\subsection{Examples}
ICL> ASTCONV OLD\_DS NEW\_DS - perform appropriate conversion
from OLD\_DS to NEW\_DS
\section{AST2QDP}
AST2QDP converts 1 dimensional binned datasets to QDP format. It
additionally translates ASTERIX graphics attributes to QDP plot
commands which are prepended to the output file. Data variance and
quality are handled.

The procedure ASTQDP is a wrap-up of AST2QDP which takes an
ASTERIX dataset and runs QDP directly to the specified graphics
device.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   UNIV      The input dataset. May be primitive
 OUT         2   CHARACTER The name of the output QDP file

\end{verbatim}\subsection{Example}
\begin{quote}\begin{verbatim}
ICL> ast2qdp time_series_a time.qdp
% cat time.qdp
READ SERR 2
LABEL X "MJD"
LABEL Y "Intensity (count/s)"
45001.2  2.34 0.04
45002.1  1.22 0.07
45003.0  0.05 0.12
45003.9  1.45 0.11
45004.8  1.37 0.04
...
\end{verbatim}\end{quote}
\section{ASTQDP*}
A command procedure which runs the AST2QDP command to convert
an ASTERIX dataset to QDP format, and then places the user inside
QDP to enter the commands he/she chooses. The procedure should be
invoked thus,

\begin{quote}\begin{verbatim}
   % astqdp [-h] [-k] [filename [device]]
\end{verbatim}\end{quote}
The -h option prints a short summary of the usage of the procedure.
The -k option instructs ASTQDP that the intermediate QDP file
should be retained (the default is to delete it). If this option
is selected then the file will be named .qdp. If the
filename is not supplied then it will be prompted for. If the
output device is not specified then QDP will use the device defined
by the PGPLOT\_TYPE environment variable, which is usually set to
the value "/xwin".

\subsection{Devices}
The normal version of QDP uses the so called native mode PGPLOT.
This is faster than the normal Starlink version but there are
annoying differences in device names for users of the regular
version. On UNIX, the list of devices is as follows.
\begin{verbatim}
     /GF       GraphOn Tek terminal emulator
     /NULL     Null device, no output
     /PS       PostScript file, landscape orientation
     /RETRO    Retrographics VT640 Tek emulator
     /TEK4010  Tektronix 4010 terminal
     /TK4100
     /VPS      PostScript file, portrait orientation
     /VT125    DEC VT125 and other REGIS terminals
     /XDISP    pgdisp or figdisp server
     /XTERM    XTERM Tek terminal emulator
     /XWINDOW  Window on Xwindow server
\end{verbatim}
\subsection{Example}
Invoke ASTQDP on a spectrum, sending the output to the postscript
device, and retain the QDP file.

\begin{quote}\begin{verbatim}
 % astqdp -k spectrum /ps
 PLT> fit lin ...

 ... more QDP commands ...

 PLT> quit
\end{verbatim}\end{quote}
\section{AST2XSP}
Converts a standard Asterix spectral dataset into XSPEC format.
Both the XSPEC pulse height (.PHA) and detector response (.RSP)
files are created.

This routine works on EXOSAT ME, LE, ROSAT WFC, PSPC and
HEXE data at present.

\subsection{Changes in V1.6}
PSPC spectral files may now have any number of spectral bins
and arbitrary bin widths. The \%match\% field has been added to the
.PHA file. A single spectrum may be extracted from a multi-spectral
file, such as a RADIAL\_SPECTRAL series. Support for the EXOSAT LE
instrument has been added.

\subsection{ROSAT WFC}
First use WFCSPEC to create a spectral file. AST2XSP will run transparently
on this file.

\subsection{ROSAT PSPC}
Create a spectral file containing an arbitrary number of PH channels.
Then run XRTCORR (and of course XRTSUB if required) and XRTRESP.
The file can then be converted to XSPEC format using AST2XSP.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   CHARACTER The input dataset to be converted
 SLICE           INTEGER   The number of the spectrum to use, if
                           the file is multi-spectral.

\end{verbatim}\subsection{Examples}
To convert a spectral file into XSPEC format
\begin{quote}\begin{verbatim}
csh> ast2xsp filename output
\end{verbatim}\end{quote}
To produce an XSPEC, spectrum and response file from a 2-d datafile
\begin{quote}\begin{verbatim}
csh> ast2xsp filename SLICE=2 output
\end{verbatim}\end{quote}
\section{AXCONV}
Converts ASTERIX datasets for use within KAPPA. Actually all that happens
is that spaced axes (i.e. those with a BASE and SCALE value) are
expanded into simple arrays. Conversion takes place in the input
datafile. VALIDATE may be used to reverse this operation.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   CHARACTER The input dataset to be converted

\end{verbatim}\section{EVBIN}
Converts ASTERIX EVENT datasets, produced by instrument specific
software, into BINNED datasets for use with ASTERIX applications.

See the ASTHELP entry for EvBin under Event\_Dataset\_Commands for
full details.

\section{EXPORT}
Converts a binned dataset to ascii file containing all axis, data,
quality and error information. A header is written which enables
the ascii file to be converted back to ASTERIX format using IMPORT
directly, although this feature can be turned off.

\subsection{Input Output}
Input: Up to 7D binned dataset, or primitive object

Output: Text file

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1   CHARACTER Dataset to be converted to ascii
   - GLOBAL.BINDS (Default binned dataset)
 OUT         2   CHARACTER Filename for output. Default extension
                           is .DAT.
 IMPREAD     -   LOGICAL   Write an IMPORT compatible header?
   Hidden default Y

\end{verbatim}\section{HDS2TEXT}
Outputs data from one or more data arrays into an ASCII file. The
dimensionality of the arrays is ignored - they are treated as if
they are all simple lists of numbers. All the data arrays should
contain the same number of elements.
The output format can be any Fortran floating point format, eg.
{\tt F10.4}, {\tt G12.5} or {\tt 1PE15.6}.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 IN1         1   CHARACTER The first data array to be output
 ...         .   ...       ...
 IN9         9   CHARACTER The ninth data array to be output
 FMT         -   CHARACTER The output format to use
 OUT         -   CHARACTER Name of ASCII file to receive output

\end{verbatim}\subsection{Example}
\begin{quote}\begin{verbatim}
ICL> HDS2TEXT OUT=X.DAT IN1=FILE1.DATA_ARRAY IN2=FILE1.VARIANCE ~
     IN3=FILE2.AXIS(1).DATA_ARRAY
FMT - Output format /'1PG11.6'/ >
\end{verbatim}\end{quote}
\section{IMPORT}
Converts data in a ASCII file into an ASTERIX binned dataset. It
is designed to permit users to import data into Asterix making as
few changes as possible to their input text files.

IMPORT is a very flexible program - a full description is rather
long, so read the "Quick\_Guide" section below before attempting
to use it.

Demonstration files can be found in the directory {\tt \$AST\_ROOT/data/demo}, for
both 1 and 2 dimensional files.

\subsection{Quick Guide}
This section describes IMPORT working on the following example data file, a
copy can be made from \$AST\_ROOT/data/demo/import\_1d.dat

\begin{quote}\begin{verbatim}
  ; IMPORT demo file - 1 dimensional
  ;
  TOPLEVEL TITLE "Fitted Fe abundance" ~
  TYPE "Radial_Profile"
  AXIS LABEL "Radius" UNITS "Arcmin" SCALAR_WIDTH 6.0
  DATA UNITS "Solar" LABEL "Abundance"
  3  1.5
  6  0.5
  9  0.3
  12 1.2e-1
\end{verbatim}\end{quote}
The file can be divided into two parts, the header part and the
data part; the former describes how IMPORT is to interpret the
latter. The numbers in the data part are in columns, separated
by spaces, but are not required to be aligned or any particular
format. The header above is now described.

The first 2 lines are comments - all text from the semicolon to
to the end of the line is ignored.

The 3rd line contains the word TOPLEVEL. This informs IMPORT to
create certain named components in the output dataset. Those
components allowed are TITLE and TYPE.Their contents follow the
name of the component - if the case of the text is important
then enclose it in double quotes. The 3rd line demonstrates the
use of the continuation character {\tt ~} to continue the information
on the next line of the text file. The TOPLEVEL information is
optional - IMPORT will use defaults if it is not present.

The next line ( the 5th ) contains the word AXIS. This word is
a column descriptor - ie. it indicates to IMPORT the nature of
a column in the data part of the file. Like TOPLEVEL, AXIS can
optionally be followed by several keywords. See under the
section on "Header\_Part" for their meaning.

Each data descriptor corresponds to one column in the data part.
The order of the descriptors is the same as the columns.

Once all the descriptors have been specified then the data
begins. IMPORT reads in the data lines and contructs the dataset.

Note that the data is sorted by IMPORT, so there is no need to
arrange the data in the correct order. This feature can be
disabled using the SORT keyword - useful for scattered data,
such as phase plots.

As mentioned above, much of the header data is optional - the
minimal header required to get the above data into Asterix
would be,

\begin{quote}\begin{verbatim}
  AXIS
  DATA
  3  1.5
  6  0.5
  9  0.3
  12 1.2e-1
  15 2.3e-3
\end{verbatim}\end{quote}
This completes the quick guide - for more information see the
other subtopics in this help.

\subsection{Asymmetric Errors}
IMPORT currently provides the only means of getting asymmetric
axis and data errors into ASTERIX applications. This is achieved
using the LO/UPWIDTH and/or LO/UPERROR descriptors to describe
the axis and data error bars respectively. See the "Asymmetric
Errors" under the examples sub-topic for a demonstration of
these descriptors.

An alternate pair of descriptors for settings lower and upper
data errors is LOLIM/UPLIM. Here the data columns should specify
the lower and upper error bounds, rather than the sizes of the
errors themselves. IMPORT converts the bounds to errors using the
value of the corresponding DATA element.

\subsection{Data Part}
The data part should contain a row of data for each output data
pixel with as many columns as descriptors in the header. Numeric
data can be entered in free format, including scientific notation.
QUALITY information can be entered either numerically, or using
the tokens OK, MISSING, OVERFLOW, BAD either singly or in
combination (see Quality subtopic for more information).

\subsection{File Format}
At all points in IMPORT input files, the following characters
have special significance.

; ! * Comment characters. All text on a line after one of
these is met is ignored.
~ + Continuation characters. Lines are concatenated if
this character is found.
Blank lines are always ignored, as are superfluous spaces and
tab characters.

IMPORT is not case sensitive - if case is required to be
preserved, for example in keyword data, then the text should
be enclosed in double quotes.

\subsection{Header Part}
The only item in the header with no counterpart in the
data section is the TOPLEVEL record.

\begin{verbatim}
     TOPLEVEL
 \end{verbatim}
If present, this line must be the first significant text
in the input file.

The data descriptors supported by IMPORT are, with their
keywords:
AXIS
Creates an axis component in output. Up to seven
axis descriptors may be specified. Label and units
can be set for each, the normalisation and a scalar
width value. If NORM is not specified, then IMPORT
assumes the data to be un-normalised about that axis.
If non-scalar widths are required, use the WIDTH
descriptor. If DECREASING is specified, axis values
are ranked in descending numeric order.
WIDTH Specifies that the corresponding column contains
bin widths symmetric about the bin centre for the
axis most recently specified by an AXIS descriptor.
LOWIDTH Specifies that the corresponding column contains the
lower bin widths for the axis most recently specified
by an AXIS descriptor.
HIWIDTH Specifies that the corresponding column contains the
upper bin widths for the axis most recently specified
by an AXIS descriptor.
DATA
Specifies the data for the DATA\_ARRAY component of
the output dataset. Label and units can be specified
IGNORE Use the descriptor to force IMPORT to ignore the
corresponding column of data.
QUALITY
Generates quality array. This can be numeric data,
but alternatively the names of bits can be specified.
See "Quality\_Bit\_Names" below. If MASK is not
specified, the default mask of \%11111111 binary will
be used.
VARIANCE Generates variances in output dataset.
ERROR Allows user to enter data errors, which are squared
on output to form a variance array for Asterix.
LOERROR Specifies that the corresponding column contains the
lower data error in the case of asymmetric data errors.
UPERROR Specifies that the corresponding column contains the
upper data error in the case of asymmetric data errors.
\subsubsection{Quality Bit Names}
Valid quality bit names are, with their equivalent bit values
\begin{verbatim}
   OK             00000000
   BAD            00000001
   MISSING        00000010
   OVERFLOW       00000100
   PATCHED        01000000
   IGNORE         10000000
\end{verbatim}
Values can be combined by separating names by commas and enclosing
them in brackets, eg.
\begin{quote}\begin{verbatim}
(OVERFLOW,MISSING)

(PATCHED,BAD)
\end{verbatim}\end{quote}
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------

  INP       1   CHARACTER  The name of the ASCII text file to be
                           read. If a file extension is not given
                           then .DAT is assumed.
  SORT      -   LOGICAL    Default TRUE. Should input data be
                           sorted for each axis. Specify NO for
                           scattered data.
  OUT       2   CHARACTER  The name of the output ASTERIX dataset

\end{verbatim}\subsection{Examples}
\subsubsection{1D dataset}
The first IMPORT demo file,

\begin{quote}\begin{verbatim}
ICL> IMPORT AST_DEMO:IMPORT_1D OUTPUT         ;

 ;--------------------------------------------------
 ;|       IMPORT demo file - 1 dimensional         |
 ;--------------------------------------------------
 TOPLEVEL TYPE "Profile"
 AXIS LABEL "Radius" UNITS "Arcmin" SCALAR_WIDTH 6.0
 DATA UNITS "Solar" LABEL "Abundance"

 3  1.5
 6  0.5
 9  0.3
 12  1.2e-1
 15  2.3e-3
 ;--------------------------------------------------
\end{verbatim}\end{quote}
\subsubsection{2D dataset}
\begin{quote}\begin{verbatim}
ICL> IMPORT test.dat output

  TOPLEVEL TITLE "Test" TYPE "Image" LABEL ~  ; Continuation
    "Flux" +                                  ; And another one
     UNITS "mJy"
  ;
  AXIS LABEL "X_COORD" UNITS "femto-parsecs"  ; 1st axis
  AXIS UNITS "miles" LABEL "Y_COORD"          ; 2nd axis
  DATA                                        ; data array
  *
  2 2 7    ; Note data values are jumbled - IMPORT still places
  1 2 4    ; them correctly.
  3 3 45
  1 1 3
  2 3 8
  3 1 4
  1 3 5
  3 2 10
  2 1 6
\end{verbatim}\end{quote}
The above example creates,
\begin{quote}\begin{verbatim}
 .TITLE                         "Test"
 .TYPE                          "Image"
 .AXIS[2]
    .AXIS(1)
       .LABEL                   "X_COORD"
       .UNITS                   "femto_parsecs"
       .DATA_ARRAY[3]           1.0, 2.0, 3.0
    .AXIS(2)
       .LABEL                   "Y_COORD"
       .UNITS                   "miles"
       .DATA_ARRAY[3]           1.0, 2.0, 3.0
 .DATA_ARRAY[3,3]                3  6  4
                                 4  7 10
                                 5  8 45
 .HISTORY
\end{verbatim}\end{quote}
\subsubsection{Asymmetric Errors}
\begin{quote}\begin{verbatim}
 ;--------------------------------------------------
 ;         IMPORT demo file - 1 dimensional
 ;           with x-axis asymmetric errors
 ;--------------------------------------------------
 TOPLEVEL TYPE "Profile"
 AXIS LABEL "Radius" UNITS "Arcmin"
 LOWIDTH
 UPWIDTH
 DATA UNITS "Solar" LABEL "Abundance"

 3 3 1.5 1.5
 6 1.5 1.5 0.5
 9 1.5 1.5 0.3
 12 1.5 1.5 1.2e-1
 15 1.5 4 2.3e-3
 ;--------------------------------------------------
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
 ;--------------------------------------------------
 ;         IMPORT demo file - 1 dimensional
 ;        with x and y-axis asymmetric errors
 ;--------------------------------------------------
 TOPLEVEL TYPE "Profile"
 AXIS LABEL "Radius" UNITS "Arcmin"
 LOWIDTH
 UPWIDTH
 DATA UNITS "Solar" LABEL "Abundance"
 LOERROR
 UPERROR
 3 3 1.5 1.5 0.1 0.12
 6 1.5 1.5 0.5 0.02 0.03
 9 1.5 1.5 0.3 0.015 0.02
 12 1.5 1.5 1.2e-1 0.003 0.003
 15 1.5 4 2.3e-3 0.0015 0.0015
 ;--------------------------------------------------
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
 ;--------------------------------------------------
 ;         IMPORT demo file - 1 dimensional
 ;        with x and y-axis asymmetric errors
 ;   but using data error limits rather than errors
 ;--------------------------------------------------
 TOPLEVEL TYPE "Profile"
 AXIS LABEL "Radius" UNITS "Arcmin"
 LOWIDTH
 UPWIDTH
 DATA UNITS "Solar" LABEL "Abundance"
 LOLIM
 UPLIM
 3 3 1.5 1.5 1.4 1.62
 6 1.5 1.5 0.5 0.48 0.53
 9 1.5 1.5 0.3 0.285 0.32
 12 1.5 1.5 1.2e-1 0.117 0.123
 15 1.5 4 2.3e-3 0.0008 0.0038
 ;--------------------------------------------------
\end{verbatim}\end{quote}
\section{TEXT2HDS}
Converts an Ascii text file of numbers into an HDS file.

The numbers must be arranged in columns, e.g.

\begin{verbatim}
   0.1    0.1     63
   0.2    0.1     80
   0.3    0.1    190
\end{verbatim}
Any format is permissible, but note that at present all output arrays
will be REAL. The program will cope with upto 40 columns and line
lengths of upto 512 characters.

Note: If you want to produce a standard Asterix binned dataset containing
a data array and axes use IMPORT. This program simply produces an
HDS file containing a number of primitive HDS arrays or an Asterix
EVENT dataset.

Information about the input file will be gained either from the user
or from a SCAR descriptor file if one is available - (see SCARFILE).

\subsection{Input \& Outpu}
The program needs an Ascii text file as input. It has two modes of
output. The basic mode produces an HDS file where each column in the
input file becomes a separate HDS array in the output file.
The second mode obtainable by putting the command word "EVENT" on
the command line produces a standard Asterix EVENT\_DATASET where each
column in the input file is written into a LIST in the output file.
Each list contains a data array, units if known and the min and
max values in the data array. This file can subsequently be converted
into an Asterix binned datafile using EVBIN.

\subsection{SCARFILE}
If the textfile has an associated SCAR descriptor file then this can
be used to tell the program about the contents of the file. To choose
this option, enter "SCAR" on the command line and you will prompted
for the name of the descriptor file.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type             Description
 -------  --------  ----             -----------
 INFILE      1      CHARACTER    Name of input text file
 OUTFILE     2      UNIV         Name of HDS output file
 NCOLS       3      INTEGER      Number of columns in the file
 NROWS       4      INTEGER      Maximum number of lines in the
                                 file.
 CNAME1      5      CHARACTER    Name of HDS object to hold the
                                 array of values in column 1
 CNAME2             CHARACTER    Name of object to hold column 2
   .
   .
   .
 CNAME10            CHARACTER    Name of object to hold column 10
 SCAR               LOGICAL      Is a SCAR descriptor file available
 DSCFILE            CHARACTER    Name of SCAR descriptor file
 EVENT              LOGICAL      Produce an EVENT dataset ?

\end{verbatim}\subsubsection{NCOLS}
If the file has the following structure:
\begin{verbatim}
  0.0    1.0    10.0       20.0        3.5    2.0
  0.0    2.0    10.0       20.0       56.5    3.0
  0.0    2.0    10.0       20.0        5.5    2.0
\end{verbatim}
Then to get all the columns enter NCOLS=6.
If you want the first three columns for instance enter NCOLS=3.

\subsubsection{NROWS}
This value is used to set up a mapped area of memory to contain all
the data from the input file. If you know the number of lines i.e.
the number of values in EACH column then enter this value. Otherwise
enter a number which is too high. e.g. if you know there are
about 10000 lines in the file then enter 20000. The only drawback of this
is that the execution time is a weak function of the size of the mapped
area of memory due to page faulting. So in other words if there are
say 100 lines in the file dont set NROWS to 100000. If you set NROWS to
less than the number of lines in the file then the program will
give you a warning message. It will however produce an output file
with arrays of length NROWS, so you can use this parameter to say
select the first 1000 lines from a file.

\subsubsection{CNAME}
This specifies the name of the output array in the HDS file for
a particular data column

\subsection{Examples}
To convert a text file of about 10000 lines containing the columns
X, Y and PIXEL VALUE into a basic HDS file.

\begin{quote}\begin{verbatim}
ICL> TEXT2HDS
INFILE - Name of input text file > M33.DAT
OUTFILE - Name of output HDS file > M33_IMAGE
NCOLS - Number of columns in the text file > 3
NROWS - Mximum number of lines in input file > 20000
CNAME1 - Name of array to hold 1st column > XAXIS
CNAME2 - Name of array to hold 2nd column > YAXIS
CNAME3 - Name of array to hold 3rd column > DATA_ARRAY
\end{verbatim}\end{quote}
Produces:
\begin{quote}\begin{verbatim}
   M33_IMAGE.
             XAXIS(10000)
             YAXIS(10000)
             DATA_ARRAY(10000)
\end{verbatim}\end{quote}
To convert the same file into an EVENT dataset
\begin{quote}\begin{verbatim}
ICL> TEXT2HDS EVENT OUTFILE=OUT
\end{verbatim}\end{quote}
Produces:
\begin{quote}\begin{verbatim}
OUT
  .XAXIS
  .  .DATA_ARRAY(10000)
  .  .UNITS                   =" "
  .  .FIELD_MIN               =  0.00000E+00
  .  .FIELD_MAX               =   9999.0
  .YAXIS
  .  .DATA_ARRAY(10000)
  .  .UNITS                   =" "
  .  .FIELD_MIN               =  0.00000E+00
  .  .FIELD_MAX               =   9000.0
  .DATA_ARRAY
  .  .DATA_ARRAY(10000)
  .  .UNITS                   =" "
  .  .FIELD_MIN               =  3.50000E+00
  .  .FIELD_MAX               =  512.400
\end{verbatim}\end{quote}
If you have a SCAR descriptor file for the input file.
\begin{quote}\begin{verbatim}
ICL> TEXT2HDS SCAR EVENT
INFILE - Name of input text file > M33.DAT
DSCFILE - Name of SCAR descriptor file > DSCFM33.DAT
OUTFILE - Name of output HDS file > M33_EVENT
\end{verbatim}\end{quote}
\section{FITS2HDS}
Converts FITS table or image data into HDS files.
A HDS file is produced for each image or table contained in the
FITS file. IF the FITS keyword "EXTNAME" is present this is
appended to the output filename supplied, otherwise a number is
added for each extra table. Similar tables are joined together
if they have the same column names and extension name.

FITS2HDS will attempt to create the standard NDF axis structure
for image data using information contained in the keywords, unless
the hidden parameter "ASTERIX" is set false.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type          Description
 -------  --------  ----          -----------
 INPUT       1      CHARACTER   Name of input FITS datafile
 OUTPUT      2      CHARACTER   Rootname of HDS output file.
 ORIGIN             CHARACTER   Origin of dataset (MPE, US, RDF)
 JOIN               LOGICAL     Join similar tables together
                                (Default = TRUE)
 ASTERIX            LOGICAL     Attempt to extract axis information
                                and create NDF structure
                                (Default = TRUE)

\end{verbatim}\subsection{Examples}
To convert a FITS file containing an image dataset into a HDS file.
\begin{quote}\begin{verbatim}
ICL> FITS2HDS
FITS2HDS version 1.7-2
INPUT - Name of FITS file > RH201756N00_IM1.FITS
FITS file type is RDF
OUTPUT - Generic name of HDS output files > rh201756_im1
Writing to rh201756_im1
\end{verbatim}\end{quote}
Produces:
\begin{quote}\begin{verbatim}
FITSCONV

  DATA_ARRAY(512,512)     0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
                                 ... 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
  AXIS(2)        <AXIS>          {array of structures}

  Contents of AXIS(1)
     UNITS                'degrees'
     DATA_ARRAY     <ARRAY>         {structure}
        VARIANT               'SPACED'
        BASE                    0.5688889
        SCALE                   -2.2222223E-03
        DIMENSION            512

  MORE                      {structure}
     ASTERIX                   {structure}
        HEADER              {structure}
           TARGET               'RE0537+02'
           OBSERVER             'JEFFRIES, ROBIN,DAVI'
           OBSERVATORY           'ROSAT'
           INSTRUMENT            'XRT'
           AXIS_RA               84.44
           AXIS_DEC              2.52
           FIELD_RA              0.E00
           FIELD_DEC             0.E00
           EQUINOX              2000
           POSITION_ANGLE        0.E00
           BASE_MJD             49420
           BASE_DATE            '1994-Mar-09'
           BASE_UTC              48831.000000014
           BASE_TAI              8103.5653703704
           OBS_LENGTH              2356.136
           EXPOSURE_TIME           2356.136

        PROCESSING          {structure}
           BGND_SUBTRACTED      FALSE
           CORRECTED           {structure}
              DEAD_TIME            FALSE
              VIGNETTING           FALSE


        INSTRUMENT          {structure}
           PIXEL_SIZE              8
           FILTER               'OFF'
           DETECTOR             'HRI'
           SC_BASE               118945621
           SC_CONV               1
           RAWDATA               '     '
           SASS_VERSION         '                         '


     FITS(79)             'SIMPLE  =                    T / fi...'
                                    ... 'HISTORY     image(x,y)...','HISTORY'

  HISTORY               {structure}
     CREATED              '18-May-94 14:39:09'
     UPDATE_MODE          'NORMAL'
     EXTEND_SIZE          10
     CURRENT_RECORD       1
     RECORDS(10)          {array of structures}

     Contents of RECORDS(1)
        DATE                 '   18-May-1994 14:39:09'
        COMMAND              'FITS2HDS Version 1.7-2'
        HOST                 'LTVS4'
\end{verbatim}\end{quote}
To convert a FITS file containing a number of tables to a set of HDS
files.
\begin{quote}\begin{verbatim}
ICL> FITS2HDS
FITS2HDS version 1.7-2
INPUT - Name of FITS file > RH201756N00_BAS.FITS
OUTPUT - Generic name of HDS output files > rh201756
Writing to rh201756_stdgti
Converting 3 columns and 33 rows
Reading/mapping from 1 to 33
Writing to rh201756_stdevt
Converting 8 columns and 12784 rows
Reading/mapping from 1 to 10000
Reading/mapping from 10001 to 12784
Writing to rh201756_rejevt
Converting 8 columns and 3108 rows
Reading/mapping from 1 to 3108
Writing to rh201756_tsi
Converting 18 columns and 286 rows
Reading/mapping from 1 to 286
Writing to rh201756_stdqlm
Converting 33 columns and 1 rows
Reading/mapping from 1 to 1
Writing to rh201756_allqlm
Converting 33 columns and 1 rows
Reading/mapping from 1 to 1
\end{verbatim}\end{quote}
Produces 6 HDS files.
\section{OMD2HDS}
OMD2HDS converts binary event datafiles (.OMD) and the associated text
header file (.HDR), previously converted using PREPXRT and XRTDISK into
the HDS format used in ASTERIX version 1.7. This program is necessary
for porting data from VAX VMS machines onto Unix, the other HDS files
are directly transferable.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type          Description
 -------  --------  ----          -----------
 RAWDIR      1      CHAR      Location of datafile
 ROOTNAME    2      CHAR      Rootname of datafile
 DIR                LOGICAL   Display contents of the raw directory
 HDSROOT            CHAR      Rootname of output HDS datafile

\end{verbatim}\section{XRTCONV}
XRTCONV is a FORTRAN wrapup of FITS2HDS, XRTINDEX and XRTHOT. It
knows which files to look for, for any given dataset type (MPE, US and RDF)
and converts them into HDS. XRTINDEX is run on the events data to
extract useful information into a HDS header file. If hotspot/deadspot
information is available (currently only MPE HRI data) XRTHOT is run
and the data added to the HDS header file.
XRTCONV will automatically detect the origins of a dataset. It does this
using the following rules:
\begin{itemize}
\item MPE dataset, if any files ending in ".*fits" or ".mt".
\end{itemize}
otherwise the first file ending in ".fits" is opened
\begin{itemize}
\item MPE if keyword ORIGIN exists
\item US if keyword QPOENAME or XS-CNTRY exists
\item RDF if keyword RDF\_VERS exists
\end{itemize}
The automatic detection can be overridden by specifying one of the
following:
\begin{quote}\begin{verbatim}
ICL> XRTCONV origin=
\end{verbatim}\end{quote}
\chapter{Source Searching commands}
The PSS application searches for sources in binned data. The
output files produced are called Source Search DataSets (SSDS).
Applications beginning with "SS" manipulate these files.
BSUB performs global background subtraction on binned data.

The 'Techniques' subtopic gives suggestions for performing some
common source related analysis tasks.

\section{Source Search DataSets}
Source search datasets ( SSDS ) are HDS files containing results
of one or more source searching sessions.

The information is split into 2 parts, the BOOK-keeping structure
and the POSIT structure. The former contains all the data needed
to identify the image in which a source was found, the program
which found it, and other processing data.

The POSIT structure contains data about each source in the SSDS.
These are stored in lists - eg. right ascension or count rate.

\section{Techniques}
\subsection{Finding Sources}
\subsection{Removing Sources}
\subsection{Upper limits}
\section{BSUB}
Generates the background surface for WFC survey images and (optionally)
subtracts it from the input image. The output subtracted image is a
binned dataset can be used by PSS for source searching.
\subsection{Algorithm}
The program begins by defining rectangular image boundaries within which
all (non-zero) data lies. The data within this window is then searched
for diagonal edges running top to bottom (caused by, for example,
coverage effects at the start/end of the survey), flagging pixels outside
any such edges for exclusion during later procesing.

With the `real' data defined, the image is scanned for prominent sources
(using the median-filtering NIPS algorithm for speed). The approximate
radius and centroid position of such sources are then used to establish
source contaminated pixels that are to be ignored when evaluating the
background model.

The image is then binned into boxes of user defined size (but modified
if necessary to fit the image) and the mean in each box is determined,
by fitting a Poisson distribution function to the brightness histogram
of the pixels in the box (in an attempt to bias against remaining
weak undetected sources). An uncertainty on each box mean is also computed.
A weighted smoothing function (top-hat or Gaussian of user defined size
- but modified to accommodate holes left by ignored sources) is then applied
to the resulting grid of box mean values.

The background surface thus computed can be (optionally) subtracted from the
input image. At the same time, statistics about the quality of the
match between the background model and the input image are derived and
output.

\subsection{Problem images}
Some types of image can cause prolems.

\begin{enumerate}
\item Diagonal edges: Although there is a diagonal edge finding algorithm
in the program, if the data around the edge is sparse (i.e. the
edge is poorly-defined) or there are edges running from side to
side (which are assumed to be absent) or the edge is close to and
nearly parallel with an edge, the routine may not find the edge or it
may misrepresent it. In these cases, the resulting background surface
near the edge may not be as accurate as desired. This can lead to
spurious source detections within PSS.
\item Extended sources: The algorithm makes use of the appropriate point
spread function (PSF) to parameterize sources. Since extended
sources can not be adequately described by the PSF, their presence
may not be detected, or parts of them (their wings in particular)
may be missed. This can result in a contaminated background
region and a correspondingly overluminous background surface
around the location of the extended source.
\item Bright sources: Very bright sources possess broad wings which
are difficult to completely allow for (they can cover a very
substantial portion of the entire image). As a result, the
output background surface may contain evidence of `domed'
emission where the source was found. This should be at a
low level compared to the flux in the source before removal,
but could still jeopardise the identification of other weak
sources by PSS.
\item Double sources. Close double sources can not be adequately
parameterized by the PSF and so determination of their
spatial extent on the image is not reliable. Again this
can result in an unrealistic background surface estimate
in the vicinity of the sources.
\end{enumerate}
\subsection{Parameters}
\begin{verbatim}
  Keyword       Posn  Type       Description
  -------       ----  ----       -----------

  INP           1     UNIV       Any 2D dataset.
                - GLOBAL.BINDS (Default binned dataset)
  OUT           2     UNIV       Output file name for background
                                 subtracted image.
                -> GLOBAL.BINDS (Default binned dataset)
  BGND          -     UNIV       Optional background model output.
                - GLOBAL.MODEL (Default model dataset)
      Default = !
  PSF           -     CHAR       Input PSF to use.
  MASK          -     CHAR       If an analytical PSF to be used input
                                 the function from the list.
  AUX           -     CHAR       Used by the analytic function (see PSS).
                                 line?
  SOURCE_THRESH -     REAL       The threshold to remove source from the
                                 image (see "Algorithms").
      Default = 5
  BOX_DIM       -     INTEGER    Dimension of the box in pixels into
                                 which the image is subdivided.
      Default = 30
  SM_FILT       -     CHAR       Selects smoothing filter (top-hat or
                                 Gaussian) for smoothing.
      Default = H
  SM_FWHM       -     REAL       FWHM of smoothing function if Gaussian
                                 filter selected.
      Default = 15
  SMOOTH_BORDER -     LOGICAL    Can apply a sloping border to a top-hat
                                 smoothing function
      Default = N
  SMOOTH_SIZE   -     INTEGER    Limiting dimension of smoothing function
      Default = 25
  OUT_MESSADES  -     LOGICAL    Are messages to be output?
      Hidden default = YES

\end{verbatim}\subsection{Examples}
\section{COVERAGE}
Returns a histogram whose abscissa is the fraction of the pixels in
a 2D dataset whose value exceeds the data value on the ordinate.
The most common use for this program is in processing PSS sensitivity
maps, where the quantity of interest is often the fraction of an
image covered to a given sensitivity.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn    Type        Description
 -------    ----    ----        -----------

  INP           1   UNIV        Any 2D dataset.
                - GLOBAL.BINDS (Default binned dataset)
  OUT           2   UNIV        The coverage histogram
  MIN,MAX       -   REAL        The extrema of the ordinate in the
                                output. Defaults to valid data range
                                of the input dataset
  NBIN          -   INTEGER     Number of bins in histogram
    Default = 100

\end{verbatim}\section{CREPSF}
Creates a tabular psf dataset using the psf selected by the user.
This dataset can be used as the input into programs using the PSF
system by selecting the TABULAR option at the PSF prompt.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn    Type        Description
 -------    ----    ----        -----------
   OUT        1     UNIV        Output psf tabular dataset.
               -> GLOBAL.BINDS (Default binned dataset)
   UNITS      -     CHAR        Angular units to use for tabular
                                spatial axes
       Default = arcmin
   PIXSIZE    -     REAL        Pixel size in units of UNITS
   RADIUS     -     INTEGER     Half width of image in pixels.
   MJD        -     DOUBLE      MJD at time of observation.
       Hidden default = !
   PSF        -     CHAR        PSF system prompt. See psfs
                                for details.
   X0, Y0     -     REAL        Image position for psf.
       Hidden default = 0.0 for both
   DX, DY     -     REAL        Offset from centre of tabular
                                file to centre of psf.
       Hidden default = 0.0 for both
   MASK       -     CHAR        Name of profile to use.
   AUX        -     CHAR        Filter identification.

\end{verbatim}\subsection{Example}
\begin{quote}\begin{verbatim}
> CREPSF
 CREPSF Version 1.6-0
 OUT - PSF data image > wfc_s1a
 UNITS - Axis units for table /'arcmin'/ >
 PIXSIZE - Pixel size in arcmin > 1
 RADIUS - Radius of image in pixels > 10
 PSF system options :

   ANAL         EXOLE        PWFC         TABULAR      WFC
   XRT_HRI      XRT_PSPC

   POLAR(psf,rbin[,abin])    RECT(psf,xbin[,ybin])

 PSF - Choose PSF to use for search /'ANALYTIC'/ > wfc
 AUX - Enter filter id (S1A/B,S2A/B) > s1a
  Filter Lexan/C/B4C
 Unable to get IRIS value from dataset - defaulting to 2.5 degrees
 \end{verbatim}\end{quote}
\section{IMSIM}
IMSIM creates simulated images either from scratch or given an input
model dataset. Single images can be created, or IMSIM can create
arbitrary numbers of images with a trailing number appended to the
filename.

IMSIM's image model consists of a background plus sources. The background
distribution is taken from the input model dataset if present, otherwise
is assumed to be flat. Sources (additional to any present in the model
dataset, if used) can be added in two ways. Either a simple list of
expected fluxes and positions can be supplied, or the user can specify
a Log N vs. Log S relationship. In the latter case IMSIM generates a
population of sources with the prescribed luminosity function and
distributes them randomly in X,Y. Each psf for each source is scaled by
its flux and added to the internal model.

The output images are generated by taking a total number of counts
specified by the user and distributing them using the internal model.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn    Type        Description
 -------    ----    ----        -----------
   OUT        1     CHAR        Name of output image
   MODEL      -     CHAR        Name of a model dataset.
     Default = !
   PIXSIZE    -     REAL        Pixel size in arcmin
   FIELDSIZE  -     REAL        Field size in arcmin
   SEED       -     INTEGER     Seed for random number generator. If ! is
     Hidden default = !         is specified then the system time is used
   NFILE      -     INTEGER     Number of files to create.
     Default = 1
   FFILE      -     INTEGER     Index of first file. Only needed if NFILE
     Default = 1                is greater than one.
   BACK       -     INTEGER     Number of background counts. Default
     Default = !                gives none
   LOGNS      -     LOGICAL     Specify source luminosity function? SMIN
     Default = NO               to SNORM parameters are only used if this
                                parameter is specified
   SOURCEC    -     REAL[]      Array of source fluxes. Only used if LOGNS
     Default = !                is false.
   SOURCEP    -     REAL[]      List of source positions. Should be 2
                                values for each value in SOURCEC
   WIDTHS     -     REAL[]      List of source extensions. If specified and
                                greater than zero this is the FWHM of a
                                gaussian used to blur the source psfs.
   SMIN       -     REAL        Lowest source flux to use in luminosity
                                function
   SMAX       -     REAL        Highest source flux to use in luminosity
                                function
   SB         -     REAL        Flux at which break in Log NS occurs
   EU         -     REAL        Index of Log NS where flux > SB
   ED         -     REAL        Index of Log NS where flux  SB
   SNORM      -     REAL        Normalisation flux. This is the flux at
                                which we choose to normalise the source
                                luminosity distribution
   NSNORM     -     REAL        Normalisation. The number of sources
                                with flux>SNORM per square degree.
   PSF        -     CHAR        PSF system prompt. See psfs
                                for details.
   MASK       -     CHAR        Ditto.
   AUX        -     CHAR        Ditto.
   SRADIUS    -     INTEGER     Half-width of psf box used to evaluate
     Default = 15               the source model

\end{verbatim}\subsection{Examples}
Create a simulated copy of 'mymod' with a 100 count source added at
position +2,-4
\begin{quote}\begin{verbatim}
 > imsim sim model=mymod
 Using model quality array...
 Model is exposure corrected, scaling up by exposure of 23212.4 seconds
 Normalising model...
 Model contains 16266.1 counts
 BACK - Background counts /!/ > 16266
 SOURCEC - Number of counts per source /!/ > 100
 SOURCEP - Source positions /[0,0]/ > 2,-4
 WIDTHS - Width of gaussian for psf convolution /0/ >
 SRADIUS - Half width of psf in pixels /15/ >
 ...
 PSF - Choose PSF for simulated sources /'ANALYTIC'/ > XRT_PSPC
 MASK - PSPC psf option (LIST for descriptions) /'VARPROFILE'/ >
 Loaded XRT psf cube with 10 energy channels
 AUX - Mean photon energy in KeV > 0.6
 Background 8051 counts
 Source 1 95 counts
\end{verbatim}\end{quote}
Create a 5 simulated copies of 'mymod' with a specified Log N Log S
distribution.
\begin{quote}\begin{verbatim}
 > imsim nfile=5 out=sim model=mymod
 ...
 as before
 ...
 LOGNS - Choose source counts using Log NS /NO/ > y
 SNORM - Normalisation flux > 400
 NSNORM - Number of sources with flux >= SNORM/degree**2 > 10
 ED - Index of Log NS for flux 1.5
 EU - Index of Log NS for flux>SB > 1.2
\end{verbatim}\end{quote}
\section{PSS}
Searches for and parameterises sources in 2D binned datasets.Can
operate on raw or background subtracted data in conjunction with
a background model. Output is to a source search dataset which
can be displayed using SSDUMP.

See the ASTERIX document USER\_004 for a full description.

\subsection{Introduction}
PSS compares the input dataset with a model composed of a background
model plus point spread function. At a given image position the
comparison is performed over box defined by the PSFPIX parameter.This
box is chosen to include as much of the psf as possible without
increasing execution time to unreasonable values.

Two statistics can be used to compare the data with this model - the
Cash C-statistic or a gaussian correlation statistic. The evaluation
of both these statistics results in a detection significance in sigma.
The equivalent probability of a one tailed normal distribution gives
the probability of the detection being spurious (over the whole sky).

PSS can be run in 4 different modes,
SEARCH - Search the input image for detections above a significance
level specified by the user.
PARAM - Test the value of source parameters at one or more points.
UPLIM - As PARAM mode, but find upper limits at a specified level
of confidence.
UPMAP - Map the upper limit flux in a specified area.
The mode is controlled by the hidden parameter MODE. For modes other
than SEARCH, the value must either be specified on the command line
or be given in reply to the prompt forced by the PROMPT keyword.

PARAM and UPLIM modes require input of source positions. See the
topic "Source\_Lists" above.

\subsection{EXPERT Mode}
The EXPERT logical parameter enables more detailed controls over
the various stages of PSS's processing. PSS remembers your setting
of EXPERT from one to run to the next.

The following values are used for different PSS parameters when
the default NOEXPERT mode is selected, and are offered as defaults
when EXPERT is selected.

\begin{verbatim}
  Parameter       Value         Description

  PSFCON          Y             PSF constant across field
  PSFPIX          68%           The value which encloses 68% of
                                the incident energy
  RESCALE         N             Do not rescale background
  EXTEN           N             Don't fit source extension
  SAMPLE          1             Grid spacing on first pass equals
                                that of input pixels
  FERL            68%           Flux error confidence level
  ASYMMETRIC      N             Produce asymmetric errors
  PERL            90%           Position error confidence level

  MULREJ*         1.0           Controls rejection of spurious
                                peaks in significance map. Lower
                                values permit closer sources.

  * indicates hidden parameters
\end{verbatim}
\subsection{Text Output}
Most PSS text output can be redirected in standard ASTERIX
fashion using the DEV parameter. In PSS parameter has a hidden
default of TERMINAL.

This parameter can be used to achieve a quick source list without
bothering to create a source search results file.
\subsection{Background Rescaling}
Background rescaling is a technique for countering bad background
models - it is available only in EXPERT mode when using the Cash
statistic. In addition to determining a best fit psf scale factor
PSS locally optmises a background scaling factor across the box in
use. This is useful if the gradient of the error of the background
model is not large on the scale of the box size.

To enable rescaling, specify EXPERT mode and RESCALE. PSS will ask
for an initial guess for the background rescaling factor to use on
each pixel - this is 1.0 by default.

\subsection{Confidence Levels}
Confidence levels for errors or upper limits can be specified
in two ways. A simple number is interpreted as a percentage
confidence level and converted into a probability from a one-
tailed distribution. By adding "sigma", or any abbreviation
of it to the number, the value of probability for 1 sigma
($\sim$ 68\%) is multiplied by that number. For example,

\begin{verbatim}
    User Input       Probability

       90.0           0.1
       99             0.01
       3 sigma        1.1446e-5
       5 sigma        5.5788e-13
 \end{verbatim}
This input method is used for the FERL and PERL parameters.

\subsection{Error Form}
PSS can produce either SYMMETRIC or ASYMMETRIC errors at any
confidence level. For a given fitted source parameter (x,y,flux
and optionally background and extension) it is possible to find
both upper and lower limits and the given confidence levels. In
the SYMMETRIC case the average of the two is returned - ASYMMETRIC
returns both.

\subsection{Extension Test}
The PSS extension test involves finding that convolution of the psf
with a gaussian which maximises the significance of the detection.
This option can be turned on in EXPERT mode using the EXTEN switch.

\subsection{Oversampling}
In SEARCH mode PSS evaluates a significance map which by default is
on a grid corresponding to the pixel centres of the input dataset.
By using the SAMPLE parameter keyword, it is possible to specify
that the grid should be either coarser or finer that of the dataset.

For example, SAMPLE=2 means oversample by a factor of two, ie. the
significance map will have a pixel size half that of the input
dataset. Undersampling is specified by making SAMPLE negative. A
value of -2 makes the significance map pixel size twice that of
the dataset.p.
Oversampling should be used with caution as it slows PSS down by at
least a factor of SAMPLE**2 - for the same reason undersampling may
be a useful technique for speeding up the program when source
confusion is known not to be a problem.

\subsection{Using Psfs}
PSS accesses the source point spread function using the ASTERIX PSF
system (see psfs). The psf is accessed using a 2D array of
integrated probabilities. The PSFPIX parameter controls the half-width
of this box. The larger the box, the more accurate the results and
the slower the execution time.

The usual mode of operation is to use a psf which is constant across
the area being in searched. For certain instruments, however, this
is inappropriate - in these cases the PSFCON parameter should be set
false forcing re-evaluation of the psf at every point. In order that
cpu time is kept to a minimum, it is recommended that the psf model
facility is used. In this case, the default of PSFCON is changed to
FALSE.

When PSFCON is false, PSS provides the option to vary the size of
the source box across the field by providing more than one value
for PSFPIX. A table of enclosed energy is printed as a function of
off-axis angle. From these numbers a triplet of psf radii can be
chosen. PSS will then interpolate to decide the box size to use
anywhere on the image.

\subsection{Restarting PSS}
In SEARCH mode the situation may arise where a significance map has
been calculated at great expense in cpu time, but an inappropriate
value of SIGMIN was supplied resulting in no, or fewer than hoped
for sources.

PSS can be restarted using a previously calculated significance map
using the RESTART keyword. Run PSS as you would have otherwise with
the addition of RESTART on the command line. When PSS gets to the
stage where it would normally print the "First pass" message, the
MAP prompt will appear. At this point the name of a significance map
can be entered. PSS will then inform you of the range in the map and
prompt for SIGMIN as usual.

\subsection{Source Subtraction}
In SEARCH and PARAM modes the last option provided by PSS is to
subtract the sources found from the input dataset. This is performed
by scaling the psf template at each source position by the source
flux.
Note that this is liable to generate negative data values, that the
data errors are not modified, and that quality is ignored if present.
\subsection{Source Confusion}
Source confusion is an area of PSS which is handled somewhat crudely.
The first problem is to identify two or more close sources in the
significance map.

The search mechanism is to find distinct peaks in the significance
map which lie above the specified threshold. This method gives the
first problem, namely;
two sources can only be identified as such if there is a "saddle"
in the significance map between them, and the value at the saddle
point lies below the search threshold.

The second source confusion is that of spurious sources caused by
blips in the significance map which tend to occur on the flanks of
bright sources. PSS removes such sources by comparing their separation
with the 50\% enclosed energy radius multiplied by the value of the
MULREJ parameter (default 1). Should two sources be found closer than
this, the less significant (ie. less bright and less psf-like) is
discarded.

\subsection{Diagnostic Mode}
This special mode enables several sets of numbers of to be extracted
from PSS into external data files (usually IMPORT compatible). It is
intended as a tool for use by experienced users and the author to
help in diagnosing problems. Use the DIAG keyword to switch it on.
The options available are,
A - Chi-squared vs. flux per source at image position
B - Chi-squared vs. gaussian convolution of psf
C - Output background rescale map
V - Validate input data thoroughly
Z - Performance figures
To specify options, enter the codes, eg. ABC turns on the first three.
\subsection{Future Plans}
Improvement of treatment of blended sources. More advanced search
of the significance map.
\subsection{Parameters}
\begin{verbatim}
 These parameters may appear in either expert or non-expert mode.

 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Any 2D dataset or primitive array
           GLOBAL.BINDS (Default binned dataset)
 BGND      2    UNIV        The background model dataset. A 2D
                            array matching the INP dataset, or a
                            constant value.
           GLOBAL.MODEL (Default model dataset)
 OUT       3    CHARACTER   The name of the output source search
                            results file. Enter ! for none
            -> GLOBAL.SSDS (Default results dataset)
     Default = SRCLIST
 EXPERT    -    LOGICAL     Run PSS in expert mode? Use default
                            to minimise prompting
     Default = NO
 MODE      -    CHAR        Processing mode. Should be one of
                            SEARCH, PARAM, UPLIM or UPMAP.
     Hidden default = SEARCH
 SOPT      -    CHAR        The statistic to be used. Either
                            CASH or GAUSSIAN.
     Default = CASH
 PSF, MASK, AUX CHAR        Control the source model psf used.
 SLICE     -    CHAR        Specification for range of image
                            to search.
     Default = *:*,*:*
 MAP       -    CHAR        The name of the output significance
                            or upper limits map. Accept default
     Default = !            for no map. Used for input if RESTART
                            (see below) is specified.
           -> GLOBAL.BINDS (Default binned dataset)
 SIGMIN    -    REAL        The threshold for detection in SEARCH
                            mode. Can be either a single value or
                            a pair - one threshold for each pass.
     Default = 5.0
 PLIST     -    CHAR        Source of test positions in PARAM and
                            UPLIM modes. See subtopic "Source
                            Positions" on valid replies.
            - GLOBAL.SSDS (Default results dataset)
 RA        -    CHAR        Right ascension of trial position.
                            Used in PARAM/UPLIM modes. Type ! to
                            terminate.
     Default = !
 DEC       -    CHAR        Declination of trail position. Used
                            in PARAM/UPLIM modes.
 TRY_AGAIN -    LOGICAL     Controls whether PSS will reprompt if
                            no sources were found above the given
                            threshold. Should be set false in batch
                            jobs.
     Hidden default = YES
 FERL      -    CHAR        In UPLIM or UPMAP mode, the confidence
                            level of the upper limits. In SEARCH and
                            PARAM mode, the flux error confidence.
     Default = '1 sigma'
 DEV       -    CHAR        Controls redirection of text output.
     Hidden default = TERMINAL

 These parameters may appear in only in expert mode.

 RESTART   -    LOGICAL     Restart PSS in search mode using an
                            existing significance map.
     Hidden default = NO
 QBAD      -    LOGICAL     Specified YES prevents a source in a
                            bad pixel
     Hidden default = YES
 PSFCON    -    LOGICAL     Psf to be constant across the field?
     Default = YES          Default is YES unless a psf model
                            specification is used.
 PSFPIX    -    INTEGER     The size of the psf box. This size is
                            a single number if PSFCON is true, but
                            may specify a changing size if PSFCON
                            is false (in pixels).
 RESCALE   -    LOGICAL     Rescale the background model? See the
                            "Background Rescaling" subtopic.
     Default = NO
 ISCALE    -    REAL        Initial scale factor. Only used if
                            RESCALE specified.
     Default = 1.0
 SAMPLE    -    INTEGER     Oversampling. Only available in expert
                            and SEARCH modes. See subtopic for more.
     Default = 1
 MULREJ    -    REAL        Multiplier for 50% enclosed energy radius
                            to control rejection of spurious sources.
     Hidden default = 1.0
 EXTEN     -    LOGICAL     Find measure of source extension. Only
                            in EXPERT mode. See "Extension Test" for
                            more.
     Default = NO
 SSUB      -    CHAR        Source subtracted dataset. Output
                            filename for copy of input with sources
                            removed. Default is not output.
           -> GLOBAL.BINDS (Default binned dataset)
     Default = !
 DIAG      -    LOGICAL     Switch on diagnostic mode. See subtopic
                            "Diagnostic mode" for details.
     Hiddfen default = NO
 DOPT      -    CHAR        The diagnostic options.

 ASYMMETRIC     LOGICAL     Produce asymmetric errors. The default is
                            symmetric.
     Default = NO
 PERL      -    CHAR        The positional error confidence level.
                            More than one value can be entered.
                            Separate by commas.
     Default = 90%
 MULTI     -    LOGICAL     Not usually prompted for.
     Hidden default = NO
 MLIST     -    CHAR        Image list file.

 SENSIG    -    REAL        Mapping sensitivity level.
     Default = 5.0
 NOVAR     -    CHAR        IGNORE lack of variance or invent
                            POISSON errors.
     Default = IGNORE
 VALIDATE  -    LOGICAL     Validate input datasets.
     Hidden default = NO
\end{verbatim}\subsection{Examples}
See manual USER\_004 for description of example PSS runs.
\section{SPCONVOL}
SPCONVOL convolves a spatial response which has been created by
SPRESP with a user supplied source model. The idea is that
the resulting modified response can be used for source detection
or parameterisation which is targeted to find a particular
intrinsic source surface brightness distribution.

The source model can be any 2 or 3 dimensional binned dataset,
provided the number of spatial bins is odd in both X and Y,
and that a third energy dimension (if it exists) matches that
in the spatial response.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP        1   UNIV        Dataset whose spatial response is to
                            be updated.
 SOURCE     2   UNIV        The source model

\end{verbatim}\subsection{Examples}
The actual operation of SPCONVOL is completely automatic. The
example here gives the most likely scenario for its use,

Convolving a response in a dataset 'image' with a gaussian of
FWHM 1 pixel,
\begin{quote}\begin{verbatim}
> crepsf psf=analytic mask=gaussian aux=1 pixsize=1 radius=6
             out=gauss_kernel

> spconvol image gauss_kernel
\end{verbatim}\end{quote}
Note that SPCONVOL ignores the axis units of the source image,
and assumes that the pixels are the same size as those defined
in the spatial response.

\section{SPRESP}
SPRESP attaches a spatial response to the input dataset. A spatial
response is a precomputed grid of point spread functions tailored
to a the spatial and energy sub-space occupied by a particular
dataset. They are most useful in applications which make very
intensive use of the psf. If such intensive use is not envisaged it
is better to keep using the normal psf system, as this places fewer
(if any) constraints on the binning of the dataset.

Once the response is attached, a default of RESPFILE will be
offered at the PSF prompt of any application using the psf system.
Accepting this default will generally inhibit all further user
interaction regarding the psf for that application.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP        1   UNIV        Dataset to which response will be
                            attached. Can be a NDF or an event
                            dataset.
     GLOBAL.BINDS (default binned dataset)
 PSF, MASK, AUX CHAR        Control the source model psf used.
 RADIAL     -   LOGICAL     Assume psf is radially symmetric
    Default = Y             about the on-axis detector position?
 PSFCON     -   LOGICAL     Assume psf constant over the field?
    Default = N
 PIXCENT    -   LOGICAL     Produce a pixel centred response. The
    Default = Y             alternative is a vertex centred response,
                            but all Asterix s/w currently uses
                            pixel centring.
 SRES       -   REAL        Spatial resolution in axis units
 CUTOFF     -   REAL        Cut off in fractional amplitude. SPRESP
    Default = 0.001         will truncate the response written to
                            your dataset at the radius where the
                            psf amplitude drops to CUTOFF times its
                            central value.
 RLIMIT     -   INTEGER     Maximum extent of psf, regardles of the
                            CUTOFF value.
    Default = 50

\end{verbatim}\subsection{Examples}
To construct a response for a ROSAT PSPC spectral image. Note that
the order of the axes is not important to SPRESP.
\begin{quote}\begin{verbatim}
   ICL> SPRESP myobs_cube
   SPRESP Version 1.7-3
   ;
   ; Note SPRESP removes any old response before creating a new one
   ;
   Removing existing spatial response...

   ;
   ; You now select the psf to be evaluated
   ;
   PSF system options :

     ANAL        ASCA        EXOLE       PWFC        RADIAL
     RESPFILE    TABULAR     WFC         XRT_HRI     XRT_PSPC

     CSPEC(psf/model,spec[,nbin])
     POLAR(psf,rbin[,abin])        RECT(psf,xbin[,ybin])

   PSF - Choose PSF to use for source model /'ANAL'/ > xrt_pspc
   MASK - PSPC psf option (LIST for descriptions) /'VARPROFILE'/ >
   Loaded XRT psf cube with 10 energy channels

   ;
   ; SPRESP can construct a response which varies only as a function
   ; of radius from (0,0) in the detector, or one which varies in
   ; as function of X and Y. An appropriate default should be offered
   ;
   RADIAL - Assume radial symmetry /YES/ >

   ;
   ; The SRES parameter controls the granularity of access to the
   ; the psf and is specified in the spatial axis units of the
   ; dataset to which the response is being attached.
   ;
   SRES - Radial sampling of psf in degrees >

   ;
   ; SPRESP can construct either pixel centred or vertex centred
   ; responses. No standard Asterix software uses the latter yet
   ; so the default is YES.
   ;
   PIXCENT - Response centre on pixel centre /YES/ >

   ;
   ; The psf stored in the dataset will be truncated at the minimum
   ; of two radii. The first is defined in terms of a cutoff relative
   ; amplitude. The second is an actual radius in pixels of the same
   ; size as the spatial axes of the dataset.
   ;
   CUTOFF - Cutoff amplitude relative to centre of psf /1.E-03/ >
   RLIMIT - Maximum radius of the psf in pixels /50/ > 15

   ;
   ; SPRESP now informs the user of the number of spatial bins into
   ; which the detector plane is divided, and evaluates the psf for
   ; each energy bin.
   ;
   There will be 7 radial bins
   Doing energy band 60.5
   Doing energy band 80.5
   Doing energy band 100.5
   Doing energy band 120.5
   Doing energy band 160.5
   Doing energy band 180.5
   Doing energy band 200.5
   Doing energy band 220.5
\end{verbatim}\end{quote}
\section{SSANOT}
Anotates any image dataset with contents of a source search results
file. Source positions are marked with symbols of the user's
choice, a cross by default. The error region is also drawn, although
this feature can be turned off.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
   INP      1   UNIV        The image to be anotated.
           GLOBAL.BINDS(Default source binned dataset)
   LIST     2   UNIV        SSDS to use for anotation.
             - GLOBAL.SSDS(Default source search results file)
   SYMBOL   3   INTEGER     The symbols to plot.
        Hidden default = 2
   BOLD     4   INTEGER     The boldness if the marker
                            symbol to be used.
        Hidden default = !
   ERROR    -   LOGICAL     Draw error regions if valid.
        Hidden default = YES
   NDF      -   INTEGER     Graph numbers to anotate.

\end{verbatim}\section{SSCARIN}
Outputs data in a source search results file into a sorted binary
format SCAR catalogue. The format chosen is suitable for immediate
cross-correlation with the most commonly used sorting scheme of
SCAR catalogues, decreasing DEC and increasing RA.
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
   INP      1   UNIV        The SSDS to be converted to SCAR.
            GLOBAL.SSDS (Default source search results file)
   OUT      2   CHAR        The name of the output SCAR catalogue.
   SORT     -   LOGICAL     Sort records into descending DEC,
                            ascending RA.
       Default = YES

\end{verbatim}\section{SSDUMP}
Dumps the contents of an SSDS into ASCII form. The destination
of the output is controlled using the standard ASTERIX mechanism
for Text Output which allows output to terminal, file or
hardcopy device.
Fields to be displayed can be selected using the FLDS parameter,
the default is display all fields. Should the number of fields
be so large to fit side by side, a second table will be output.
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
   INP      1   UNIV        The SSDS to be converted to ASCII.
            - GLOBAL.SSDS (Default source search results file)
   DEV      2   CHAR        Text Output device.
      Default = 'TERMINAL'
   HEADER   -   LOGICAL     Output a header describing the meaning
                            of the fields, and other data.
      Hidden default = YES
   HMS      -   LOGICAL     Output celestial coordinates in sexigessimal?
                            Alternative is decimal degrees.
      Hidden default = YES
   ERRORS   -   LOGICAL     Output field errors?
      Hidden default = YES
   FLDS     -   CHARACTER   List of fields to be output. Should be
                            separated by commas.
      Hidden default = '*'

\end{verbatim}\subsection{Examples}
Dump a file to the terminal with full header and error information.
\begin{quote}\begin{verbatim}
ICL> SSDUMP TEST \

SSDUMP Version 1.5-0
--------------------------------------------------------------------------
Results file W1:[ASTERIX88]TEST.SDF;2 contains data from one search.

    N   Searched Dataset                                At
    1   W1:[ASTERIX88]IM.SDF;1                          16-DEC-91 07:34:58

   Dataset was searched by PSS Version 1.5-5

Equatorial coordinates, equinox 2000.0
Positional errors are at 90% confidence, units arcmin
Flux units are Counts, errors are at 68.26895% confidence
Bgnd units are Counts/pix
--------------------------------------------------------------------------

 X_CORR    Y_CORR      RA         DEC      Perr   Signif    Raw Flux   Bgnd
 -0.7943   0.1183  18 14 49.4  +39 41 14   0.80    4.277   55.5- 14.   1.36
                                                               + 15.
 -0.5441  -0.0972  18 35 49.3  +39 47 02   0.72    4.452   33.5- 8.6   1.19
                                                               + 9.2
\end{verbatim}\end{quote}
The same file with header removed, and no errors
\begin{quote}\begin{verbatim}
ICL> SSDUMP TEST NOHEADER NOERRORS \
SSDUMP Version 1.5-0
 -0.7943   0.1183  18 14 49.4  +39 41 14   0.80    4.277   55.5   1.36
 -0.5441  -0.0972  18 35 49.3  +39 47 02   0.72    4.452   33.5   1.19
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
ICL> SSDUMP TEST NOHEADER FLDS="RA,DEC" \
SSDUMP Version 1.5-0
18 14 49.4  +39 41 14
18 35 49.3  +39 47 02
\end{verbatim}\end{quote}
\section{SSGET}
SSGET helps the user to write batch scripts by providing an interface to
source search results files. The program retrieves information
from an SSDS and either echoes it to the terminal (where it can be
trapped and used) or writes it to an ICL environment variable.

The examples subtopic shows ways of using SSGET from both ICL and
the UNIX shell.

\subsection{Data Items}
The following data items are recognised by SSGET,
\begin{verbatim}
   Item            Description

   NSRC            Number of sources stored in the SSDS

   NFILE           Number of files of results merged in SSDS. Has
                   value of 1 unless SSDS was created by SSMERGE.

   *        The value of a field for a particular source

   _ERROR*  The value of the error on a field

   _LABEL   The field label

   _UNITS   The field units
\end{verbatim}
Those items marked with a * require the value of the ISRC parameter
to be specified. The data value for that source is then returned.

The names of the fields in common use in ASTERIX are,

\begin{verbatim}
   CFLUX            - Correct flux
   ENCPSF           - Enclosed psf fraction
   ERRORS           - Positional errors
   EXTEN            - Extension
   EXTSIG           - Significance of extension
   FLUX             - Raw flux
   RA,DEC           - Celestial coordinates
   SIGNIF           - Significance
   X_CORR,Y_CORR    - Image coordinates
\end{verbatim}
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
   INP      1   UNIV        The SSDS to be converted to ASCII.
            - GLOBAL.SSDS (Default source search results file)
   ITEM     2   CHAR        Item of data to be extracted
      Default = 'NSRC'
   ATTR     3   any type    Value of data item. Only useful from
                            ICL
   ISRC     4   INTEGER     Source number whose data is to be
                            accessed
   ECHO     -   LOGICAL     Echo data value to terminal?
      Hidden default = NO

\end{verbatim}\subsection{Examples}
To get the number of source in a SSDS, from ICL,

\begin{quote}\begin{verbatim}
ICL> NSRC = 0
ICL> SSGET myssds NSRC (NSRC)
\end{verbatim}\end{quote}
Use the ECHO parameter from the UNIX shell,

\begin{quote}\begin{verbatim}
csh> set nsrc = `ssget myssds nsrc echo`
\end{verbatim}\end{quote}
To extract information per source use the ISRC parameter, eg. to
get the RA for the 13th source in a results file,

\begin{quote}\begin{verbatim}
ICL> RA = 0.0
ICL> SSGET myssds RA (RA) ISRC=13
\end{verbatim}\end{quote}
You can extract field attributes by appending the attribute name to
the field name, eg.

\begin{quote}\begin{verbatim}
csh> ssget myssds ra_units
degrees
\end{verbatim}\end{quote}
Here is a procedure to print the RA and DEC of each source in source
search results file,

\begin{quote}\begin{verbatim}
#!/bin/csh
set nsrc = `ssget $1 nsrc`
set isrc = 1
loop:
  set ra = `ssget $1 ra isrc=$isrc`
  set dec = `ssget $1 dec isrc=$isrc`
  echo $isrc $ra $dec
  @ isrc++
if ($isrc = $nsrc ) goto loop
\end{verbatim}\end{quote}
\section{SSMERGE}
Merges individual SSDS files together. These combined files may
themselves be merged using SSMERGE. Only fields common to all
the input files are copied to the output.

Each input file specification may take any one of the following
forms,
\begin{itemize}
\item A single file name
\item The @ character followed by the name of ascii file, with
one SSDS file name on each line
\item A wildcard file specification applied to files in the
default directory.
\end{itemize}
No comparison of sources is made to look for repetitions. The
user is informed of progress by an announcement every 10 files
processed.

\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn    Type        Description
 -------    ----    ----        -----------
  INP1-INP10  1     CHAR        File(s) to be merged, or @filename
     Default = !
  OUT         2     UNIV        Merged SSDS containing results of
                                all inputs.
                -> GLOBAL.SSDS (Default source search results file)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 ICL > SSMERGE IPSR%%%%%%_f1 OUT=IPSR_F1
 SSMERGE Version 1.7-0
 Will merge 23 input files
 Merging files 1 to 10
 Merging files 11 to 20
 Merging files 21 to 23
 \end{verbatim}\end{quote}
\section{SSZAP*}
This procedure permits areas of quality in a dataset to be altered
using the results from a source search results file. The procedure
has the following parameters.
\begin{quote}\begin{verbatim}
   SSZAP
 \end{verbatim}\end{quote}
This facility can be used to "remove" sources in n-D datasets -
the resultant dataset may then be more ameanable to fitting etc.

SSZAP is available from both the shell and ICL. Because it is a procedure
all the parameters must be supplied.

\subsection{Parameters}
\begin{verbatim}
    -  Dataset to have quality altered (write access is
                required). This need not be the dataset searched to
                produce the , but the axis units must be the
                same. The  must have at least 2 spatial
                dimensions.

       -  A source search results file

     -  The radius of the circular region around each source
                position. Should be in the units of the spatial axes
                of

      -  The mode with which CQUALITY will be invoked. For
                example, IGNORE or RESTORE.
\end{verbatim}\subsection{Examples}
To set quality temporarily bad in a 5 arcmin circle around each
source in a source search results file.
\begin{quote}\begin{verbatim}
   > SSZAP image srclist 5 IGNORE
   SSZAP Version 1.4-0
   Specify circular regions in ARCMIN
   7238 points had quality modified
 \end{verbatim}\end{quote}
\subsection{Warning}
SSZAP works on the assumption that the image coordinates have the
same units AND origin in both the dataset and source search results
file. Unpredictable behaviour will occur if this not the case.

\chapter{Graphics display commands}
\section{1-Introduction}
There are four groups of commands:

\begin{enumerate}
\item Commands to do with display devices : D***
\item Commands to plot from datasets and manipulate
datasets in a way that affects the plotting : G***
\item Low level plotting commands : G\_***
\item Shortforms for some of the more common
invocations of GSET
\end{enumerate}
The device handling commands are applicable to any ASTERIX
sub-system that uses graphics eg. Image Processing.

Also the low level commands can be used in conjunction with
any sub-package and only require a device to be open.

The basic plotting command, GDRAW, can be used standalone to
open a device, plot a dataset, then close the device again.
Alternatively if a device is already open (eg. opened by DOPEN)
then it will plot the dataset to that device and leave it open.
If a dataset is to be drawn repeatedly then it will be more
efficient to load it into the system using GLOAD. After the
initial plot this will reduce overheads considerably. When
finished with it can be released again with GCLOSE.

Complex plots can be created and manipulated with GMULTI.
This can be used to create a dataset containing the data
for multiple plots and also to determine how those data are
combined and arranged.

The command GSET is used to set plotting attributes and generally
control the form of plotting, not just for data being plotted with
GDRAW, but also for other sub-packages like Image Processing.

\section{2-Devices}
Graphical output may be sent to any device supported by GKS at
your site. Device names are handled by the Starlink GNS package
(see SUN57) and may be specified in any way currently supported
by that package. For a list of devices available at your site
use the DLIST command. To specify one of the devices listed,
use either the full name or an unambiguous abbreviation.

See the separate section on X-window devices.

\section{3-XWindows}
Two procedures are provided for creating suitable X-windows. These
should be invoked before opening the device. The appropriate device
name is then XWINdow

\subsection{CXMAKE*}
A procedure which creates a COLOUR X-window with a 128 colours
available. This procedure will generate an error unless XDISPLAY
has been invoked before either CXMAKE itself, or ICL if this is
being used. Invocation is,
\begin{quote}\begin{verbatim}
> cxmake [arg]
\end{verbatim}\end{quote}
An optional integer argument permits additional windows to be
opened, called GKS\_(3800+ARG). The GKS X-windows supported run
from 3800 to 3803.

Note that on mono-chrome terminals, the procedure MXMAKE should
be used rather then CXMAKE.

\subsubsection{Examples}
This is the error you get if you haven't done an XDISPLAY
\begin{quote}\begin{verbatim}
> cxmake
X Toolkit Error: Can't Open display
%NOMSG, Message number 03AB8204
\end{verbatim}\end{quote}
Create the default window
\begin{quote}\begin{verbatim}
   > XDISPLAY
   > CXMAKE
   ...
   GKS_3800 window appears
\end{verbatim}\end{quote}
Create a second window
\begin{quote}\begin{verbatim}
   > CXMAKE 1
\end{verbatim}\end{quote}
\subsection{MXMAKE*}
A procedure which creates a MONO-CHROME X-window for use by PGPLOT.
This procedure will generate an error unless XDISPLAY has been
invoked before either MXMAKE itself, or ICL if this is being used.
Invocation is,
\begin{quote}\begin{verbatim}
> mxmake [arg]
\end{verbatim}\end{quote}
An optional integer argument permits additional windows to be
opened, called GKS\_(3800+ARG). The GKS X-windows supported run
from 3800 to 3803.
\subsubsection{Examples}
This is the error you get if you haven't done an XDISPLAY
\begin{quote}\begin{verbatim}
> mxmake
X Toolkit Error: Can't Open display
%NOMSG, Message number 03AB8204
\end{verbatim}\end{quote}
Create the default window
\begin{quote}\begin{verbatim}
> xdisplay
> mxmake
...
GKS_3800 window appears
\end{verbatim}\end{quote}
Create a second window
\begin{quote}\begin{verbatim}
> mxmake 1
\end{verbatim}\end{quote}
\section{4-Graphics control}
Graphical output in ASTERIX is controled by the Graphics-Control-Block
(GCB) - a block of shared memory containing flags, switches and
attribute values. All commands that produce graphical output
interogate the GCB to determine the precise form of the output.

All quantities in the GCB can be set, cancelled and displayed
by the command GSET. Commands in sub-packages such as image
processing also set values in the GCB.

Values available for various plotting attributes are given in
sub-sections below.

\subsection{1-Linestyle}
The following linestyles are available:
\begin{verbatim}
              1 - full line
              2 - long dashes
              3 - dash-dot-dash-dot
              4 - dotted
              5 - dash-dot-dot-dot
\end{verbatim}
\subsection{2-Linewidth}
Line widths can be specified as integer multiples of the default
width up to a maximum of 21. The effect will depend on the
resolution of the device, but for high resolution devices such
as laser printers the following should be valid:
\begin{verbatim}
             1 0.13mm
             2 0.25mm
             3 0.38mm
             4 0.50mm
             5 0.63mm
             6 0.75mm
             7 0.88mm
             8 1.00mm
\end{verbatim}
\subsection{3-Line colour}
A number of colours at the lower end of the colour index range
are reserved for line plotting, depending on the total number
of colours available. The remainder are made available for
pixel plotting. The command DSHOW will show the ranges for
both which are available on the current device. The maximum
range reserved for line plotting is as follows:
\begin{verbatim}
      0 black (background)     1 white
      2 red                    3 green
      4 blue                   5 cyan
      6 magenta                7 yellow
\end{verbatim}
\subsection{4-Character fonts}
The following fonts are available:
\begin{verbatim}

          1 - normal (simple)
          2 - roman
          3 - italic
          4 - script
\end{verbatim}
Fonts and other properties of text can also be changed by
embedding escape sequences into text. The following are
available:
\begin{verbatim}
    \fn - switch to normal font  \fr - switch to roman font
    \fi - switch to italic font  \fs - switch to script font
    \gx - print greek letter corresponding to roman letter x
    \u  - move up    \d - move down     \b - backspace
    \\  - backslash  \A - Angstrom      \(n) - character n
\end{verbatim}
\subsection{5-Character size}
The default character height is 1/40th of the smaller dimension
of the display surface. Any fraction or multiple of this may be
specified.

\subsection{6-Character boldness}
Characters are made bolder by drawing with a thicker line.
Integer multiples of the default line thickness up to a
maximum of 21 may be specified. The effect will vary
depending on the resolution of the plotting device.

\subsection{7-Symbols}
The following symbols are available for marking points -
the centre of the symbol usually marks the point.
Character size and boldness also applies to symbols.
\begin{verbatim}
  0  open square                16 filled square
  1  dot                        17 filled circle
  2  cross                      18 filled five-point star
  3  asterisk                   19 larger open square
  4  open circle                20 very small open circle
  5  diagonal cross             21           ^
  6  open square                22           |
  7  open triangle              23    open circles of
  8  open circle with cross     24    increasing size
  9  open circle with dot       25           |
  10 open 'pin-cushion'         26           v
  11 open diamond               27 very large open circle
  12 open five-point star       28 left arrow
  13 filled triangle            29 right arrow
  14 open cross                 30 up arrow
  15 open six-point star        31 down arrow

       32-127  the corresponding ASCII character
 \end{verbatim}
\section{DOPEN}
This opens a graphics display device and makes it available to
any ASTERIX sub-package that uses graphics. If another device
is already open, it will be closed first. The display surface
may optionally be divided into hard zones which are accessed
sequentially by plotting commands.

See the separate sections on Devices and XWindows.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    DEV        1      CHAR      Device name
    XZONES     2      INT       Hard zones horizontally
    YZONES     3      INT       Hard zones horizontally

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > DOPEN IKON                open IKON with one zone

 > DOPEN VWS 2 2             open VWS with 2 by 2 zones
\end{verbatim}\end{quote}
\section{DCLOSE}
Closes the device currently open. The device may have been opened
with DOPEN or with one of the sub-package loading commands, GLOAD,
ILOAD etc.

\section{DERASE}
Clears the display surface or moves to the next hard zone.

\section{DLIST}
Lists the device names available at your site.

\section{DSHOW}
Shows the device currently open and the sub-package using it.
It also gives information on the device, such as cursor availability
and colour capability.

\section{DSPOOL}
This is a system/site dependent procedure which automatically
spools output to a hardcopy device. If it doesn't work then
your ASTERIX installer may need to change it. Depending on
the system, the device may need to be closed before spooling.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    -          1      CHAR      Device name
   DEL         -      LOGICAL   Delete file after spooling
         (Default = N)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> DSPOOL POST         spool to postscript device

> DSPOOL INKJET DEL   spool to inkjet and delete file
\end{verbatim}\end{quote}
\section{GDRAW}
Plots data loaded with GLOAD, or a given primitive array, NDF
or multiple dataset such as created by GMULTI. If no device
is open then one will be prompted for and will be closed again
after plotting.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    INP        1      CHAR      Data to be plotted
    DEV        2      CHAR      Graphics device
    ID         -      LOGICAL   Put ID/timestamp in corner of plot

\end{verbatim}\section{GMULTI}
Used to form complex plots of more than one dataset. It has the
following modes:

\begin{verbatim}
       NEW     - create a new multiple dataset
       APPend  - add further data to an existing multiple dataset
       DELete  - remove data from a multiple dataset
       SPLit   - create multiple dataset by splitting 2D dataset
       SHOw    - list content and status of dataset
       COMbine - combine data into single plots (overlays)
       LAYout  - define layout of plots on display surface
       HELp    - list above options and reprompt
 \end{verbatim}
More detailed information on the different modes is given in the
specific sub-section.

\subsection{NEW}
Collects together NDFs into a multiple dataset, from which complex
plots can be created. If an existing multiple dataset is given as
input then its contents will be merged into the new dataset. The
name of each input dataset is entered into the index and may
subsequently be viewed using the SHOW mode.

\subsection{APPend}
As for NEW mode except that the datasets are appended to an existing
multiple dataset.

\subsection{DELete}
Deletes a specified NDF from a multiple dataset. The data relating
to a particular NDF number can be determined using SHOW mode. This
can also be useful to show realignments after deletion.

\subsection{SPLit}
Where 1D data are stored in a 2D array, such as a spectral series,
this mode can be used to split them into separate components of a
multiple dataset.

\subsection{SHOw}
Lists the content of a multiple dataset, the composition of any
complex plots, and any specified plot layout.

\subsection{LAYout}
Allows the user to specify how plots are arranged on the display
surface in simple terms of numbers horizontally and vertically.
This is overidden by individual positions set by GSET POS. Plots
are displayed starting from the top left-hand corner.

\subsection{COMbine}
Allows NDFs to be combined together into one or more single plots.
A base plot is specified, which determines axes, position (where
specified), titles, labels etc. Other NDFs may be specified as
overlays to this. Unless specifically overidden with GSET POS,
plots are arranged starting with Plot 1 in the top left-hand
corner.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
   SWITCH      1      CHAR      NEW, APP, SHO etc
   INP1        2      CHAR      Input dataset for NEW or APP modes
   INP2        3      CHAR        "      "     "   "  "   "    "
    .          .       .
   INP24       25     CHAR        "      "     "   "  "   "    "
   OUT         -      CHAR      Output dataset for NEW mode
   INOUT       -      CHAR      Existing dataset to be modified
   NDF         -      INT       NDF number to DELete
   CANCEL      -      LOGICAL   Cancel LAYout or COMbine mode
      (Hidden default = N)
   NX          -      INT       Plots horizontally for LAYout mode
   NY          -      INT       Plots vertically for LAYout mode
   PLOT        -      INT       Plot number for COMbine mode
   BASE        -      INT       Base NDF for plot
   OVLY        -      CHAR      List of NDFs to overlay on BASE
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > GMULTI NEW ds1 ds2 ds3 OUT=mds \     - form new multiple dataset
                                          from three inputs
 > GMULTI APP ds4 INOUT=mds             - add another dataset

 > GMULTI LAY NX=1 NY=4 INOUT=mds       - arrange plots on top of
                                          each other
 > GMULTI COM PLOT=1 BASE=1 OVLY=2,4    - overlay NDFs 2 and 4 over
                                          base plot 1
 > GMULTI COM PLOT=1 BASE=3 OVLY=*      - overlay all other NDFs in
                                          dataset over NDF 3
 > GMULTI COM PLOT=2 BASE=1 OVLY=3:5    - overlay 3 to 5 over 1

 > GMULTI SHO \                         - list contents and plot
                                          arrangements
\end{verbatim}\end{quote}
\section{GSET}
Sets various parameters and attributes which control the way data
are plotted. The specific area of control is selected via the
SWITCH parameter. The value 'HELP' will list all the other switches
available and reprompt. Each of those values is described below
in specific sub-sections. The command has three modes, SET which
is the default, SHOW and CANCEL which are invoked by including the
respective keywords on the command line. The latter two modes will
accept the wildcard '*' as a value for SWITCH. The keyword OFF can be
used as an alternative to CANCEL.

\subsection{DEFaults}
Sets the default linewidth, linestyle, colour and font. These
attributes will be used except where specifically overidden. Note
that linewidth will also determine the character boldness.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   WIDTH     INT       Multiple of default linewidth
   STYLE     INT       Linestyle
   COLOUR    INT       Colour index
   FONT      INT       Font number

\end{verbatim}\subsection{AXEs}
Sets attributes for both axes. Some attributes only apply
to both axes and cannot be applied individually. Others may
be applied individually using XAXis and YAXis. These are
distinguished in the parameter list.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
 only apply to both axes
   RADEC      LOG       Display RA/DEC axes
   SCALE      LOG       Scale both axes equally
   WIDTH      INT       Linewidth for axes
   FONT       INT       Font for numeric lables
   SIZE       REAL      Character size of numbers
   BOLD       INT       Character boldness
   COLOUR     INT       Colour index

 setable for individual axes
   OPT       CHAR      Axis options code
   LO        REAL      Left limit of axis
   HI        REAL      Right limit of axis
   TICK      REAL      Spacing of major ticks
   DIV       INT       Number of minor divisions between major ticks
   LOG       LOG       Logarithmic axis
\end{verbatim}\subsubsection{Axis options}
The following codes can be combined for fine control of the way
axes are plotted. They are given in response to the OPT parameter.
A : draw x=0 or y=0 line
B : draw axis at bottom (x) or left (y) edge
C : draw axis at top (x) or right (y) edge
G : draw grid of vertical (x) or horizontal (y) lines
I : invert tick marks to go outside axes
N : put numbers along bottom (x) or left (y) side
M : put numbers along top (x) or right (y) side
P : project major tick marks outside axes
T : draw major tick marks at major coordinate intervals
S : draw minor ticks marks
V : write numbers perpendicular to axis (y only)
The default value used by commands that produce graphical output
is 'BCNST'. A code 'BC' will just cause a plain box to be drawn
and a blank code will suppress the axes.

\subsection{XAXis}
Sets control parameters for x-axis only.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   OPT       CHAR      Axis options code
   LO        REAL      Left limit of axis
   HI        REAL      Right limit of axis
   TICK      REAL      Spacing of major ticks
   DIV       INT       Number of minor divisions between major ticks
   LOG       LOG       Logarithmic axis
\end{verbatim}\subsection{YAXis}
Sets control parameters for y-axis only.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   OPT       CHAR      Axis options code
   LO        REAL      Bottom limit of axis
   HI        REAL      Top limit of axis
   TICK      REAL      Spacing of major ticks
   DIV       INT       Number of minor divisions between major ticks
   LOG       LOG       Logarithmic axis
\end{verbatim}\subsection{LABels}
This mode will set the text labels on both axes simultaneously.
It is probably most useful for setting things like font and size
rather than the text itself, although setting TEXT=" " is a way
of suppressing the labels altogether.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   TEXT      CHAR      Text of axis label
   FONT      INT       Font number
   BOLD      INT       Character boldness
   SIZE      REAL      Multiple of default character size
   OFFSET    REAL      Offset in multiples of character height
   COLOUR    INT       Colour index
\end{verbatim}\subsection{XLAbel}
Set x-axis label. The default text is that in the axis component
of the dataset, if it exists. A response given to the parameter
TEXT will override this.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   TEXT      CHAR      Text of axis label
   FONT      INT       Font number
   BOLD      INT       Character boldness
   SIZE      REAL      Multiple of default character size
   OFFSET    REAL      Offset in multiples of character height
   COLOUR    INT       Colour index
\end{verbatim}\subsection{YLAbel}
Set x-axis label. The default text is that in the axis component
of the dataset, if it exists. A response given to the parameter
TEXT will override this.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   TEXT      CHAR      Text of axis label
   FONT      INT       Font number
   BOLD      INT       Character boldness
   SIZE      REAL      Multiple of default character size
   OFFSET    REAL      Offset in multiples of character height
   COLOUR    INT       Colour index
\end{verbatim}\subsection{TITles}
Allows up to 5 title lines at top of plot. By default the TITLE
component in the dataset is used if it exists. Setting TEXT=" "
will suppress any titles. The default line number offered at the
prompt is such that the default behaviour is to add another line.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   NUM       INT       Line number (1 at top)
   TEXT      CHAR      Text of axis label
   FONT      INT       Font number
   BOLD      INT       Character boldness
   SIZE      REAL      Multiple of default character size
   COLOUR    INT       Colour index
   JUST      CHAR      Justification (L, C or R)
\end{verbatim}\subsection{POSit}
Sets the position of the graph window in Normalised Device
Coordinates (0 .. 1) or millimetre. If any given coordinate
is > 1 then all are assumed to be millimetre, otherwise NDC
are assumed.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   X1        REAL      Left edge of graph window
   X2        REAL      Right "   "    "      "
   Y1        REAL      Bottom    "    "      "
   Y2        TOP       Top   "   "    "      "
\end{verbatim}\subsection{POLyline}
Flags that a polyline is to be drawn through 1D data points.
Also optionally sets the linestyle, linewidth, and
colour index.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   STYLE     INT       linestyle
   WIDTH     INT       linewidth
   COLOUR    INT       colour index
\end{verbatim}\subsection{STEpline}
Flags that a stepped line is to be drawn through 1D data points.
Also optionally sets the linestyle, linewidth, and
colour index.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   STYLE     INT       linestyle
   WIDTH     INT       linewidth
   COLOUR    INT       colour index
\end{verbatim}\subsection{POInts}
Flags that 1D data points are to be marked individually
with the specified symbol.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   SYMBOL    INT       plotting symbol number
   SIZE      REAL      symbol size
   BOLD      INT       symbol boldness
   COLOUR    INT       colour index
\end{verbatim}\subsection{ERRors}
Flags that error bars are to be drawn through 1D data points.
The following types of error bars are available and are
specified by the SHAPE parameter:
1 Simple Cross 2 Barred Cross
3 Rectangle 4 Diamond
5 Ellipse
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   SHAPE     INT       type of error bars
   WIDTH     INT       linewidth
   COLOUR    INT       colour index
\end{verbatim}\subsection{PIXel}
Flags that 2D data are to be plotted as pixels. The scaling mode
and limits over which the data are scaled may also be specified.
The following scalings are available:
LINear LOGarithmic SQRt PARabolic
SINusoidal CYClic HIStogram-equalisation
Further details of the different scalings can be found under the
entry for ISCALE.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   SCALING   CHAR      Scaling type
   MIN       REAL      Minimum scaling level
   MAX       REAL      Maximum scaling level
   CYCLES    INT       For cyclic scaling
\end{verbatim}\subsection{CONtour}
Flags that 2D data are to be contoured at the given levels. If
no levels are specified, then defaults will be chosen. If PIXEL
and CONTOUR flags are set then contours will be plotted over
the pixel plot.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   LEVELS    REAL      Contour levels
   STYLE     INT       Linestyle of each contour level
   WIDTH     INT       Linewidth of each contour level
   COLOUR    INT       Colour of each contour level
\end{verbatim}\subsection{SURface}
Flag that 2D data are to be drawn as a pseudo-3D surface. This
overrides both PIXEL and CONTOUR flags. The apparent eye position
above the horizontal is controled via the VIEW parameter.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   VIEW      REAL      Angle of 'eye' above horizontal
\end{verbatim}\subsection{NOTes}
Allows textual annotations to be put on the plot at random
positions and angle. When prompting for note number the
default behaviour is such that a new note will be created.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   NUM       INT       Note number
   TEXT      CHAR      Text of annotation
   X         REAL      X-position (x-axis coords)
   Y         REAL      Y-position (y-axis coords)
   ANGLE     REAL      Angle in deg. anticlockwise from horizontal
   JUST      CHAR      Justification (L, C or R)
   FONT      INT       Font number
   BOLD      INT       Character boldness
   SIZE      REAL      Character size
   COLOUR    INT       Character colour
\end{verbatim}\subsection{MARks}
Allows individual positions to be marked with a symbol. When
prompting for the marker number, the default behaviour is such as
to create a new one.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   SYMBOL    INT       Symbol number
   X         REAL      X-position (x-axis coords)
   Y         REAL      Y-position (y-axis coords)
   SIZE      REAL      Marker size
   BOLD      INT       Marker boldness
   COLOUR    INT       Marker colour
\end{verbatim}\subsection{SHApes}
Allows a variety of shapes to be overlayed on the graph. The
shapes are identified by name and defined by position (X,Y)
and the parameters DATA1, DATA2, DATA3 (not all apply to all
shapes).
Shapes Defining quantities
VECTOR Offsets DATA1 from X and DATA2 from Y
CIRCLE Radius DATA1 from centre (X,Y)
ELLIPSE Centre (X,Y), Semi-major axis DATA1,
Ratio of axes major/minor DATA2 and
position angle DATA3
BOX Centre (X,Y) and lengths of sides
DATA1 and DATA2
All positions and dimensions are in axis units, angles are
in degrees. For circles and ellipses the axes need to be
equally scaled. This is normally the default for 2D data,
but can be forced using 'GSET AXES SCALED'.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   TYPE      CHAR      Shape type
   X         REAL      X-position (x-axis coords)
   Y         REAL      Y-position (y-axis coords)
   DATA1     REAL      Other
   DATA2     REAL        Defining
   DATA3     REAL          Parameters
   WIDTH     INT       Line width
   STYLE     INT       Linestyle
   COLOUR    INT       Line colour

\end{verbatim}\subsection{GRId}
Flags that a coordinate grid is to be drawn over a 2D plot.
Celestial, ecliptic and galactic frames are available,
determined by the FRAME parameter. The parameter POS
determines where the labels are put.

\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
    FRAME    INT       1=celestial 2=ecliptic 3=galactic
    POS      INT       1=edge 2=centre 3=outside

\end{verbatim}\subsection{KEY}
Flags that a key is to be drawn at the side of the plot. The form
of the key is controlled by the OPT parameter. This facility is
fairly basic at the moment and OPT has the following values:
P - pixel key (colour bar)
C - show contour levels
The format of the textual labels may also be controlled.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   OPT       CHAR      Control options
   SIZE      REAL      Character size
   FONT      INT       Character font
   BOLD      INT       Character boldness
\end{verbatim}\subsection{COLours}
Sets the colour table for 2D pixel plots. In default mode
it prompts for an external table to read in. This may be an
HDS object of the right type and size, or the number of a
standard ASTERIX colour table. There are two other modes
invoked by including one of the following keywords: NEG produces
the negative of the present table (note that using NEG again does
not get back to the original table - use NONEG); RGB produces a
special 3-colour table for use with images created by MKCOLIM.
\subsubsection{Parameters}
\begin{verbatim}
  Keyword    Type      Description
  -------    ----      -----------
   TABLE     CHAR      Table to read in
   NEG       LOG       Invert table
   RGB       LOG       Create 3-colour table

\end{verbatim}\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
   SWITCH      1      CHAR      Control parameter
   INP         2      CHAR      Dataset where appropriate
   NDF         3      CHAR      NDF numbers for multiple dataset
   CANCEL      -      LOG       Cancel selected quantities
   OFF         -      LOG       Same as cancel
   SHOW        -      LOG       Show selected quantities

  Other parameters are listed with the SWITCH modes to which
  they apply
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > GSET * SHOW               - show all attributes etc.

 > GSET * CANCEL             - cancel all attributes etc

 > GSET HELP                 - list all switch values

 > GSET AXES                 - set values for both axes

 > GSET PIX CANCEL           - cancel pixel flag and attributes

 > GSET SHOW CONT            - show contour data
\end{verbatim}\end{quote}
\section{GLOAD}
Load dataset to be plotted by GDRAW. This will reduce overheads
if a particular dataset is being plotted repeatedly. If no device
is open then this will also be prompted for.

It is intended that in future this command will be used to load
non-NDF data into a form accessible to GDRAW.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    INP        1      CHAR      Input dataset
    DEV        2      CHAR      Graphic device
    XZONES     3      INT       x zones
    YZONES     4      INT       y zones

\end{verbatim}\section{GCLOSE}
Releases the data loaded by GLOAD, and if GRAFIX is the only
sub-package using the device, then this will also be closed.

\section{POS}
A low-level command to select the position of the viewport in Normalised
Device Coordinates (0..1). Requires a device to be open. Default
behaviour is to select opposite corners with the cursor. This can
be overidden, however, by use of the KEY parameter which will force
prompting at the keyboard.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
     KEY       -      LOGICAL   entry via keyboard
     X1        1      REAL      left hand edge
     X2        2      REAL      right hand edge
     Y1        3      REAL      bottom edge
     Y2        4      REAL      top edge
\end{verbatim}\section{WINDOW}
Maps world (user) coordinates onto viewport defined by G\_POS.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
     X1        1      REAL      x-coord of left hand edge
     X2        2      REAL      x-coord of right hand edge
     Y1        3      REAL      y-coord of bottom edge
     Y2        4      REAL      y-coord of top edge

\end{verbatim}\section{AXES}
Plots the axis perimeter around the viewport defined by G\_POS
using coordinates defined by G\_WINDOW. The parameter values
control the form of the axes.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
   XOPT        1      CHAR      axis option for x-axis
   XTICK       2      REAL      major tick interval on x-axis
   XDIV        3      INT       no. of sub divisions between major ticks
   YOPT        1      CHAR      axis option for y-axis
   YTICK       2      REAL      major tick interval on y-axis
   YDIV        3      INT       no. of sub divisions between major ticks

\end{verbatim}\section{MOVE}
Low-level command to move imaginary pen to a new position without
plotting. The position is selected by cursor unless the KEY
parameter is invoked.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    KEY        -      LOGICAL   entry via keyboard
    X          1      REAL      New x position
    Y          2      REAL      New y position

\end{verbatim}\section{PLOT}
Draw a line from current pen position to new position defined
by cursor or via parameters if KEY is included on the command
line.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    KEY        -      LOGICAL   entry via keyboard
    X          1      REAL      New x position
    Y          2      REAL      New y position

\end{verbatim}\section{LINE}
Draw a polyline through the points given by positioning
the cursor, or entered via parameters if KEY is included
on the command line.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    KEY        -      LOGICAL   entry via keyboard
    X          1      REAL      Array of x positions (HDS)
    Y          2      REAL      Array of y positions (HDS)
\end{verbatim}\section{MARK}
Mark points with symbols at positions given by cursor
or by parameter if KEY is specified.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    KEY        -      LOGICAL   entry via keyboard
     X         1      REAL      Array of x positions (HDS)
     Y         2      REAL      Array of y positions (HDS)
\end{verbatim}\section{LABEL}
Puts simple labels on axes.
\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
   XLBL        1      CHAR      x label
   YLBL        2      CHAR      y label
   TOPLBL      3      CHAR      top label
\end{verbatim}\section{TEXT}
Put text at an arbitrary position and orientation. The position
is specified by cursor, unless KEY is included on the command line.
\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
   KEY         -      LOGICAL   entry via keyboard
   TEXT        1      CHAR      text
   JUST        2      CHAR      justification L)eft C)entre R)ight
   ANGLE       3      REAL      angle in deg anti-clock from horiz
   X           -      REAL      x position
   Y           -      REAL      y position

\end{verbatim}\section{CURS}
A general command for reading the cursor position from an appropriate
display device. If a multiple plot is being displayed then the plot
number will be prompted for. In default mode the command returns a
single position. If the option keyword BOX is used on the command
line then a second position will be returned, thereby defining a box.
Including MARK on the command line will cause the point or box to be
marked.

\subsection{Parameters}
\begin{verbatim}
  Keyword   Position  Type      Description
  -------   --------  ----      -----------
    PLOT       -      INT       Plot number for multiple plots
    MARK       -      LOG       Mark point or box
    BOX        -      LOG       Define a box with a second point
    X1         1      REAL      x pos of first point
    Y1         2      REAL      y pos of first point
    X2         3      REAL      x pos of second point
    Y2         4      REAL      y pos of second point

\end{verbatim}\section{Shortforms}
The following shortform commands have been brought forward from the
old graphics system for people who whinge about having to type a
couple of extra characters when using GSET:
\begin{verbatim}
 XLABel     - set x-axis label
 YLABel     - set y-axis label
 XAXis      - set x-axis attributes
 YAXis      - set y-axis attributes
 XLOG       - make x-axis logarithmic
 YLOG       - make y-axis logarithmic
 XYLOG      - make both axes logarithmic
 AMULTI     - add to multiple dataset
 DMULTI     - delete from multiple dataset
 COLBAR     - put colour bar at side of plot
 ALEGend    - add title/legend line
 DLEGend    - delete title/legend line
 GREYSCale  - select greyscale
\end{verbatim}
\chapter{Spatial file commands}
The spatial descriptor (or ARD) file has been introduced into Asterix
as a method of defining complex areas on a 2-d surface. These files
are simple text files and may be created with an editor or by running
the REGIONS application (or an ICL procedure such as WELLARD). They
may then be used in the QUALITY program (to set pixels BAD or GOOD) or
in the future, they will be used to define regions to sort data within
in e.g. XSORT. For a full description of the file format see the Starlink
document SUN183.

\section{REGIONS}
Produces a file containing a description of a region on a 2-d image.
The textfile produced (an ARD file) may be used in the QUALITY program
to set pixels BAD (or GOOD).

\subsection{FILE FORMAT}
The ARD file produced by REGIONS, is a simple ascii text file which
may be modified with any suitable editor. A full description of the
format is contained in the Starlink document (SUN183).

\subsection{SHAPES}
The shapes allowed in the ARD file are:

\begin{verbatim}
 CIRCLE, ANNULUS, BOX, POLYGON, ELLIPSE, COLUMN, ROW, LINE, PIXEL
\end{verbatim}
The first five have their standard meanings. Column means a line of pixels
with the same X value, upto ten columns may be described in a single
shape. Row means a line of pixels with the same Y value, again up to
ten rows may be described at a time. Line means a single line of pixels
joining one point on the image to another. Pixel, means a single pixel,
up to ten discrete pixels may be selected.

\subsection{WARNING}
The ARD language is defined in a serial fashion and it is very possible
for entries early in the file to be modified by later entries, if they
contain EXCLUDE regions. e.g. a file containing a definition of an
annulus followed by a circle will behave as you would expect, however, if the
annulus and circle are the other way round the circle definition will
probably be completely superceeded by the annulus. At present this
effect is limited to ANNULI, beware that the XSPOKES language definition
contains an annulus. When producing a file containing the ribs definition,
the ribs should be selected {\em first}.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 ARDFILE     1   CHARACTER  Name of the output text file
 NEW             LOGICAL    Open a new file (Y) or append to an
                            old one (N) ?
 EXCLUDE         LOGICAL    Is this an EXCLUDE (Y) or INCLUDE (N) region ?
 SHAPE           CHARACTER  Shape of the area e.g. circle, ellipse...
 XCENT           REAL       Centre of the shape in the X axis
 YCENT           REAL       Centre of the shape in the Y axis
 XWIDTH          REAL       Width of the box (axis units)
 YWIDTH          REAL       Height of the box (axis units)
 RADIUS          REAL       Radius of the circle (axis units)
 SMINOR_AXIS     REAL       Semi minor axis (axis units)
 SMAJOR_AXIS     REAL       Semi minor axis (axis units)
 ORIENT          REAL       Orientation of the ellipse (degrees).
                            Anti-clockwise from the X axis
 NPTS            INTEGER    Number of points in the polygon
 X1              REAL       X axis value of the first polygon point
 Y1              REAL       Y axis value of the first polygon point
 .
 .
 X10             REAL       X axis value of the tenth polygon point
 Y10             REAL       Y axis value of the tenth polygon point
 NCOL            INTEGER    Number of columns if shape is column
 NROW            INTEGER    Number of rows if shape is row
 XEND1           REAL       X value of one end of the line
 YEND1           REAL       Y value of one end of the line
 XEND2           REAL       X value of one end of the line
 YEND2           REAL       Y value of one end of the line
 NPIX            INTEGER    Number of pixels
 INNRAD          REAL       Inner radius of an annulus (axis units)
 OUTRAD          REAL       Outer radius of an annulus (axis units)
 MORE            LOGICAL    Choose another shape ?

\end{verbatim}\subsection{EXAMPLE}
To define an annular region, of inner radius 0.1 degrees and outer,
0.2 degrees in a file called ANN.ARD
ICL> REGIONS ANN.ARD
NEW - Create a new file (Y) or append to an old file (N) > YES
EXCLUDE - is this an exclude region > NO
SHAPE - Shape of the region > ANNULUS
XCENT - X centre of region > 0.2
YCENT - Y centre of region > 0.25
INNRAD - Inner radius of annulus > 0.1
OUTRAD - Outer radius of annulus > 0.2
MORE - Enter another shape /NO/ > NO
ICL> cat ann.ard
COMPOSITE ANNULUS
CIRCLE 0.2000000 0.2500000 0.2000000
.AND. .NOT. CIRCLE 0.2000000 0.2500000 0.1000000
END COMPOSITE
\section{SETBAD*}
An ICL procedure which wraps up the WELLARD command and the quality
setting command. It sets the quality of a complicated area on an image
to BAD.

\section{WELLARD*}
This is an ICL procedure which allows a user to interactively select a
region on an image and write it into a spatial descriptor (ARD) text file.
It makes extensive use of the MENU system and hence needs to be used
on an image which has been drawn with a PGPLOT graphics package such
as the 'I' routines. The ARD file produced, may be used in the QUALITY
program to set pixels, within the region specified, to be BAD (or GOOD).
Specify FSEL=YES with one of the QUALITY commands to use this option.
To set the pixels bad you can use the command ARDQUAL.

A common use may be to set the areas underneath the ribs in an XRT image
bad.

In future, it will be possible to use an ARD file in the sorting routines,
e.g. XSORT, to define an area within which to sort data.
\subsection{Method}
WELLARD lets the user specify a region on an image by selecting a shape
from a menu and setting its extent using the cursor. A textfile
containing the shapes selected is produced. You may select as many shapes
as you like, however, you should heed the WARNING item.
The initial menu displayed by WELLARD, contains the simple shapes, box, circle
and annulus, a QUIT button (which means exit and save the ARD file),
and an XRT button.

The XRT button produces a further menu with buttons
to write the PSPC rib structure into the file or include sources
found by PSS. The latter calculates the radius of a circle at the
off-axis angle of the source which will include 95\% of the source
counts (at an energy of 0.2 keV), this uses the PSF determination
calculated by I.Georgantopolous. A circle of this radius is then written
into the ARD file.

\subsection{WARNING}
The ARD language is defined in a serial fashion and it is very possible
for entries early in the file to be modified by later entries, if they
contain EXCLUDE regions. e.g. a file containing a definition of an
annulus followed by a circle will behave as you would expect, however, if the
annulus and circle are the other way round the circle definition will
probably be completely superceeded by the annulus. At present this
effect is limited to ANNULI, beware that the XSPOKES language definition
contains an annulus. When producing a file containing the ribs definition,
the ribs should be selected {\em first}.
\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 ARDFILE     1   CHARACTER  Name of the output text file
 SRCLIST         CHARACTER  The name of the source file from PSS
                            (if the PSS option is selected)

\end{verbatim}\chapter{HDS general commands}
A set of commands for displaying and manipulating HDS objects.
These are general facilities and no assumptions are made about
the structure of the HDS datasets used.

A 'Current Object' is stored in the Global parameter HDSOBJ and
most commands will present this as the default object to be taken
as input. If a different object is specified this will
subsequently become the Current Object.

You can obtain a description of HDS in SUN92. See also
the 'Standard ASTERIX Structures' document PROG\_002 in
AST\_DOCS: (VMS) or \$AST\_DOCS (UNIX) for a description of
what you will find in ASTERIX files.

\section{HCOPY}
Copies a data object from one place to another. The copy is
recursive, so if a structure is specified as input then
everything below that level will be copied to the output
destination.

NOTE: if an output object of the name given already exists then
it will be deleted and replaced by a new object of that name. The
exception to this is if the top-level container file is given as
the output, in which case a new version will be created (VMS only).

The command can also be used to create a simple primitive object
with specified values - see 'Examples'.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Input data object, structured or
                            primitive.
 OUT       2    CHAR        Output data object takes shape and
                            type (but not necessarily name) of
                            input (name is user specified).

\end{verbatim}\subsection{Examples}
Copy complete container file
\begin{quote}\begin{verbatim}
> HCOPY FILE1 FILE2
\end{verbatim}\end{quote}
Extract an object into its own container file
\begin{quote}\begin{verbatim}
> HCOPY FILE.MORE.ASTERIX.GRAFIX.COLTAB TAB1
\end{verbatim}\end{quote}
Copy the component DATA\_ARRAY in FILE1
to DATA\_ARRAY in FILE2. The DATA\_ARRAY
will be created, but FILE2 must already exist.
\begin{quote}\begin{verbatim}
> HCOPY FILE1.DATA_ARRAY FILE2.DATA_ARRAY
\end{verbatim}\end{quote}
Copy the explicit values given into a component ARRAY within FILE
\begin{quote}\begin{verbatim}
> HCOPY [1 2 3 4 5] FILE.ARRAY
\end{verbatim}\end{quote}
A more practical example of the above
\begin{quote}\begin{verbatim}
> HCOPY 'Counts/sec' FILE.AXIS(2).UNITS
\end{verbatim}\end{quote}
\section{HCREATE}
Creates an HDS data object of specified name, type and
shape. Whether the object is primitive or a structure is
determined by the type given (see Valid\_Types subtopic).

By default the object created will be a scalar (dimension 0). If
you want to create an object of different shape then either
supply the dimensions on the command line or force prompting with
the prompt keyword. Note that the size of each dimension can
subsequently be changed with HRESHAPE, but the number of
dimensions cannot.

Primitives are not given values and this action must be
performed subsequently by HMODIFY, HFILL or HCOPY.

\subsection{Valid Types}
HDS divides objects into two classes, primitive and structured.
The former contain simple data such as numbers or characters,
whereas the latter contain collections of primitives. The valid
primitive types recognised by HDS are,

\begin{verbatim}
  Type         Equiv Fortran      Range

  _LOGICAL     LOGICAL*4          .TRUE., .FALSE.
  _UBYTE       not supported      0..255
  _BYTE        BYTE               -128..127
  _UWORD       not supported      0..65535
  _WORD        INTEGER*2          -32768..32767
  _INTEGER     INTEGER*4
  _REAL        REAL*4
  _DOUBLE      DOUBLE PRECISION
  _CHAR*n      CHARACTER*(n)
\end{verbatim}
Any type not in the above list will be assumed to be a structured
type.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Name of object.
           -GLOBAL.HDSOBJ (Default object dataset)
 TYPE      2    CHAR       Type of object to be created
 DIMS      3    INT        The dimensions of the object eg 0 for
                           scalar or, n for vector, [n m] for 2D
                           etc.
    Hidden default = 0

\end{verbatim}\subsection{Examples}
Creates a new container file containing a scalar structure
of type SPECTRUM with name SPECTRUMHZ43.
\begin{quote}\begin{verbatim}
> HCREATE SPECTRUMHZ43 SPECTRUM
\end{verbatim}\end{quote}
Creates the vector primitive DATA\_ARRAY within the existing
structure SPECTRUMHZ43 with length 64.
\begin{quote}\begin{verbatim}
> HCREATE SPECTRUMHZ43.DATA_ARRAY _REAL  64
\end{verbatim}\end{quote}
Creates a 64 x 64 real array called DATA\_ARRAY within the
structure IMG
\begin{quote}\begin{verbatim}
> HCREATE IMG.DATA_ARRAY _REAL [64 64]
\end{verbatim}\end{quote}
\section{HDELETE}
Deletes a specified data object. Deletion is recursive so
everything below the level of the object specified is also
deleted.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type      Description
 -------  ----  ----      -----------
 INP       1    UNIV      Name of the object to be deleted

\end{verbatim}\subsection{Examples}
Deletes the container file {\tt spectrumhz43}
\begin{quote}\begin{verbatim}
> hdelete spectrumhz43
\end{verbatim}\end{quote}
Deletes the ASTERIX structure and its contents
\begin{quote}\begin{verbatim}
> hdelete spectrumhz43.more.asterix
\end{verbatim}\end{quote}
-
\section{HDIR}
This displays the component objects of a given level of
an HDS file. The name, type and whether the objects are
primitive or structured are displayed.

The dimensions of array object are also given.

The value of scalar primitives is displayed.

Output is to the T(erminal), the default P(rinter), a N(ewfile)
with environment variable AST\_LIST or appended to an existing O(ldfile)
of the same name. By default output is to the terminal.

Output of multiple levels of HDS structure can be produced using HTRACE.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Name of HDS data object
          GLOBAL.HDSOBJ (Default object dataset)
 DEV       2    CHAR       text output device. Can take values of
                           (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                           or (N)EWFILE, otherwise is interpreted
                           as a filename.
    Hidden default 'TERMINAL'

\end{verbatim}\subsection{Examples}
Displays the components within FILE on the terminal
\begin{quote}\begin{verbatim}
> hdir file
\end{verbatim}\end{quote}
Sends 'directory' listing of MORE structure to printer
\begin{quote}\begin{verbatim}
> HDIR FILE.MORE DEV=PRINTER
\end{verbatim}\end{quote}
\section{HDISPLAY}
Displays the contents of a given HDS primitive of up to and
including 7-dimensions. A subset of the data may be specified,
although by default the whole object is displayed.

Each data type is output in some default format, unless
overridden by a user-supplied FORTRAN format-string.

By default the output goes to the terminal assuming a page width
of 80 columns, but it may also be sent to the P)rinter, N)ewfile
or O)ldfile when a page width of 132 columns is assumed. If your
terminal is in 132 column mode you can override the default width
to take advantage of this. If a file is selected then output will
go to the file with logical name AST\_LIST.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Data object to be displayed (must be a
                            primitive)
          GLOBAL.HDSOBJ (Default object dataset)
 DEV       2    CHAR        text output device. Can take values of
                            (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                            or (N)EWFILE, otherwise is interpreted
                            as a filename.
    Hidden default 'TERMINAL'
 SLICE     -    CHAR        Description of data subset (range in
                            1st dimension {,range in 2nd
                            dimension},{...}) eg. 5:10,1:5 for 2-D,
                            10:20 for 1-D. The range specifier for
                            each dimension follows the same
                            convention as the FORTRAN substring
                            specifier so ":50" "20:" "5:10,:" are
                            all valid.
    Hidden default = *
 FMT       -   CHARACTER    FORTRAN format string
    Hidden default = !
 WIDTH     -   INTEGER      Output page width
    Hidden default = !

\end{verbatim}\subsection{Example}
Display whole of specified array on terminal with default format
\begin{quote}\begin{verbatim}
> hdisplay file.data_array
\end{verbatim}\end{quote}
Outputs the specified slice of a
quality array in hex format to the printer
\begin{quote}\begin{verbatim}
> hdisplay img.quality.quality dev=p slice=100:120,250:300 fmt=z1
\end{verbatim}\end{quote}
Outputs the current HDS object to the terminal with page width 132
\begin{quote}\begin{verbatim}
> hdisplay accept width=132
\end{verbatim}\end{quote}
\section{HFILL}
Allows a specified primitive data object of any type
shape and size to be filled with a single value entered at
the users terminal or which is already stored in another
object.

It can be used to initialise an object created with
HCREATE or to overwrite existing values.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP       1    UNIV        Name of object to be filled - must be
                            primitive.
          GLOBAL.HDSOBJ (Default object dataset)
 VALUE     2    UNIV        Value to fill object with. This can
                            be explicit (a value entered at
                            console) or the name of another
                            object.

\end{verbatim}\subsection{Examples}
Fill quality array with 0
\begin{quote}\begin{verbatim}
> hfill hz43data.quality.quality 0
\end{verbatim}\end{quote}
Fill array with the value to be found in the specified
element of another array
\begin{quote}\begin{verbatim}
> hfill file1.array @file2.array(1)
\end{verbatim}\end{quote}
\section{HGET}
HGET returns values of various attributes of any HDS object. The
application is most useful in ICL or in UNIX scripts where it can
be used in program control.

Note that the object must be a character variable if the requested
attribute is of type '\_CHAR'.

\subsection{Items}
The table below lists the item names, types and description. All item
names can be abbreviated.

\begin{verbatim}
Item         Type         Description

PRIMITIVE    _CHAR        Is object a primitive
STRUCTURED   _CHAR        Is object a structure
NDIM         _INTEGER     Dimensionality
DIMS         _CHAR        Dimensions separated by commas
NELM         _INTEGER     Product of dimensions
TYPE         _CHAR        The HDS type of the object
VALUE        _CHAR        The object's value
MIN,MAX      _REAL        Return minimum or maximum of array
\end{verbatim}
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP       1    UNIV        Name of object to be filled - must be
                            primitive.
          GLOBAL.HDSOBJ (Default object dataset)
 ITEM      2    CHAR        The name of the attribute to be found.
                            See the "Items" subtopic for a list.
     Default 'VALUE'
 ATTR      3    CHAR        The value of the attribute. This
                            parameter will usually be a character
                            ICL variable.
 ECHO      -    LOGICAL     Echo value of attribute to the console.
                            Use this keyword in UNIX shell scripts

\end{verbatim}\subsection{Examples}
Use to get dimensions of an image. Always initialise a character
string to receive vector results in ICL,
\begin{quote}\begin{verbatim}
ICL> X = "WORK SPACE NEEDED"
ICL> HGET MY_IMAGE DIMS (X)
ICL> =(x)
60,60
\end{verbatim}\end{quote}
A procedure to list a few attributes,
\begin{quote}\begin{verbatim}
ICL> PROC HINFO OBJ
HINFO> TYPE = "            "
HINFO> DIMS = "            "
HINFO> NDIM = 0
HINFO> HGET (OBJ) TYPE (TYPE)
HINFO> HGET (OBJ) DIMS (DIMS)
HINFO> HGET (OBJ) NDIM (NDIM)
HINFO> PRINT Object (OBJ) is of type (TYPE), dimensionality (NDIM),~
       and dimensions [(DIMS)].
HINFO> END PROC
ICL> HINFO MY_IMAGE.DATA_ARRAY
Object MY_IMAGE.DATA_ARRAY is of type _REAL, dimensionality 2, dimensions [60,60]
\end{verbatim}\end{quote}
\section{HMODIFY}
Allows the value of an HDS primitive data object to be changed.
The new value(s) may be entered directly at the terminal or may
be taken from another HDS object.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Name of object to be modified - must
                            be primitive.
          GLOBAL.HDSOBJ (Default object dataset)
 VAL       2    UNIV        Values to be given to object - this
                            can be explicit values entered at
                            terminal or the name of another
                            object.

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> HMODIFY SPECTRUM.DATA.TEMP 20000
\end{verbatim}\end{quote}
Character values must be in quotes
\begin{quote}\begin{verbatim}
> HMODIFY DS.AXIS(1).UNITS 'Counts/s'
\end{verbatim}\end{quote}
Give specified slice of array the given values
\begin{quote}\begin{verbatim}
> HMODIFY DS.DATA_ARRAY(1:5) [1 2 3 4 5]
\end{verbatim}\end{quote}
Replace the values of the first DATA\_ARRAY with those in the second.
They must be the same size
\begin{quote}\begin{verbatim}
> hmodify ds.data_array  @ds2.data_array
\end{verbatim}\end{quote}
\section{HREAD}
Allows values to be read from an ASCII or binary file into a
primitive HDS object. The object must already exist and the data
type is taken from this. The file is read with a FORTRAN
free-format read and one item per record (line) is assumed (real
numbers must have a decimal point). An ASCII (formatted) file is
assumed unless explicitly overridden.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 FILE      1    CHAR        Filename (ascii or binary).
 BINARY    -    LOGICAL     Whether file is binary
    Hidden default = NO
 OUT       2    UNIV        HDS object to receive data.
          GLOBAL.HDSOBJ (Default object dataset)

\end{verbatim}\subsection{Examples}
Read values sequentially from the ASCII file VALUES.DAT into DATA\_ARRAY
\begin{quote}\begin{verbatim}
> hread values.dat file.data_array
\end{verbatim}\end{quote}
Same as above but values read from a binary file
\begin{quote}\begin{verbatim}
> hread binary values.dat file.data_array
\end{verbatim}\end{quote}
\section{HRENAME}
Changes the name of an HDS object. Can be useful for causing a
part of a dataset to be temporarily ignored by processing
software without deleting it.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       HDS object to be changed
 TO        2    CHAR       New name of the HDS object.

\end{verbatim}\subsection{Examples}
Renames the component DATA to OLD\_DATA.
\begin{quote}\begin{verbatim}
> HRENAME FIG1.IMAGE.DATA OLD_DATA
\end{verbatim}\end{quote}
Change name of QUALITY
\begin{quote}\begin{verbatim}
> HRENAME FILE.QUALITY TQUAL
\end{verbatim}\end{quote}
Now change it back
\begin{quote}\begin{verbatim}
> HRENAME FILE.TQUAL QUALITY
\end{verbatim}\end{quote}
\section{HRESET}
Reset value(s) of specified primitive object to 'undefined'.
Note that HDS does this by writing magic values into the object
concerned, so previous data values cannot be restored.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Object to be reset

\end{verbatim}\subsection{Examples}
Reset quality array
\begin{quote}\begin{verbatim}
> hreset file.quality.quality
\end{verbatim}\end{quote}
\section{HRESHAPE}
Allows the size of each dimension of a non-scalar object to be
changed. It does not allow the number of dimensions to be
changed.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Input data object structured or
                            primitive.
          GLOBAL.HDSOBJ (Default object dataset)
 DIMS      2    INTEGER(S)  List of numbers specifying new
                            dimensions

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> HRESHAPE FILE.DATA_ARRAY 25     - change length of vector
                                    array

> HRESHAPE IMG.DATA_ARRAY [25 25] - change the size of a 2-D
                                    array
\end{verbatim}\end{quote}
\section{HRETYPE}
Allows the type of an HDS structure to be changed - does not work
for primitives.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Object to be retyped
          GLOBAL.HDSOBJ (Default object dataset)
 TO        2    CHAR        New type

\end{verbatim}\subsection{Examples}
Change type of top level structure IMGFILE to IMAGE
\begin{quote}\begin{verbatim}
> HRETYPE IMGFILE  IMAGE

\end{verbatim}\end{quote}
\section{HTAB}
Provides the facility to display vector HDS data objects
simultaneously in a tabular form. The range to be output is
selectable but by default the whole of each object is output.

Output is to the user T(erminal, the line P(rinter), a N(ewfile)
with logical name AST\_LIST or appended to an existing O(ldfile)
of the same name.

Note that the input object names must either be given on the
command line or the PROMPT keyword must be used.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
  The following parameter is repeated for n = 1 to 6 inclusive
 INPn      n    UNIV        nth object in table.
          GLOBAL.HDSOBJ (Default object dataset)
     Hidden default = ! for INP2..6

 DEV       -    CHARACTER   text output device. Can take values of
                            (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                            or (N)EWFILE, otherwise is interpreted
                            as a filename.
    Hidden default 'TERMINAL'
 SLICE     -    CHARACTER   Range of data to be output in form
                            "n1:n2" ":n2" or "n1:"
    Hidden default = *
 WIDTH     -    INTEGER     Output page width
    Hidden default = !

\end{verbatim}\subsection{Examples}
Tabulate the first ten elements of the given vectors on printer,
\begin{quote}\begin{verbatim}
> HTAB VEC1 VEC2 VEC3 DEV=PRINTER SLICE=1:10 -
\end{verbatim}\end{quote}
Tabulate six vectors to terminal in 132 column mode
\begin{quote}\begin{verbatim}
> HTAB VEC1 VEC2 VEC3 VEC4 VEC5 VEC6 WIDTH=132
\end{verbatim}\end{quote}
\section{HTRACE}
Lists the properties of the input object (its name, shape, type
and value), and all the sub-components of that object if
appropriate.

By default only the first element of structure arrays is listed,
but this can be changed using the FULL keyword. Fine details of
the layout can be controlled using the identation parameters.

Output is to the T(erminal), the default P(rinter), a N(ewfile)
with logical name AST\_LIST or appended to an existing O(ldfile)
of the same name. By default output is to the terminal.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Name of HDS data object
          GLOBAL.HDSOBJ (Default object dataset)
 DEV       2    CHAR       text output device. Can take values of
                           (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                           or (N)EWFILE, otherwise is interpreted
                           as a filename.
    Hidden default 'TERMINAL'
 FULL      -    LOGICAL    Display full details of structure arrays.
                           By default, only the first element of
                           such structures is listed.
    Hidden default = NO
 NEWLINE   -    LOGICAL    Put data values on a new line. By default,
                           data values are appended to the line
                           containing object name and type info
    Hidden default = NO
 NLINES    -    INTEGER    Number of lines in which the data can be
                           displayed in addition to the first line.
    Hidden default = 1
 TYPIND    -    INTEGER    Indentation from beginning of name field
                           to beginning of type field
    Hidden default = 17
 VALIND    -    INTEGER    Indentation from beginning of type field
                           to beginning of value field
    Hidden default = 17

\end{verbatim}\subsection{Examples}
Displays the components within FILE on the terminal
\begin{quote}\begin{verbatim}
> htrace file
\end{verbatim}\end{quote}
Sends full listing of MORE (containing all the
structure array elements ) to the printer.
\begin{quote}\begin{verbatim}
> htrace file.more dev=printer full
\end{verbatim}\end{quote}
\section{HWRITE}
Writes values from an HDS primitive object into an ASCII
(default) or optionally a binary file. One value is written per
record.

For ASCII file output a default format is used unless explicitly
overridden.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Object to be input and read.
          GLOBAL.HDSOBJ (Default object dataset)
 FILE      2    CHAR        Output filename
 BINARY    -    LOGICAL     Whether file to be binary
    Hidden default = NO
 FMT       -    CHARACTER   Output format for ASCII file
    Hidden default = !

\end{verbatim}\subsection{Examples}
Write out DATA\_ARRAY to ASCII file VALUES.DAT with default format
\begin{quote}\begin{verbatim}
> HWRITE FILE.DATA_ARRAY VALUES.DAT
\end{verbatim}\end{quote}
As above but with specified format
\begin{quote}\begin{verbatim}
> HWRITE FILE.DATA_ARRAY VALUES.DAT FMT=F12.6
\end{verbatim}\end{quote}
Write DATA\_ARRAY in binary form into sequential, unformatted file.
\begin{quote}\begin{verbatim}
> HWRITE FILE.DATA_ARRAY VALUES.DAT BINARY
\end{verbatim}\end{quote}
\chapter{Spectral Fitting commands}
The spectral fitting package contains a suite of programs
which allow fitting of multi-component models to multiple spectra.
The ADAM parameter system is used fairly extensively to save the
user unnecessary typing.

Users who are not familiar with this system are advised to consult
the `Overview' first.

\section{Overview}
A typical spectral analysis sequence might consist of:

\begin{quote}\begin{verbatim}
SDATA -> SMODEL -> SFIT -> SPLOT -> DRAW -> SERROR
\end{verbatim}\end{quote}
The various steps define the data to be fitted, set up a spectral
model, fit it, create a plot file showing the resulting fit, draw
this to a graphics device and finally evaluate error intervals for
the best fit parameters, if this level of detail is required.
When fitting a single spectrum, the first step is not needed.

The spectral analysis package makes use of the parameter system
so that most program parameters are defaulted or passed on from
previous programs. Where it is required to override defaults,
this may be done on the command line. Two global parameters,
FIT\_DATA and FIT\_MODEL, are much used. The former contains the
name of the current dataset to be fitted, or the name of a file
containing references to a number of datasets (for fitting of
multiple spectra) as set up by SDATA. FIT\_MODEL contains the name
of a dataset specifying the current fit model. Current values of
these global parameters (amongst others) can be viewed by typing
GLOBAL.

Spectral fitting can only be achieved when information about the
instrument energy response is available to the system. The
response information is stored in a standard data structure which
must be attached to the spectrum itself. For example EXORESP performs
this function for EXOSAT spectra. The fitting software will
inform you if you have failed to attach a response to your spectrum.

For analysis of extragalactic sources it is possible to set a
value of redshift to be applied to the source spectrum. See HELP
on `Redshift' for details.

For raw spectra with 10 counts in a significant fraction of the spectral
bins, the implicit assumption of Gaussian errors made in chi-squared
fitting is no longer valid. In this case both the best fit and the
confidence interval may be misleading. There are two possible solutions:
the data can be rebinned, using SBIN, to give fewer bins with more
counts, or you can abandon chi-squared in favour of
likelihood fitting.

\section{Likelihood fitting}
Under the assumption that the detection of photons in each spectral
bin is a pure Poisson process, the likelihood statistic representing
the probability of obtaining the observed spectrum from the current
model can be calculated. This likelihood is then maximised by adjusting
the model parameters, to give a "maximum likelihood" fit. A confidence
region for the free parameters can be defined, in a similar way to
that employed for chi-squared fitting, using the Cash statistic
(see {\em Ap.J., 228, p.939 (1979)}).

This can be done by invoking SFIT and SERROR with the keyword LIK on
the command line.

Since a spectrum is only Poissonian before background is subtracted,
it is necessary to work with raw data, and allow for any background
contribution as part of the model. However the software has been
written such that you can use a background-subtracted,
exposure-corrected spectrum, as for chi-squared fitting. The programs
will reconstruct the raw data from this by scaling by the effective
exposure time (stored in the spectral file HEADER) and adding back the
background. However the software needs to be given a file containing
the background subtracted. The ROSAT reduction software is configured
to make this easy for the user, XRTSUB will generate a background
model file, and enter a "BGFILE" flag in the spectral dataset pointing
to it. This is then automatically picked up by the software.
If you don't have a background file, then you can either construct
one (e.g. from raw source spectrum - background subtracted spectrum)
and make it known to the system by running SBG, or you can proceed
without a background file, in which case the background will be
assumed to be negligible.

The one drawback with likelihood fitting is that one does not get
a measure of the goodness of fit (the numerical value of the
likelihood does not help). The best way to judge the fit at
present is to plot it out with SPLOT. The chi-squared statistic
may also be useful, but very low values of reduced chi-squared
are obtained for extremely Poissonian data.

\subsection{ROSAT procedure}
To use likelihood fitting with ROSAT data, you proceed in just the
same way as for chi-squared fitting:
XSORT to get source and background spectra
XRTSUB to subtract background, use BGMODEL=bfile to
generate a background file for spectral fitting
XRTCORR corrects the spectrum to ct/s (no need to
correct the bfile)
XRTRESP attaches response to spectrum
SFIT LIK will perform likelihood fit, picking up bfile
automatically
SERROR LIK generates confidence region using Cash statistic
SPLOT as normal
Likelihood fitting will also work with spectral sets. The background
file must in this case be a spectral set of the same dimensions as
the source dataset (as it will be if you follow the above procedure).
\section{Constrained Fitting}
Constrained fitting allows the user to specify constraints between
different parameters within a model. At the moment only one type of
constraint is available, namely equality, but more will be added in
future. Constraints are created, modified or deleted using the TIE
program.

A constraint consists of a base parameter and a one or more dependent
(or constrained) parameters. In the equality constraint the first
parameter specified is the base parameter. When minimising the fitting
system only works with free parameters, where free is defined as both
not frozen and not constrained. The system ensures that the constraints
are obeyed at all times.

The effect of placing constrains on model parameter values is to
reduce the number of degrees of freedom in the fit - consequently the
size of confidence regions produced by SERROR will change from those
of the equivalent unconstrained fit.

\section{Redshift}
The effects of source redshift can be allowed for by specifying
the redshift on the command line (Z=value) in any of the
applications (currently SFIT,SERROR,SPLOT and SFLUX) for which it
is supported. This value is stored as a global parameter and will
be used for subsequent runs of any of these programs until
explicitly overridden. The user is always warned when any non-zero
value of redshift is in use.

The spectral model is assumed to apply in the SOURCE frame, and
the resulting spectrum is then redshifted before folding through
the instrument response for comparison with the data. SPLOT
corrects for redshift of the model to plot the spectrum which is
observed at Earth, and SFLUX, similarly, gives the Earth-based
flux.
Absorption:-
The AG (galactic absorption) model component represents absorption
occurring along the sight-line within our own galaxy. This component
is therefore NOT redshifted. The AB (intrinsic absorption model), on
the other hand IS redshifted, since it represents absorption local
to the source.
\section{Models}
Details of parameters, problems and references of all the models
available under the spectral fitting package can be found by typing
SMHELP from the command line.
\section{IGNORE}
IGNORE (which invokes the program QUALITY with appropriate switch
settings) can be used to exclude selected channels of a spectrum
from the fitting by setting their quality flags. This process is
easily reversed by RESTORE.
See HELP on Quality\_Processing\_Commands for details.
\section{RESTORE}
RESTORE will reinstate spectral channels previously excluded from
the fitting by IGNORE.
See HELP on Quality\_Processing\_Commands for details.
\section{FREEZE}
Freezes model parameters at either current or user-specified
values so that they will not be varied during fitting.
\subsection{Input Output Data}
Input: fit-model dataset (modified)
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 MODEL      -   UNIV        Data object containing fit
                            model.
       GLOBAL.FIT_MODEL (Default model dataset)
 PAR        1   INTEGER     Numbers of parameters to be
                            frozen in array.
 VALS       2   REAL        Values at which parameters
                            in array will be set.
     Hidden default = !

\end{verbatim}\subsection{Examples}
Typical usage. Freeze parameter 2 at current value
\begin{quote}\begin{verbatim}
ICL> FREEZE 2
\end{verbatim}\end{quote}
Freeze parameter 3 at 5.2
\begin{quote}\begin{verbatim}
ICL> FREEZE 3 5.2
\end{verbatim}\end{quote}
Freeze parameters 2, 4 and 5
\begin{quote}\begin{verbatim}
ICL> FREEZE [2 4 5]
\end{verbatim}\end{quote}
Freeze parameter 1 and 4 at given values
\begin{quote}\begin{verbatim}
ICL> FREEZE [1 4] [.1 3]
\end{verbatim}\end{quote}
Full prompts,

\begin{quote}\begin{verbatim}
ICL> FREEZE PROMPT
Model > SPECT
Parameter numbers > 2 3
Parameter values > 5 0
\end{verbatim}\end{quote}
\section{SBG}
Associates a background file with a spectrum.
The two files must have the same dimensions. The background file
may be in raw counts or in count/s.

\subsection{Input Output Data}
Input: spectrum or spectral\_set (background-subtracted and corrected
to count/s)
background spectrum (in counts or count/s)
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP        1   UNIV        Input spectrum
       GLOBAL.FIT_DATA   (Default spectral dataset)
 BGFILE     2   UNIV        Background dataset

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> SBG
INP - Data /@SPEC/ >           Default is current spectral dataset
BGFILE - Background > BSPEC
\end{verbatim}\end{quote}
\section{SBIN}
Rebins a spectrum (by amalgamating channels) to achieve either as
near as possible equal counts in a specified number of bins, or a
given minimum count in each bin.

SBIN can be run on any 1D dataset, but it makes most sense to
run it on a background subtracted spectrum BEFORE exposure
correction, if there appear to be potential problems with
sparse counts (i.e. a number of bins with 10 counts in the
raw spectrum, before background subtraction). Since it changes
the data binning, SBIN must be run BEFORE attaching a spectral
response to the data.

For ROSAT XRT spectra see the help on SASSBIN.

\subsection{Input Output Data}
Input: spectral dataset (or any 1-dimensional NDF)

Output: rebinned spectrum

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP        1   UNIV        Input spectrum
 OUT        2   UNIV        Rebinned spectrum
 OPT        3   INTEGER     Option number (1)=rebin to fixed number of bins
                                          (2)=rebin to give a min flux/bin
 NBIN           INTEGER     Number of output bins (for Opt 1)
 MIN            REAL        Minimum flux per bin (for Opt 2)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> SBIN
INP - Input > SPEC
OUT - Output > SPECR
OPT - Mode option /1/ >
NBIN - Number of bins > 5
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
ICL> SBIN SPEC
OUT - Output > SPECR
OPT - Mode option /1/ > 2
MIN - Minimum bin content > 10
\end{verbatim}\end{quote}
\section{SDATA}
Sets up file of data object names for fitting of multiple
datasets. A sequence of up to 32 dataset names (! to terminate)
is accepted from the user and references to them are written into
reference data object. This is assigned to global parameter
FIT\_DATA, so as to be picked up automatically by subsequent
applications.

In the case of some instruments (such as the EXOSAT ME) multiple
spectra are collected simultaneously by a bank of detectors.
These may be stored in a single 2D array within a dataset of type
SPECTRAL\_SET. Where such a dataset is entered SDATA allows the
user to specify which particular component spectra are to be
used. These are then treated as completely independent spectra by
subsequent software.

The optional keyword SHOW used in conjunction with a dataset
previously created using SDATA will display a list of contents.

\subsection{Input Output Data}
Input: names of a sequence of spectra to be fitted
Output: fit\_data file containing references to these datasets
\subsection{Parameters}
\begin{verbatim}
 Keyword    Posn    Type        Description
 -------    ----    ----        -----------

 OUT         1      UNIV        Data object to contain
                                references to datasets to be
                                fitted.
            -> GLOBAL.FIT_DATA (Default fit dataset)
 INP1...32  2...11  UNIV        Input SPECTRUM datasets
      Default = !
 DETNO              CHAR        Range of spectra (from
                                 SPECTRAL_SET)
 SHOW        -      LOGICAL     Display existing file
   Hidden default = NO

\end{verbatim}\subsection{Examples}
Typical usage
\begin{quote}\begin{verbatim}
ICL> SDATA SPECTRA     (data object SPECTRA will be created)
 Dataset 1 > SPEC1
 Dataset 2 > SPEC2
 Dataset 3 > !         (null to terminate input)
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
ICL> sdata show
 Dataset 1
     File : DISK$USER:[USER]SPEC1
 Dataset 2
     File : DISK$USER:[USER]SPEC2
\end{verbatim}\end{quote}
With SPECTRAL\_SETs
\begin{quote}\begin{verbatim}
ICL> SDATA
Object to contain references > REFS
Dataset 1 > SPEC1
Spectra to be selected /'1:2'/ > 2     (use 2nd spectrum only)
Dataset 2 > SPEC2
Spectra to be selected /'1:8'/ > 1:4   (use spectra 1,2,3 and 4)
Dataset 3 > !
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
ICL> SDATA SHOW
 Dataset 1
     File : DISK$USER:[USER]SPEC1
     Detectors : (2)
 Dataset 2
     File : DISK$USER:[USER]SPEC2
     Detectors : (1,2,3,4)
\end{verbatim}\end{quote}
\section{SEDIT}
Provides a command driven interfaces to create new or edit existing
spectral models.

\subsection{Command Mode}
The commands, entered at the OPTION prompt, are,

\begin{verbatim}
    A    Alter parameter values/bounds
    L    List model components and parameter values
    F/T  Freeze/thaw one or more parameters
    P    Same as L but to hardcopy
    NP   Alter values/bounds of all parameters
    NM   New model specification
    RE   Reset errors of selected parameters
    X    Exit
\end{verbatim}
A "?" response will list these options.

See "Examples" for the additional details of command prompts. SEDIT
is best run with your terminal in 132 column mode.

\subsection{Input Output Data}
Input: fit\_model dataset
Output: fit\_model dataset modified
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 MODEL      1   UNIV        Data object containing model
                            specification.
        GLOBAL.FIT_MODEL (Default fit model)
 MODEL_SPEC 2   CHAR        Model specification, eg. AB*(BR+LG)
 PAR        3   INTEGER     Component number
 PARS       4   INTEGER[]   Component numbers, separated by
                            commas or spaces
 VALUES     5   CHAR        1,2 or 3 real numbers separated by
                            commas or spaces
 OPTION     6   CHAR        Command option character
 GEN        -   CHAR        Genus of model
    Hidden default = SPEC
 SCREEN     -   LOGICAL     Start up in screen mode?

\end{verbatim}\subsection{Examples}
Freeze one or more parameters. Use Asterix list syntax to specify
the parameter numbers.
\begin{quote}\begin{verbatim}
  OPTION - ... > F
  PARS - Enter component numbers > 1:3

  OPTION - ... > F
  PARS - Enter component numbers > 4:*
\end{verbatim}\end{quote}
Thaw one or more parameters
\begin{quote}\begin{verbatim}
  OPTION - ... > T
  PARS - Enter component numbers > 3,2,7:8
\end{verbatim}\end{quote}
Create new model
\begin{quote}\begin{verbatim}
  OPTION - ... > NM
  A composite model can be synthesised using + - * ( ) and
  any of the following primitive models:
  AG     galactic interstellar absorption             multiplicative
  ..     ...                                          ...
  LL     Lorentzian line                              additive
  MODEL_SPEC - Model specification > AG*PL
\end{verbatim}\end{quote}
List existing model
\begin{quote}\begin{verbatim}
  OPTION - ... > L
\end{verbatim}\end{quote}
Alter parameter values
\begin{quote}\begin{verbatim}
  OPTION - ... > A
  PAR - Enter component number > 3
  VALUES - New values /'none'/ >
\end{verbatim}\end{quote}
{\tt none} leaves the values unchanged. 1 value is parsed as the new
parameter value, 2 values as the new lower bound + new value, and
3 values as new lower bound, new value, new upper bound.

\section{SERROR}
Evaluates a confidence region corresponding to a specified
chi-squared increase, along each parameter dimension in the fit
space. A chi-squared increase of unity gives approximate 1 sigma
errors for each parameter individually - i.e. there is a 68\%
probability of a parameter lying within its confidence interval
where the other parameters are free to have any value at all.

Note that the parameter space volume defined by the intersection
of the 1 sigma intervals for each parameter is NOT a 1 sigma
confidence volume, it is considerably smaller (i.e. it contains
less than 68\% of the probability).

If 90\% confidence intervals are required, the corresponding
chi-squared increase is 2.71.

Not all parameters need have errors calculated. The hidden PARS
parameter contains the list of parameter numbers to process - see
examples for usage.

For a likelihood evaluation of errors, using the Cash statistic,
SERROR should be invoked with LIK on the command line.

Note that the first run of SERROR will not always produce an
accurate confidence interval for every parameter - read the
WARNING subsection below for further information.

\subsection{Input Output Data}
Input: one or more SPECTRUM or SPECTRAL\_SET datasets
fit\_model dataset
Output: fit\_model dataset modified
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Input data (either a single
                            dataset or a file of
                            references).
        GLOBAL.FIT_DATA (Default fit dataset)
 Z         -    REAL        Source redshift
        GLOBAL.REDSHIFT (Default redshift parameter)
      Default = 0.0
 MODEL     2    UNIV        Data object containing model
                            specification.
       GLOBAL.FIT_MODEL (Default model dataset)
 DS        3    REAL        Chi-squared increase defining
                            confidence interval.
      Default = 1.0
 MAX       4    INTEGER     Max number of iterations to
                            be performed.
 PARS      5    CHAR        List of parameter numbers on
                            which error analysis is performed.
                            Use Asterix standard lists notation
      Hidden default = *
 OP        -    LOGICAL     Output summary of fit
                            results?
 FITOUT    -    CHAR        Name of fit text file
    Hidden default = FIT.OP
 MINS      -    REAL        Minimum reduced chi-squared
                            gradient for termination.
    Hidden default = 20.0
 APPEND         LOGICAL     Append output to FIT.OP file?
    Hidden default = NO
 LIK       -    LOGICAL     Likelihood(Y) or chi-square(N) fit?
    Hidden default = NO

\end{verbatim}\subsection{Examples}
Typical usage
( data and model from global, current
\$ SERROR values of MAX and NUP used )
Output results? > Y
\$ SERROR APPEND ( appends output to file FIT.OP )
Output results? > Y
Prompts (if globals and current values not defined)
> SERROR
Data > SPECT
Model > SPECMOD
Chi-squared increase /1/ > 2.71
Max. no. of iterations /30/ > 40
Update interval /3/ > 5
Output results? > N
Using keywords
> SERROR MODEL=SPECMOD MINS=1 APPEND OP
( Global value for INP and current value for DS and
MAX used. APPEND and OP on command line set the
logicals true.)
Restricting parameters in analysis
> SERROR ... PARS="1:3,6:8" ; Find errors on parameters
1,2,3,6,7,8
> SERROR ... PARS="2:*" ; Find errors for all but
parameter 1
\subsection{Method}
The initial position in parameter space is checked to ensure that
it corresponds to a chi-squared minimum. Each parameter in turn
is then perturbed from its best fit value and frozen, and the
other parameters optimised. From the actual chi-squared values
attained, the parameter offset giving the specified chi-sq. min
increase (SOFF) is estimated on the assumption that the minimum
chi-squared surface is a parabolic function of the parameter
whose error is being evaluated. The parabola may have a different
shape on each side of the minimum, the curve on each side being
defined by the assumption of zero slope at the minimum, and the
requirement that it should pass through the single offset point
evaluated. e.g.
\begin{verbatim}
     .                             !
      *                            !            .
       .                           !
         .!.      -
            .                      !          *       ^
                 .                 !        .        SOFF
                        .          !     .            ^
                                   M                  ^
  \end{verbatim}
where * are the evaluated points, and the resulting confidence
limits are shown by .

From Lampton, Margon \& Bowyer ({\em Ap.J. 208, p177, 1976}) a value
SOFF=1 gives a 1 sigma (i.e. 68\% confidence) interval and
SOFF=2.71 a 90\% confidence interval for the case where the model
is linear (i.e. the predicted values are linear functions of the
parameters). Where this is not the case (i.e. always!) the
results may still be approximately right LMB and also {\em Avni Ap.J.
210, p642, 1976}). A safe limit in all cases is obtained by
projecting the full NPAR-dimensional confidence interval onto the
subspace desired (a single parameter axis in this case). This
corresponds to taking

\begin{verbatim}
        SOFF = chi-squared with NPAR d.o.f.( n% conf)
\end{verbatim}
for a safe upper bound on the n\% confidence limit. For large NPAR
this will be way above the linear approximation result.

\subsection{Problems}
If the number of free parameters (apart from that whose interval
is being evaluated) is reduced as a result of parameter bounds
being encountered, then a warning is issued. The confidence
region for an affected parameter must be treated with caution.

Pegging of a parameter is likely to truncate the confidence limit
(since an extra impediment to chi-squared minimisation is
introduced). Where the limit encountered is real (i.e. the user
{\em knows} it cannot be crossed) the derived confidence limit stands.
If the bound is arbitrary then it should be removed and the
confidence region reevaluated.

Errors arising in the fitting process, and possible remedial
action, are discussed in the HELP for SFIT.

\subsection{WARNING}
For each parameter, the program proceeds by guessing what the
confidence region should be (using errors derived in previous
SFIT or SERROR runs if available), stepping off by this amount,
evaluating chi-squared and then using this point together with
the chi-squared minimum to define a parabola (see "Method" section
or more detail). This parabola is then used to predict the
confidence region.

However the program does {\em not} proceed iteratively to check these
estimates (this would be time consuming in many cases). Hence
the confidence region is only as good as the parabolic assumption.
In many cases this is entirely adequate, however a check is easily
made by running the program twice. On the second run, the confidence
region from Run 1 is picked up (from the model file) and is used
as the basis for the stepping off. If the values calculated as a
result show little change from the first run them they are reliable.
If not, then further runs should be made until they converge.

There is one case in particular where the intial parameter estimate
may be significantly wrong. In the case of a parameter which is up
against a bound at the chi-squared minimum (e.g. if nH=0 for the
best fit spectrum), the minimum of the parabola may lie
{\em beyond} this bound. The assumption made that the slope of the
chi-squared surface is zero at the minimum will then be invalid. A
number of iterations may be required to get a consistent confidence
interval for such a parameter.

\section{SFIT}
Adjusts parameters in a multicomponent spectral model to achieve
minimum chi-squared fit to one or more datasets.

Only a single input dataset can be entered - if multiple datasets
are to be fitted a file containing references to them must be
first set up using SDATA.

A fixed number of iterations are performed (default 30) but if a
minimum is not found the user may restart fitting by typing SFIT
again (no arguments should be required). The current model
parameters are stored in the fit\_model data object, this is
updated by the program every NUP (default 3) iterations. If the
user is running under DCL the program can be aborted with CTRL-C at
any point and the last update to the parameter values will be
retained, these can then be used to generate plotted output or
taken as the starting point for further fitting. Aborting under
ADAM can currently only be effected with CTRL-Y, which also exits
the monolith - this situation should improve soon.

Approximate error estimates for the free parameters are computed
from the matrix of second derivatives at the minimum. These can
be taken as only a very rough (factor of two or so) guide to the
errors. Refitting at offset positions in parameter space is
required for an accurate error assessment - this is performed by
SERROR.

The error assessment is rather lengthy and can be disabled by
invoking:
\begin{quote}\begin{verbatim}
ICL> sfit noerr
\end{verbatim}\end{quote}
however, this is unwise if SERROR is to be run subsequently,
since the latter program uses the approximate error values as a
starting point.

The fit results are normally printed at program termination, but
may be appended to file FIT.OP if the APPEND option is selected.
This is useful when performing a sequence of related spectral
fits. SERROR appends to the same file by default (which is created
if necessary), although the filename can be changed in both programs.

A source redshift may be included in the fitting process by
specifying Z=value on the command line. The specified redshift
will be used thereafter (also by SERROR, SPLOT and SFLUX) until
disabled. The fitted model in this case corresponds to the
spectrum in the SOURCE frame, and is redshifted by the software
before being folded through the instrument response.

Likelihood fitting is available by including LIK on the
command line.

\subsection{Input Output Data}
Input: one or more SPECTRUM or SPECTRAL\_SET datasets
fit\_model dataset
Output: fit\_model dataset modified
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP       1    UNIV        Input data (either a single
                            dataset or a file of
                            references).
        GLOBAL.FIT_DATA (Default parameter dataset)
 Z         -    REAL        Source redshift
        GLOBAL.REDSHIFT (Default redshift parameter)
    Default = 0
 MODEL     2    UNIV        Data object containing model
                            specification.
       GLOBAL.FIT_MODEL (Default model dataset)
 MAX       3    INTEGER     Max number of iterations to
                            be performed.
 NUP       4    INTEGER     Number of iterations between
                            updates of model parameter
                                file.
 MINS      -    REAL        Minimum reduced chi-squared
                            gradient for termination.
    Hidden default = 20.0
 ERR       -    LOGICAL     Compute approx. parameter
                            errors?
    Hidden default = YES
 OP        -    LOGICAL     Output summary of fit
                            results?
 FITOUT    -    CHAR        Name of fit text file
    Hidden default = FIT.OP
 APPEND    -    LOGICAL     Append output to FIT.OP file?
    Hidden default = NO
 LIK       -    LOGICAL     Likelihood(Y) or chi-squared(N) fit?
    Hidden default = NO

\end{verbatim}\subsection{Examples}
Typical usage
( data and model from global, current values
\$ SFIT of MAX and NUP used )
Output results? > Y
Prompts (if globals and current values not defined)
\$ SFIT
Data > SPECT
Model > SPECMOD
Max. no. of iterations /30/ > 40
Update interval /3/ > 5
Output results? > N
Using keywords
\$ SFIT MODEL=SPECMOD NUP=2 OP
( Global value for INP and current value for MAX used.
OP on command line sets the logical true.)
\$ SFIT Z=0.3 APPEND NOERR
( Redshift 0.3, default values for all other parameters,
output will be appended to file FIT.OP, no errors to be
computed (saves time).)
\subsection{Method}
The chi-squared minimisation is done by a combined
gradient/quadratic-fitting minimisation routine based on the
CURFIT program of Bevington. Parameter bounds are incorporated by
'pegging' parameters on bounds and excluding them from the
fitting process until the chi-squared gradient takes them back
into the allowed region. 'FROZEN' parameters are never allowed to
vary. Computing time is saved by using an approximation to the
chi-squared second derivs rather than a rigorous evaluation.

\subsection{Errors}
The most likely error to arise in fitting, is that the matrix of
second derivatives of chi-squared (with respect to the
parameters) may become singular or ill-conditioned. This arises
when the fit is insensitive to one or more parameters. This could
arise for example if the temperature of a black body component
became so low that its norm (leading constant) became irrelevant
to the problem, since it is emitting at energies below those
accessible to the instrument. The solution to such problems is
generally to set appropriate parameter bounds and to start the
fit from a position in parameter space not too far removed from
the likely fit.

It must be remembered that, particularly for complex
multicomponent models, the chi-squared surface may have local
minima. The program will generally settle down into the closest
such minimum, so it is important to start from a reasonable point
in parameter space. With complex models it is often helpful to
start by fitting a subset of the model to get some parameters
into the right regime. e.g. where a line is present, it is best
to fit the underlying continuum first, before adding the line to
the model. It may also improve stability to freeze line width at
a small value until position and strength have been optimised.

Some of the more complex models (e.g. ST and CS) are valid only
within a limited volume of parameter space which may have complex
boundaries (depending on combinations of parameter values). If
the bounds of the region of validity are transgressed during the
fitting process then the user will be warned (each time!). If the
FINAL fit values are outside the valid region then the result is
suspect, otherwise no harm has been done.

\section{SFLUX}
SFLUX calculates the energy flux and number of photons in a model
spectrum integrated between two energy limits. The number of
energy channels used for the integration is 1000 by default, but
may be overriden on the command line using the keyword NCH (max
value allowed is 5000).

The output is the energy flux in (ergs/cm**2/sec) and photon flux
in (photons/cm**2/sec)
which are output to the terminal and optionally to parameters.

The optional SPLIT keyword enables the absorbed fluxes of all the
additive model components in a composite model to be displayed.

Note that redshifted spectra are corrected such that the flux
seen at Earth in the specified energy range is returned. The
redshift previously set up for SFIT will be used automatically
(unless overridden by keyword).

\subsection{Input Output Data}
Input: fit model dataset
Output: to terminal
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type        Description
 -------  --------  ----        -----------

 MODEL       1      UNIV        Data object containing fit
                                model.
           GLOBAL.FIT_MODEL (Default model dataset)
 Z                  REAL        Source redshift
            GLOBAL.REDSHIFT (Default redshift parameter)
    Default = 0
 NCH         -      INTEGER     Number of energy channels
                                for integration.
    Hidden default = 1000
 LEN         2      REAL        Lower energy limit for
                                integration (KeV).
 UEN         3      REAL        Upper energy limit for
                                integration (KeV).
 SPLIT       -      LOGICAL     Display fluxes of additive
                                model components
    Hidden default = NO
 EFLUX       -      REAL        Energy flux (output)
 PFLUX       -      REAL        Photon flux (output)
\end{verbatim}\subsection{Examples}
Typical usage (MODEL from global and default NCH),

\begin{quote}\begin{verbatim}
ICL> sflux
Lower energy limit > 2
Upper energy limit > 10
\end{verbatim}\end{quote}
Using keywords
\begin{quote}\begin{verbatim}
ICL> sflux model=specmod nch=3000
Lower energy limit > 2
Upper energy limit > 10
\end{verbatim}\end{quote}
Under ICL with results output to variables
\begin{quote}\begin{verbatim}
ICL> sflux eflux=(energy_flux) pflux=(photon_flux)
Lower energy limit > 2
Upper energy limit > 10
\end{verbatim}\end{quote}
Using SPLIT mode on a {\tt AG*(BB+BR)} model lists the two additive
components with absorption folded in, and then the total flux.
\begin{quote}\begin{verbatim}
 ICL> SFLUX SPLIT LEN=1 UEN=10

   Model term          Photons/cm**2/sec  Erg/cm**2/sec

   AG*BB                 0.6672986       1.5618454E-09
   AG*BR                 0.1784134       8.5624380E-10

 Energy flux = 6.1529504E-10 ergs/cm**2/sec
 Photon flux = 9.8584913E-02 photons/cm**2/sec
\end{verbatim}\end{quote}
\section{SGRID}
Evaluates the fit statistic comparing the input observed data
with a multicomponent spectral model on a user defined grid of
component parameter values.

In addition, maps of parameters reoptimised at each grid point
can also be produced. For example, given the 3-component model
{\tt AG*BR}, the user can construct a 2-dimensional grid of chi-squared
versus emission measure and temperature, and simultaneously a
map of reoptimised hydrogen column in the same space.

The model dataset can optionally be updated with the best
parameter set found in the grid.

A source redshift may be included in the fitting process by
specifying Z=value on the command line. The specified redshift
will be used thereafter (also by SFIT, SERROR, SPLOT and SFLUX)
until disabled. The fitted model in this case corresponds to the
spectrum in the SOURCE frame, and is redshifted by the software
before being folded through the instrument response.

Likelihood fitting is available by including LIK on the
command line.

\subsection{Grid Axes}
Grid axes are defined by selecting parameters at the PARS prompt.
For example, 2,3 defines a 2 dimensional parameter grid whose
first (ie. x) axis is model parameter 2, and its second (ie. y)
axis is model parameter 3. Unless model parameter 1 has been
frozen by the user, it will be reoptimised by SGRID at every
grid point.

The parameter range for each grid axis is specified by choosing
an upper and lower axis range in standard ASTERIX ranges
format. The selected range must fall within the valid range of
the parameter within the model file - if it does not then it is
truncated to that range. The spacing of points along each grid
axis may be either linear or logarithmic. The OPT parameter
controls how this is specified -
\begin{verbatim}
   OPT = 1     All axes are linearly spaced
   OPT = 2     All axes are linearly spaced in log(parameter value)
   OPT = 3     User chooses from linear/logarithmic for each axis
               using the LOG1..7 parameters.
\end{verbatim}
The default for OPT is 1.

An improvement to choosing default axis ranges from allowed
range of parameter value is choose the current error bounds. This
feature can be enabled by specifying the ERR keyword on the
command line. This assumes that sensible error values are present
in the model dataset, ie. that one of SFIT/SERROR has already been
run.

\subsection{Input Output Data}
Input: one or more SPECTRUM or SPECTRAL\_SET datasets

Input/Output: fit\_model dataset

Output: one or more binned datasets

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP       1    UNIV        Input data (either a single
                            dataset or a file of
                            references).
        -GLOBAL.FIT_DATA (Default parameter dataset)
 Z         -    REAL        Source redshift
        -GLOBAL.REDSHIFT (Default redshift parameter)
    Default = 0
 MODEL     2    UNIV        Data object containing model
                            specification.
        -GLOBAL.FIT_MODEL (Default model dataset)
 LIK       -    LOGICAL     Likelihood(Y) or chi-squared(N) fit?
    Hidden default = NO
 PARS      3    INTEGER     Array of parameter numbers, one for
                            each grid axis. See examples.
 ERR       -    LOGICAL     Use parameter errors to define default
                            for AXIS1..7 ranges.
    Hidden default = NO
 AXIS1..7  -    CHARACTER   Range of parameter value for
                            each grid axis. Default is
                            current parameter range.
 NBIN1..7  -    INTEGER     Number of bins for each grid axis
 OPT       -    INTEGER     Option for specifying grid axis
    Default = 1             bin spacing. Can be 1 = all linear,
                            2 = all logarithmic, 3 = user
                            selects for each axis.
 LOG1..7   -    LOGICAL     Logarithmic or linear for each
                            grid axis. Only used ifg OPT=3.
    Default = NO
 GPARS     -    INTEGER     List of values to grid. See examples
    Default = 0

 MAX       -    INTEGER     Max number of iterations to be
                            performed. Only used if grid
    Default = 30            leaves free parameters.
 MINS      -    REAL        Minimum reduced chi-squared gradient
                            for termination. Only used if grid
    Hidden default = 20.0   leaves free parameters.

 AUTO      -    LOGICAL     Automatic naming for multiple
                            output files.
    Default = YES
 OUT       -    UNIV        Output filename for 1st grid
    -> GLOBAL.BINDS (Default binned dataset)
 OUTROOT   -    CHAR        Root filename for automatic naming
 OUT2..5   -    CHAR        Output filenames for each grid. Only
                            used if AUTO is specified false.
 SUBSTAT   -    LOGICAL     Subtract minimum value of statistic
                            from statistic grid. Only used if
    Default = NO            GPARS contains a 0 in the list.
 UP        -    LOGICAL     Update the model with the best fit
                            parameter set?
    Hidden default = NO

\end{verbatim}\subsection{Method}
The method used depends on whether after specifying the grid
axes, there remain any free parameters. If there are none, then
SGRID simply performs a model evaluation and calculates the
statistic at each grid point. If there are free parameters,
then a minimisation is performed at each grid point using the
iteration and slope control parameters MAX and MINS. The latter
mode is much slower than the former and grid sizes should be
chosen with discretion.

\subsection{Examples}
Consider an example of a model dataset GX7 with a model AG*BR
in a file BR\_MOD. Invoke SGRID,
\begin{quote}\begin{verbatim}
> SGRID GX7 BR_MOD
SGRID Version 1.7-0                    ; Version id and user
                                       ; informed of inputs
...

Existing model is: AG*BR               ; User is informed of all
                                       ; model parameters, with
                                       ; lower bound, current
                                       ; value and upper bound.

(1) hydrogen column density   (1.0E21 /cm**2)            .. .. ..
(2) em10                      (1e60cm**-3/(10kpc)**2)    .. .. ..
(3) temperature               (keV)                      .. .. ..

PARS - Parameters for grid axes > 2 3  ; Select those parameters
                                       ; to form the grid axes

OPT - How to specify grid axes /1/ >   ; Specify how spacing of
                                       ; grid axes to be done
All grid axes will be linearly spaced

; User now selects range for each grid axis. The default offered
; is the lower-upper bound, which will usually be too large. The
; number of bins is also selected.

Enter axis values in units of 1e60cm**-3/(10kpc)**2
AXIS1 - Range of 1st grid axis /'9.9999997E-05:100'/ > 40:50
NBIN1 - Number of grid values in 1st axis > 10
Enter axis values in units of keV
AXIS2 - Range of 2nd grid axis /'0.1:20'/ > 4:5
NBIN2 - Number of grid values in 2nd axis > 10

GPARS - Parameter(s) to be gridded     ; Select the values to grid.
       (0 for fit statistic) /0/ > 0 1 ; Here we choose to grid
                                       ; both the fit statistic and
                                       ; the hydrogen column.

AUTO - Automatic naming of output      ; We'll choose our output
                 files /YES/ > NO      ; file names
OUT1 - Output filename for 1st grid > grid_chi
OUT2 - Output filename for 2nd grid > grid_nh

; SGRID now does the work

There are free non-grid parameters -  optimising at each grid point.

; The best fit grid point is reported on completion

The minimum value of chi-squared in the grid is 1961.189

SUBSTAT - Subtract minimum value of statistic from grid /NO/ >
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
> SGRID GX7 BR_MOD PARS=2,3 GPARS=0    ; Construct 10 x 10 grid
      NBIN1=10 NBIN2=10 OUT=GRID_CHI \ ; over default region

> SGRID GX7 BR_MOD PARS=2,3 GPARS=0    ; Construct 10 x 10 grid
      NBIN1=10 NBIN2=10 OUT=GRID_CHI \ ; over default region

> SGRID MK41 AMODEL Z=0.006            ; Invoke with a redshift
\end{verbatim}\end{quote}
\subsection{Errors}
Errors arising in SGRID fall into three catagories,

\begin{enumerate}
\item Errors generated by spectral models due to inappropriate
parameter values.
\item Errors generated during minimisation when there are free
parameters remaining after the grid axes are defined, eg.
matrix of second derivatives of chi-squared or Cash
statistic (with respect to the parameters) may become
singular or ill-conditioned.
\item Failure to optimise at a grid point due to insufficient
iterations.
\end{enumerate}
In the first two cases SGRID will generate a bad quality (MISSING)
grid point for the combination of grid parameters giving the error.
If this is a problem then that area of parameter space giving the
the errors should not be gridded.

The third case is handled differently. On the assumption that a
good fit would have been achieved had sufficient iterations been
allowed, SGRID uses the value of the statistic but flags it by
setting the IGNORE quality bit. The quality mask is set so that
these points will be processed by other Asterix software. If it
is required that these points be treated as bad like the first
two cases above, then this can be achieved by using the MASK
application to set the quality mask to 11111111.

\section{SMHELP*}
Access the spectral model help library. For example, to get help
on a model with keyword PL, type
\begin{quote}\begin{verbatim}
    > SMHELP PL
\end{verbatim}\end{quote}
Output can be redirected using the same qualifiers as are accepted
by {\tt asthelp}.

\section{SMODEL}
Creates or updates a fit\_model data object containing a composite
model (cmodel) specification and initial values and ranges for
the parameter set associated with each component primitive model
(pmodel). Pmodels all have keywords consisting of two
alphanumeric characters and may be of two basic types. Additive
pmodels (e.g. BR, = bremmstrahlung) have units of flux, whilst
multiplicative models (e.g. AB, =absorption) are dimensionless,
and act to modify the additive model components. Cmodels are
specified using a staightforward mathematical syntax in which
brackets and + - and * operations are allowed, for example:
\begin{verbatim}
                       PL + AB * (BR - LG)
\end{verbatim}
The cmodel is checked by the program, and translated into a
reverse Polish string from which it is more readily computed.

If an existing fit\_model object with the required name is found
then its model prescription will be displayed, and you will be
asked if you want to accept it or overwrite it with a new
prescription.

Finally the default or existing parameter values for each model
component, as well as their lower and upper bounds, will be
displayed. You may accept these values, or define new ones (one,
two or three numbers can be entered). The bounds are used by
fitting routines to prevent unphysical or uninteresting regions
of parameter space being accessed.

\subsection{Input Output Data}
Input: fit\_model data object (if present)
Output: modified fit\_model data object
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type        Description
 -------  --------  ----        -----------
 MODEL       1      UNIV        Data object to contain
                                spectral model.
           GLOBAL.FIT_MODEL (Default model dataset)
 OVER        2      LOGICAL     Override with new model
                                spec?
     Hidden default = NO
 SPEC        3      CHAR        String containing model
                                specification.
 VALS        4      CHAR        String containing parameter
                                value and/or limits.
 RESTART     -      LOGICAL     Reset parameter values and
                                limits of existing model
     Hidden default = NO
 GEN         -      CHAR        Model class (SPEC,STAT etc).

\end{verbatim}\subsection{Examples}
In the following examples the information about existing models,
parameters etc. output to the terminal by the program has been
omitted for clarity.

Creating new fit\_model object

\begin{quote}\begin{verbatim}
 > SMODEL
 Model > SPECMOD
 Model specification > AB*(BR+LG)
 New values/'none'/ > 1,0.01,10            (value, lower, upper bounds)
 New values/'none'/ >     (accept current values)
 New values/'none'/ > 5.4                  (value only entered)
 New values/'none'/ > ,0.1,100             (limits only - comma delimits)
 New values/'none'/ > ,,3.0E4              (E format also accepted)
\end{verbatim}\end{quote}
Modifying existing fit\_model
\begin{quote}\begin{verbatim}
 > SMODEL

 Existing model is :

 BB + BH

 Override with new model specification? > N
 New values/'none'/ > 0.12
     etc.
\end{verbatim}\end{quote}
Reset values and limits of existing model
\begin{quote}\begin{verbatim}
ICL> SMODEL RESTART
\end{verbatim}\end{quote}
\section{SPLOT}
Plots the data currently being fitted with overlayed model and
separate residuals if required. It works in one of two modes.
In default mode it plots directly to a given output device.
Alternatively if the SAVE keyword is supplied on the command line
then the data will be saved to a file which may subsequently be
plotted with GDRAW or further customised with GSET. The output
is to a multiple dataset which may be inspected/manipulated with
GMULTI. Data may also be redshifted and where non-zero redshift
is specified, the plotted model (whether in photon of channel
space) is plotted as seen from Earth (i.e. redshifted relative to
the source frame model).

The content and form of the plot are determined by the response
to the MODE parameter.

Options available for plotting are combinations of

\begin{verbatim}
       C or D - Channel/Data space spectrum (with overlaid model)
       P      - Photon space spectrum (with overlaid model)
       R      - Residuals (data-model)
       E      - Plot all x-axes in energy
       L      - Plot all axes logarithmic
       M      - Plot components of model only
       O      - Overlay sumultaneously fitted data on common axes
\end{verbatim}
Replying with 'HELP' to this parameter will produce this list at
run-time.

NOTE!!!! : PR is an abbreviation of the ADAM keyword PROMPT, so this
combination should either be put in quotes or reversed if
entered on the command line.
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type        Description
 -------  --------  ----        -----------
 MODE        1      CHAR        Plot mode (C,P,R combinations)
 DEV         -      CHAR        Plot device
 INP         -      UNIV        Input data (either a single
                                dataset or a file of references)
            GLOBAL.FIT_DATA (Default dataset)
 MODEL       -      UNIV        Data object containing model
                                specification.
            GLOBAL.FIT_MODEL (Default model dataset)
 Z                  REAL        Source redshift
            GLOBAL.REDSHIFT (Default redshift parameter)
      Default = 0
 ELOW        -      REAL        Lower energy bound for model plot
 EUPP        -      REAL        Upper energy bound for model plot
 NCHAN       -      INT         No. of channels/bins for energy range
 SAVE        -      LOGICAL     Save to a file
      Default = N
 OUT         -      UNIV        Output binned dataset.
               -> GLOBAL.BINDS (Default binned dataset)
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > SPLOT CPR DEV=xwindow \        - plot current data in channnel
                                    and photon space with residuals

 > SPLOT CE SAVE OUT=OUTFILE \    - save channel space data to file
                                    with energy x-axis

 > SPLOT "PR" \                   - plot in photon space with residuals
 > SPLOT RP \                        "        "      "    "       "

 > SPLOT M                        - plot components of model
 > SPLOT ML                       - same with log axes

 > SPLOT CPRO                     - overlay simultaneously fitted data
                                    on same axes
\end{verbatim}\end{quote}
\subsection{Method}
Straightforward, apart from the transformation into the photon
space. The model is readily available in this space, but the data
have to be transformed. This can only be done approximately,
using:
\begin{quote}\begin{verbatim}
  photon space data = channel data * photon model / channel model
\end{verbatim}\end{quote}
where the channel model is the result of folding the photon space
model through the instrument response. Note that the photon space
data derived from the same spectrum via different models will be
different. The transformation is strictly correct only if the
model is correct. Hence photon space plots obtained from poorly
fitting models are suspect.

Data errors are transformed into the photon space in exactly the
same way as the data.

\section{SSHOW}
All details from a spectral model are dumped to the ascii
device selected using the standard ASTERIX mechanism for
text output, which allows output to terminal, file or
hardcopy device.

If output is to be sent to the terminal then 132 column
mode will give a neater display.

\subsection{Input Output Data}
Input: Any spectral model
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type        Description
 -------  --------  ----        -----------

 MODEL       1      UNIV        Data object containing model
                                specification.
           GLOBAL.FIT_MODEL (Default model dataset)
 DEV         2      CHAR        Ascii device specification. See
                                text output for details.
   Hidden default = TERMINAL
\end{verbatim}\subsection{Examples}
Display global model to terminal
\begin{quote}\begin{verbatim}
   > SSHOW \
\end{verbatim}\end{quote}
A named model to the printer
\begin{quote}\begin{verbatim}
> SSHOW MOD_AGPL PRINTER
\end{verbatim}\end{quote}
\section{SSIM}
The flux from a spectral model is calculated, folded through
an instrument response and a background spectrum and Poisson
noise are optionally added, to give a simulated spectral
observation. Since results are given in count/sec the
observation time need only be specified where Poisson errors
are to be included.

The input file can be any object which contains an
instrument response (it need not be a SPECTRUM). Channel
values for the simulated spectrum are taken from the response
structure itself. If a redshift is currently set then it will
be applied to the model (you will be informed if a non-zero
redshift is in force). A new redshift can be specified on the
command line.

The output file becomes the current binned dataset
and so can be displayed by typing GDRAW.

\subsection{Input Output Data}
Input: Any dataset containing an instrument response
fit\_model dataset

Background: This dataset (if required) must be the same size
as the output spectrum (i.e. it must match the
number of channels in the instrument response).
Any negative values in the background will be taken
as zero.

Output: Simulated SPECTRUM dataset

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type        Description
 -------  --------  ----        -----------

 INP         1      UNIV        Input data (either a single
                                dataset or a file of
                                references).
            GLOBAL.FIT_DATA (Default dataset)
 MODEL       2      UNIV        Data object containing model
                                specification.
           GLOBAL.FIT_MODEL (Default model dataset)
 BG          3      UNIV        Background spectrum.
      Default = !
 POISS       4      LOGICAL     Poisson noise to be added (Y)?
 OBLEN       5      REAL        Observation length (in
                                seconds).
 Z           -      REAL        Source redshift
            GLOBAL.REDSHIFT (Default redshift parameter)
      Default = 0
 OUT         6      UNIV        Output binned dataset.
              -> GLOBAL.BINDS (Default binned dataset)

\end{verbatim}\subsection{Examples}
Typical usage (global model used)
\begin{quote}\begin{verbatim}
ICL> SSIM
Object containing instrument response > SPECT
Background spectrum > BGSPEC
Add Poisson noise? > Y
Observation length > 10000
Output spectrum > SIMSPEC
\end{verbatim}\end{quote}
Using keywords
\begin{quote}\begin{verbatim}
ICL> SSIM INPUT=SPECT MODEL=SPECMOD Z=0.1
 Background spectrum > !            (no background)
 Add Poisson noise? > N        (no obs.length required therefore)
 Output spectrum > SIMSPEC
\end{verbatim}\end{quote}
\section{TIE}
Adds, modifies or deletes fit constraints. The program is controlled
by 3 mode keywords, ADD, SHOW and CANCEL. If none of these is specified
then SHOW is used.

The only constraint current supported is the equality constraint - the
syntax for this is simply to enumerate the numbers of the parameters
to be constrained.

\subsection{Input Output Data}
Input: fit-model dataset (modified)
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type        Description
 -------  --------  ----        -----------
 MODEL       -      UNIV        Data object containing fit
                                model.
           GLOBAL.FIT_MODEL (Default model dataset)

 NUM         1      CHAR        List of ties to operate on (either in
   Default = *                  in SHOW or CANCEL modes
 PARS        2      CHAR        Constraint string. Currently just a
                                list of parameters for equality constraint
 ADD         -      LOGICAL     Add a new tie?
   Default = NO
 SHOW        -      LOGICAL     Display ties
   Default = NO
 CANCEL      -      LOGICAL     Delete a tie
   Default = NO

\end{verbatim}\subsection{Example}
Given the two temperature model AB*(RZ+RZ) constrain the metallicities
of the two additive components to be the same. First display the model
(bounds omitted):
\begin{quote}\begin{verbatim}
 > sshow \

Existing model is: AB*(RZ+RZ)
Component 1 - AB
  1) hydrogen column density    (1.0E21 /cm**2)          EF  0.30000
Component 2 - RZ
  2) em10                       (1E60cm**-3/(10kpc)**2)  E   1.91760E-05
  3) temperature                (keV)                    E   0.93121
  4) metal abundance            (solar)                  E   0.25343
Component 3 - RZ
  5) em10                       (1E60cm**-3/(10kpc)**2)  E   2.82116E-05
  6) temperature                (keV)                    E   0.98383
  7) metal abundance            (solar)                  E   0.32822
\end{verbatim}\end{quote}
Add a tie,
\begin{quote}\begin{verbatim}
 > tie add

 TIE Version 1.8-0
 NUM - Tie numbers /'1'/ >
 PARS - Parameters to tie together > 4,7
\end{verbatim}\end{quote}
Display the model again:
\begin{quote}\begin{verbatim}
 > sshow \                          ;

Existing model is: AB*(RZ+RZ)
Component 1 - AB
  1) hydrogen column density    (1.0E21 /cm**2)          EF  0.30000
Component 2 - RZ
  2) em10                       (1E60cm**-3/(10kpc)**2)  E   1.91760E-05
  3) temperature                (keV)                    E   0.93121
  4) metal abundance            (solar)                  E   0.25343
Component 3 - RZ
  5) em10                       (1E60cm**-3/(10kpc)**2)  E   2.82116E-05
  6) temperature                (keV)                    E   0.98383
  7) metal abundance            (solar)                  EC  0.32822
\end{verbatim}\end{quote}
Note that parameter 7 is now constrained to be equal to parameter 4. The
constraints are not actually applied until the first invocation of SFIT
or SERROR. The ties can be displayed using,
\begin{quote}\begin{verbatim}
> tie show \\                         ;

 TIE Version 1.8-0
 Tie  Constraint

   1  Equal(4,7)
\end{verbatim}\end{quote}
\section{THAW}
Thaws model parameters (previously frozen with FREEZE) at either
current or user-specified values so that they can be varied
during fitting.

Note that THAW can also be used to simply change the value of
parameters.

\subsection{Input Output Data}
Input: fit-model dataset (modified)
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type        Description
 -------  --------  ----        -----------
 MODEL       -      UNIV        Data object containing fit
                                model.
           GLOBAL.FIT_MODEL (Default model dataset)
 PAR         1      INTEGER     Numbers of parameters to be
                    array       thawed.
 VALS        2      REAL        Values at which parameters
                    array       will be set
     Hidden default = !
\end{verbatim}\subsection{Examples}
Typical usage. Thaw parameter 2
\begin{quote}\begin{verbatim}
> THAW 2
\end{verbatim}\end{quote}
Thaw parameter 3 at 5.2
\begin{quote}\begin{verbatim}
> THAW 3 5.2
\end{verbatim}\end{quote}
Thaw parameters 2, 4 and 5
\begin{quote}\begin{verbatim}
> THAW [2 4 5]
\end{verbatim}\end{quote}
\begin{quote}\begin{verbatim}
 > THAW [1 4] [.1 3]     Thaw at given values
\end{verbatim}\end{quote}
Full prompts
\begin{quote}\begin{verbatim}
> THAW
 Model > SPECT          (no global defined)
 Parameter numbers > 2 3
 Parameter values > 5 0
\end{verbatim}\end{quote}
\section{Future developments}
A number of extensions to the current spectral fitting package
are envisaged. These include:
\begin{itemize}
\item A user-model facility
\item Fitting to collections of spectra (e.g. spectral images)
\item Extension of the range of spectral models available
\end{itemize}
If you have any additional suggestions, and particularly any useful
software to contribute (e.g. spectral models), please contact
asterix@star.sr.bham.ac.uk.

\chapter{Image processing commands}
\section{Introduction}
The image processing facilities provided by ASTERIX comprise
two types of command. There are a number of standard commands
of the 'input-process-output' type, and there are commands
which form part of an interactive image processing system.
Commands should be assumed to be part of the interactive system
unless stated otherwise in the help.

The interactive system makes use of interactive graphic display
and cursor or keyboard input to provide fast and friendly image
processing capabilities. The system must be activated with the
command ILOAD which copies the image to be processed into dynamic
memory and opens the display device if necessary. Processing is
performed on the copy, not on the original file, and a buffer is
kept which allows the user to go back one stage of processing with
the command IUNDO. Default mode for entry of positions etc.
is via interactive cursor. Keyboard entry can be selected as
an option when the system is started and can be changed by the
command IMODE. As well as facilities for manipulating
and displaying the 2D image data, there are commands which
derive 1D data, such as radial profiles. At any point in
processing the 2D and 1D data may be saved to file with the
commands ISAVE and ISAVE1D and may then be processed by other
ASTERIX commands. The original image file may be processed
by other ASTERIX commands whilst the interactive system is
active. A new image may be loaded at any time with the same
command, ILOAD and a different display device (eg hardcopy)
can be opened with DOPEN without losing the image. To release
the image use the command ICLOSE. The results of processing will
be lost at this point unless they have been saved explicitly.

The exact format of both 2D and 1D plots can be controlled
using GSET. All plotting attributes thus set are saved with
the data by ISAVE and ISAVE1D, such that the general plotting
command GDRAW will reproduce identical plots.

\section{Recipes}

\begin{enumerate}
\item Make a simple investigation of an image
\begin{quote}\begin{verbatim}
    ILOAD image_file                 load the image
    IDISP                            display it
    ISCALE                           change scaling and colours
    ICOLOUR                          to highlight features
    IBROWSE                          browse around looking at data values
    ICONTOUR                         put on some contours
    IBOXSTATS                        statistics on sections of image
    ISLICE                           profiles of sections of image
    ICLOSE                           shut down
\end{verbatim}\end{quote}
\item Refine background subtraction and source detection of a previously
auto-subtracted and searched image
\begin{quote}\begin{verbatim}
    ILOAD raw                        load unsubtracted, raw image
    IDISP                            display it
    IEX SOURCE LIST=source_list      remove auto-detected sources
    IPATCH                           patch up holes
    ISTATS                           get new peak values
    ISCALE                           change scaling to bring out peaks
    IPEAKS                           mark high spots
    IEX PIX                          remove other high spots
    IPATCH                           patch new holes
    ISAVE bgnd                       save to file
    ILOAD raw                        reload raw image
    ISUB bgnd                        subtract background
    IDISP                            display it
    .
    .
    ICLOSE
\end{verbatim}\end{quote}
\item Investigate possible extended sources
\begin{quote}\begin{verbatim}
    ILOAD image                      load image
    IDISP                            display it
    IZOOM                            zoom in on relevant section
    IBROWSE                          investigate data values
    ICONTOUR                         mark with contours
    IRADIAL                          plot radial distribution
    IPSF                             compare with point-spread-function
    IAZIM                            look for azimuthal variation
    .
    .
    ICLOSE
\end{verbatim}\end{quote}
\item Investigate a specific area for an expected source
\begin{quote}\begin{verbatim}
    ILOAD image                      load image
    IDISP                            display it
    IGRID 1                          put RA/DEC grid on it
    IZOOM                            zoom in on area of interest
    IMODE                            switch to keyboard mode
    IPOSIT (RA) (DEC)                find exact spot
    IBOXSTATS                        get max and min of small area
    ISCALE                           rescale to highlight features
    ICONTOUR                         put on contours
    .
    .
    ICLOSE
\end{verbatim}\end{quote}
\end{enumerate}
\section{Getting hardcopy}
There are two routes for getting hardcopy of your current image
or 1D derivative:

\begin{quote}\begin{verbatim}
> DOPEN hardcopy_device
> IDISP                                   .
> DOPEN interactive_device
\end{verbatim}\end{quote}
or,
\begin{quote}\begin{verbatim}
> ISAVE new_image
  ...
> GDRAW new_image hardcopy_device        ; after interactive session
\end{verbatim}\end{quote}
\section{Command summary}

\begin{description}
\item[IADD]
add image or scalar value
\item[IAZIMUTH]
azimuthal distribution around point
\item[IBLUR]
smooth image
\item[IBOX]
define box of interest
\item[IBROWSE]
inspect values around selected points
\item[ICENTROID]
get centroid of circular region
\item[ICIRCLE]
define circle of interest
\item[ICLEAR]
clear current zone
\item[ICLOSE]
release image
\item[ICOLOUR]
select or change colour table
\item[ICONTOUR]
contour image
\item[ICURRENT]
show current position etc
\item[IDISPLAY]
display image
\item[IDIV]
divide by specified image or scalar
\item[IDMSTOD]
dd:mm:ss to decimal degree
\item[IDTODMS]
decimal degree to dd:mm:ss
\item[IDTOHMS]
decimal degree to hh:mm:ss
\item[IHMSTOD]
hh:mm:ss to decimal degree
\item[IEXCLUDE]
remove pixels, sources or regions (set QUAL=IGNORE)
\item[IGRID]
put coordinate grid on image
\item[IHIST]
histogram pixel values
\item[IKEY]
put key at side of image
\item[ILOAD]
load an image
\item[IMARK]
mark positions on plot
\item[IMASK]
change QUALITY mask
\item[IMODE]
change input mode CURSOR/KEYBOARD
\item[IMOSAIC]
mosaic several images into one
\item[IMULT]
multiply by image or scalar value
\item[INOISE]
add random noise
\item[IPATCH]
patch bad pixels
\item[IPEAKS]
mark peaks above threshold
\item[IPLOT]
plot 1D data
\item[IPIXEL]
pixel plot of image
\item[IPOLAR]
polar distribution
\item[IPOSIT]
specifiy position on image
\item[IPSF]
put PSF on radial distribution
\item[IRADIAL]
radial distribution
\item[IREGION]
define a spatial region
\item[ISAVE]
save image
\item[ISAVE1D]
save 1D derivatives
\item[ISCALE]
change scaling of pixel plot
\item[ISEP]
angular separation of two points
\item[ISLICE]
profile of section of image
\item[ISTATS]
basic statistics
\item[IBOXSTATS]
statistics of specified box
\item[ICIRCSTATS]
statistics of specified circle
\item[ISUB]
subtract image or scalar value
\item[ISURFACE]
surface plot of image
\item[IUNDO]
undo last processing step
\item[IWHOLE]
select whole image
\item[IXCONV]
2D convolution of images
\item[IZED]
bin vertically to image plane from cube
\item[IZOOM]
zoom image display
\item[IUNZOOM]
unzoom image display
\end{description}
\section{IADD}
Add given image, array or scalar constant to current image.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   INP       1      CHAR       Image or constant to add

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IADD 1.0             add 1.0 to each pixel

 > IADD IMG2            add IMG2
\end{verbatim}\end{quote}
\section{IAZIMUTH}
Specifies a circular region of the image and plots an integrated
azimuthal distribution with a specified number of bins in the next
zone. The azimuthal bins are displayed on the image. The centre
and radius of the circle are selected either by cursor or parameter
depending on the current mode (change mode with command IMODE).

The data units are normalised to be per square axis unit. So if
the axes of your image are in degrees the units of the azimuthal
profile will be "counts/square degree".

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 XCENT       1      REAL       X-coord of centre (axis units).
 YCENT       2      REAL       Y-coord of centre   "     "
 RAD         3      REAL       Radius of circle (axis units).
 NBIN        4      INTEGER    Number of bins (segments).

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IAZIMUTH NBIN=12         bin in 30 degree bins
\end{verbatim}\end{quote}
\section{IBLUR}
Blurs current image by applying a Gaussian or Box filter of specified
widths. The FWHM of the Gaussian or the width of the box are specified
in pixels.

If the image contains bad quality pixels these will be treated as
zero and a warning message will be given.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 GAUSS       -      LOGICAL    Use Gaussian (or Box)filter.
      Default = YES
 XWID        1      REAL       FWHM or box width in x (pixels).
      Default = 1.0
 YWID        2      REAL       FWHM or box width in y (pixels).
      Default = 1.0

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> IBLUR GAUSS 2.5 2.5        perform Gaussian smooth
.
> IBLURG 2.5 2.5             same as above
\end{verbatim}\end{quote}
\subsection{Shortforms}
\begin{verbatim}
             IBLURG = IBLUR GAUSS       smooth with Gaussian filter
             IBLURB = IBLUR NOGAUSS     smooth with box filter
\end{verbatim}
\section{IBLURG+}
Shortform of IBLUR which selects a Gaussian filter.

\section{IBLURB+}
Shortform of IBLUR which selects a Box filter.

\section{IBOX}
Select a rectangular subset of the image for further use. Where
applicable and sensible, subsequent commands will act only on the
pixels within the box. The box is specified by its centre and
size in each dimension, either by cursor or parameter, depending
on the current mode (change mode with command IMODE). The box
selected is displayed on the image.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 XCENT       1      REAL       X-coord of box centre (axis units).
 YCENT       2      REAL       Y-coord of box centre (axis units).
 XWID        3      REAL       Horizontal full-width (axis units).
 YWID        4      REAL       Vertical full-width (axis units).

\end{verbatim}\section{IBROWSE}
Displays data, variance, error, significance or quality around
selected points in the image, together with the position of
the central point in various coordinate systems. The displayed
values may be scaled by a power of 10 using the SCALE parameter.

To select points in cursor mode use mouse buttons or any keyboard
keys other than the special keys listed below. In keyboard mode a
single position is requested (default = current position) and a
single screen of information presented.

Pressing any of the following keys will change the display as
described whilst preserving the position previously selected
(NOTE: these keys are input to the graphics window, so this
must be the current one).

\begin{verbatim}
      X - exit from IBROWSE           D - display data values
      V - display variance values     E - display errors
      S - display significances       Q - display quality flags
      > - scale up by 10              < - scale down by 10
 \end{verbatim}
The final data (or variance/error/significance/quality) value may
be extracted via a command line parameter.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 VAR         -      LOGICAL    Display variance (not data)values.
     Hidden default = NO
 ERR         -      LOGICAL    Display error (not data)values.
     Hidden default = NO
 SIGNIF      -      LOGICAL    Display significance (not data)values.
     Hidden default = NO
 QUAL        -      LOGICAL    Display quality (not data)values.
     Hidden default = NO
 SCALE       -      INT        Power of 10 to scale values by
     Hidden default = 0
 XPOS        1      REAL       x-pos for keyboard mode.
 YPOS        2      REAL       y-pos for keyboard mode.
 VAL         -      REAL       Final data value output.

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IBROWSE 3.5 10.6      - display values around given
                           axis positions (keyboard mode)

 > IBROWSE VAL=(Z)       - extract final value into variable Z

 > IBROWSE VAR           - display variances (until changed by key-press)

 > IBROWSE SCALE=2       - scale displayed values by 10**2
\end{verbatim}\end{quote}
\section{ICENTROID}
Selects a circle on the image by cursor or parameter and finds
the centroid of that circle. If more than one iteration are
specified it uses the centroid as the centre of a new circle
and repeats the process. The final centroid may be returned
to ICL variables and also becomes the current position which
can be displayed in all coordinate systems with ICURRENT.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
    X        1      REAL       Centre of starting circle.
    Y        2      REAL           "          "
    RAD      3      REAL       Radius of circle (axis units).
    ITER     4      INTEGER    Number of iterations.
        Default = 1
    XCENT    -      REAL       Centroid centre (output).
    YCENT    -      REAL           "      "        "

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ICENT XCENT=(X) YCENT=(Y)               cursor mode

 > ICENT 25.5 35.5 15 XCENT=(X) YCENT=(Y)  keyboard mode
\end{verbatim}\end{quote}
\section{ICIRCLE}
Select a circular subset of the image for further use. Where
applicable and sensible, subsequent commands will act only on the
pixels within the circle. The circle is specified by its centre and
radius, either by cursor or parameter, depending on the current mode
(change mode with command IMODE). The circle selected is displayed
on the image.

Note that after using ICIRCLE, subsequent commands will act only
on the data within the circle (eg ISTATS) or the box containing
the circle (eg. IDISP) as appropriate. To use this command
purely for marking circles, follow it with IWHOLE to switch off
the selection.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 XCENT       1      REAL       X-coord of centre (axis units).
 YCENT       2      REAL       Y-coord of centre (axis units).
 RAD         3      REAL       Radius in axis units.

\end{verbatim}\section{ICLEAR}
Clears the current zone. Subsequent display or plot commands will
consider this as the next clear zone.

\section{ICLOSE}
Closes interactive image processing system. The file is closed
and the device is closed only if no other sub-package has been
using it.

\section{ICNTSUB*}
A menu driven procedure to calculate the count rates in sources
contained in an image displayed with IDISPLAY. It allows sources
to be background subtracted if desired and optionally stores the
results in a text file.

\subsection{Method}
The main menu contains buttons which allow a source region or
background region to be defined and allow the two to be subtracted.
Once a background or source region has been defined, they become
the default; subtractions will use these regions until they are
superceeded. This menu includes the following options:


\begin{description}
\item[OUTPUT]
Allows an output file to be specified - by default all
the results are written to the terminal.
\item[MULTI]
Once a background region has been defined - subtractions
can be performed on a series of sources by selecting the
MULTI option. This displays a further menu which has the
choices NEWSRC and QUIT. The NEWSRC button must be pressed
before each source is selected.
\item[SUBTCT]
Subtracts the current background data from the current
source data, normalising for area.
\item[Bckgnd]
Defines the background region. This displays a further menu
which allows the box shape to be selected.
\item[Source]
Defines the source region. This displays a further menu
which allows the box shape to be selected.
\item[SETBAD]
Allows pixels within a polygon to be set bad.
\item[ICL]
Enables ICL temporarily so that a single command such as:
IZOOM, ISCALE, \$DIR etc... can be entered
\end{description}
\subsubsection{Bckgnd menu}
Allows a bckgnd box shape to be selected.

\subsubsection{Multi menu}
When the "Multi" button has been pressed a further menu containing
the buttons, NEWSRC, COMMENT and QUIT is displayed. Each time a
new source is wanted, the NEWSRC button must be pressed. To write
a comment into the output file, at any stage - press the COMMENT
button.

\subsubsection{Source menu}
Allows the box shape to be selected. There are two extra options
which only apply if the source box is circular; CNTROID which
tells the procedure to centroid on the selected source before
calculating the counts and FIXRAD which sets the circle radius
to a fixed value (should be entered in axis units).

\section{ICOLOUR}
Changes the colour table used to display images. It operates
in the modes given below:

\begin{verbatim}
   DEFault    default table      GREY      select greyscale
   GREEN      green table        BLUE      blue table
   RED        red table          RAINbow   rainbow table
   RGB        3-colour table     NEG       invert current table
   NORM       switch off NEG     POS       same as NORM
   EDIT       interactive edit   READ      read external table
 \end{verbatim}
EDIT mode the RGB components of 16 colours equally spaced
throughout the full range are displayed on the terminal screen.
As the values of these are changed, the colours inbetween are
interpolated and the resulting changes shown on the graphics
display surface. In READ mode the name of an HDS object
containing a colour table is specified.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  MODE        1     CHAR       Mode of operation
  TABLE       2     UNIV       External HDS table

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ICOL GREY              select greyscale

 > ICOL NEG               invert colours

 > ICOL RED               red table

 > ICOL EDIT              edit present table

 > ICOL READ MY_TABLE     read table from specified object
\end{verbatim}\end{quote}
\section{ICONTOUR}
Draws a contour plot at the specified levels. If a contour or
pixel plot is currently being displayed the contours will be
superimposed.

Contours may be switched off using the OFF keyword, or
alternatively using the GSET command.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   LEVELS    1      REAL       Contour levels
   OFF       -      LOGICAL    Switch contours off

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ICONTOUR 0.5               contour at a value of 0.5

 > ICONTOUR [0.2 0.4 0.6 0.8] contour at several levels

 > ICONTOUR OFF               switch contours off
\end{verbatim}\end{quote}
\section{ICURRENT}
Allows current values (eg. position) to be extracted from the image
processing environment. Note that all celestial positions are
in decimal degrees. Other facilities are provided to convert these
to more conventional formats eg IDTOHMS will convert to hh:mm:ss.
Using the SUPPRESS option will suppress output to the screen.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   X         1      REAL       Current x position (axis coords)
   Y         2      REAL       Current y position (axis coords)
   RAD       -      REAL       Radius of region (axis units)
   RA        -      DOUBLE     Current RA (degrees)
   DEC       -      DOUBLE     Current DEC (degrees)
   ELON      -      DOUBLE     Current Ecl.long (degrees)
   ELAT      -      DOUBLE     Current Ecl.lat  (degrees)
   L         -      DOUBLE     Current Gal.long (degrees)
   B         -      DOUBLE     Current Gal.lat  (degrees)
   X1        -      REAL       Edges of current region
   X2        -      REAL       (axis coords).
   Y1        -      REAL           "           "
   Y2        -      REAL           "           "
   NAME      -      CHAR       name of current dataset.
   MODE      -      CHAR       current mode 'curs' or 'key'.
   SUPPRESS  -      LOGICAL    suppress screen output.
       Hidden default = NO

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> ICURR (X) (Y)             get current position in axis coords
                            into ICL variables X and Y

> ICURR RA=(RA) DEC=(DEC)   get current RA and DEC into ICL
                            variables

> ICURR X1=(X1) X2=(X2) Y1=(Y1) Y2=(Y2)
                            get currently selected region into
                            variables
> ICURR NAME=(DATASET)      extract dataset name
\end{verbatim}\end{quote}
\section{IDISPLAY}
Display current image with whatever additions have been made to
it eg. contours, grid, key etc. For a simple pixel plot only use
IPIXEL.

\section{IDIV}
Divide current image by given image or scalar constant.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   INP       1      CHAR       Image or constant to divide by

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IDIV 2.0                divide each pixel by 2.0

 > IDIV IMG2               divide by IMG2
\end{verbatim}\end{quote}
\section{IDMSTOD*}
Macro command which converts a string containing deg:min:sec to
decimal degrees. The input and output parameters must be supplied
on the command line. eg.

\begin{quote}\begin{verbatim}
   > IDMSTOD "45:30:00.00" (D)  where D is an ICL real variable
   > =D
      45.5
 \end{verbatim}\end{quote}
This command does not require the interactive image processing
system to be active and can be invoked at any time.

\section{IDTODMS*}
Macro command which converts decimal degrees to a string containing
degrees:minutes:seconds. The input and output parameters must be
supplied on the command line. eg.

\begin{quote}\begin{verbatim}
   > IDTODMS 45.5 (DMS)        where DMS is an ICL string variable
   > =DMS
      45:30:00.00
 \end{verbatim}\end{quote}
This command does not require the interactive image processing
system to be active and can be invoked at any time.

\section{IDTOHMS*}
Macro command which converts decimal degrees to a string containing
hours:minutes:seconds. The input and output parameters must be
supplied on the command line. eg.

\begin{quote}\begin{verbatim}
   > IDTOHMS 45.5 (HMS)    where HMS is an ICL string variable
   > =HMS
      03:02:00.00
 \end{verbatim}\end{quote}
This command does not require the interactive image processing
system to be active and can be invoked at any time.

\section{IHMSTOD*}
Macro command which converts a string containing hours:min:sec to
decimal degrees. The input and output parameters must be supplied
on the command line. eg.

\begin{quote}\begin{verbatim}
   > IHMSTOD "03:02:00.00" (D)  where D is an ICL real variable
   > =D
      45.5
 \end{verbatim}\end{quote}
This command does not require the interactive image processing
system to be active and can be invoked at any time.

\section{IEXCLUDE}
Excludes specified pixels from display and processing by setting
the IGNORE quality bit for those pixels. There are five modes:


\begin{description}
\item[PIXel]
Exclude individual pixels pointed to by the
cursor - space-bar or left button selects
for exclusion, \verb+<RETURN>+ or centre button
terminates
\item[SOUrce]
Excludes pixels up to a specified radius from
sources specified by RA/DEC, which can be given
as a text file, an ASTERIX source list or separate
RA and DEC arrays
\item[CIRcle]
Excludes pixels inside a specified circle
\item[POLygon]
Excludes pixels within a polygon whose vertices
are specified with the cursor - space-bar or
left button selects position of vertex, \verb+<RETURN>+
or centre button indicates last vertex and
terminates - the first and last vertices are
automatically joined
\item[REGion]
Excludes pixels in a region previously defined
by IREGION or by a specified ARDfile
\end{description}
The keyword OUTSIDE causes the function of CIRCLE and POLYGON modes
to be inverted ie. pixels outside are excluded.

The masked QUALITY array can be viewed with IQPIXEL.

The effect of IEXCLUDE can be toggled on and off by the use of
IMASK, (set the most significant bit to 0 to switch off).

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   MODE      1      CHAR       PIXel,SOUrce,CIRcle,POLygon,REGion
   CURR      -      LOGICAL    Use current region defined by
                               IREGION when in REGion mode
   RAD       -      REAL       exclusion radius
   XCENT     -      REAL       circle centre for keyboard mode
   YCENT     -      REAL                "            "
   LIST      -      CHAR       source list file
   RA        -      REAL       array of RAs
   DEC       -      REAL       array of DECs
   OUTSIDE   -      LOGICAL    exclude outside circle or polygon
       (Hidden default = NO)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IEXC PIX                interactively exclude individual pixels

 > IEXC SOURCE LIST=PSLIST RAD=5      exclude pixels up to a radius
                                      of 5 axis units from positions
                                      in given source list

 > IEXC CIRC OUTSIDE       exclude pixels outside given circle


 > IEXC REG CURR           exclude pixels in current region as
                           previously defined by IREGION
 \end{verbatim}\end{quote}
\section{IGRID}
Put a grid over the current image display in specified coordinates.
By default the coordinate labels are put at the edge of the image.
They can optionally be put outside or at the centre.

The grid can be switched off with the OFF keyword, or by using
the command GSET.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   FRAME      1     INTEGER    Coordinate frame  1 - RA/DEC
                                                 2 - Ecliptic
        Default = 1                              3 - Galactic
   POS        2     INTEGER    Label position    1 - edge
                                                 2 - centre
                                                 3 - outside
   OFF        -     LOGICAL    Switch grid off
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IGRID 1          put RA/DEC grid over image

 > IGRID 1 POS=2    put labels in centre

 > IGRID OFF        switch grid off
 \end{verbatim}\end{quote}
\section{IHIST}
Histograms data values within current specified region of image
and plots histogram in next zone.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  NBIN       1      INTEGER    Number of bins in histogram

\end{verbatim}\section{IKEY}
Puts a colour bar at side of image showing the scaling range.
The key can be switched off with the OFF keyword, or by using
the command GSET.

\section{ILABELS}
Changes the x and y labels on the current image or plot. At
present the labels are not remembered for subsequent plotting
of the same data and clearing the current zone for redisplay
will not work correctly.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  XLBL       1      CHAR       X-label
  YLBL       2      CHAR       Y-label

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ILAB "x-label" "y-label"
 \end{verbatim}\end{quote}
\section{ILOAD}
Loads an image into the interactive image processing system closing
any image already loaded. If there is no graphics device currently
open then it will prompt for a device and optionally for hard zones
to divide the display surface into. There is also the option - using
the DISP keyword - to display the image immediately.

There is also the option whether to load the Grafix Control Block
(GCB) associated with the file, or to leave existing control
parameters intact.

If a cube is used then the slice from which the image will be
formed must be specified by the SLICE parameter in terms of
the range of "z" axis values. (Note the non-spatial axis
- "z" - must be either the first or the third). To change
the slice, simply reload the cube. The default slice
offered is the full cube.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   INP       1      CHAR       File containing image
   GCB       -      LOGICAL    Load GCB from file
   DEV       2      CHAR       Device name
   NX        3      INTEGER    Number of horizontal zones
       Hidden default = 1
   NY        4      INTEGER    Number of vertical zones
       Hidden default = 1
   DISP      -      LOGICAL    Immediate display
       Hidden default = N
   MODE      5      INTEGER    Input mode 1=cursor 2=keyboard
       Hidden default = 1
   SLICE     -      CHAR       Range of "z" axis values
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> ILOAD IMG DISP                 load image and display on current
                                 device

> ILOAD IMG NOGCB                load image but keep existing
                                 Grafix Control
> ILOAD IMG IKON 2 2             load image and open device with
                                 2 x 2 zones

> ILOAD CUBE SLICE="10:50"       load cube
 \end{verbatim}\end{quote}
\section{IMARK}
Marks the current position, the current list of positions, or reads
source positions from a file and marks them on the image currently
displayed. If reading positions from a file, the file may be a text
file with RA/DEC in one of the standard formats and separated by one or
more spaces, or it may be an ASTERIX source list file as output by PSS.
If using a text file, then the filename should be enclosed in double
quotes. The symbol, size, colour and boldness may be specified.

A further mode may be invoked by giving a null (!) filename.
In this case separate HDS arrays for RA and DEC will be requested.

Symbols may be numbered by specifying the NUMBER keyword.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  CURR       -      LOGICAL    Mark the current position.
      (Hidden default = NO)
  ALL        -      LOGICAL    Mark all positions in current list.
      (Hidden default = NO)
  LIST       1      CHAR       Source list file (TEXT/HDS).
  RA         -      UNIV       Array of RAs.
  DEC        -      UNIV       Array of DECs.
  SYMBOL     -      INTEGER    Symbol number.
  COLOUR     -      INTEGER    Colour index.
  BOLD       -      INTEGER    Symbol boldness
  SIZE       -      REAL       Symbol size
  NUMBER     -      LOGICAL    Number symbols?
      (Hidden default = NO)
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 >IMARK CURR           mark current position

 >IMARK ALL            mark positions in current list

 >IMARK "SOURCE.LIS"   mark positions in given
                       text file

 >IMARK "SOURCE.LIS" SYMBOL=5 COLOUR=3
                       use different symbol and colour

 >IMARK SOURCE_LIST    mark positions in given
                       ASTERIX source list file

 >IMARK SOURCE_LIST    as aove but number the symbols

 >IMARK
  LIST file name> !        input positions as separate
  RA array of RA> ......   arrays of RA and DEC
  DEC array of DEC>.....
 \end{verbatim}\end{quote}
\section{IMASK}
Change the current QUALITY mask, for example to toggle on
and off the effect of IEXCLUDE.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  MASK       1      CHAR       new mask value

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IMASK 01111111           switch IGNORE bit off

 > IMASK 11111111           switch IGNORE bit back on
 \end{verbatim}\end{quote}
\section{IMODE}
Allows the mode to be specified (CURSor or KEYboard) or simply
toggles between them. When cursor mode is in operation subsequent
commands such as ISLICE will read input positions via interactive
cursor; if keyboard mode is active the same information will be
entered via parameters at the keyboard (or through command procedures).
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  CURS       -      LOGICAL    set cursor mode
    (Hidden default = NO)
  KEY        -      LOGICAL    set keyboard mode
    (Hidden default = NO)
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
  > IMODE             - toggles mode
  > IMODE CURS        - set cursor mode
  > IMODE KEY         - set keyboard mode
 \end{verbatim}\end{quote}
\section{IMOSAIC}
Combines the spatial dimensions of up to 20 datasets to produce one
output Asterix dataset. If datasets with more than 2 dimensions are
supplied (eg. spectral images), then the number and sizes of the
non-spatial dimensions must match. Otherwise arrays of any size may
be added, and if astrometry is available, files with different
pixel sizes and / or rotations will be correctly superimposed.

NB: This command is not part of the interactive image processing
system and does not require it to be active.
\subsection{Astrometry}
If the first file contains the following information in the correct
format for Asterix datasets:

a) RA, DEC and Position\_angle of instrument pointing direction.

b) Axis information describing the pixel size in each dimension.

and a second file has the same information, then the program can work
out the offsets of the second file from the first and rotate the second
so that it has the same orientation as the first.

Note: for this to work, the units of the axes must be the same !
There is no checking of axis units in the program.

\subsubsection{Algorithmn}
The program uses local spherical coordinate transformations centred at
the RA and DEC of each image frame. It is assumed that all the images
lie in the same plane as the first image. The validity of this
approximation depends on the size of the final image in degrees and
the number of pixels. As a rough guide, for a 512 x 512 pixel final
image it is accurate for up to a 5 degree by 5 degree field.

\subsubsection{Direction of X-axis}
The program works for files with X-axis values increasing from right to
left or from left to right. However, all the input files must have the
X axis increasing in the same direction for the program to produce a
sensible result using the astrometry data.

\subsection{No astrometry}
If there is insufficient astrometry in the current datafile or the
first file to calculate the offsets then the user is prompted for
them. The XOFFSET required is the number of pixels between the left
hand edge of the second data array and that of the first. If the second
array will be to the right of the image then XOFFSET will be positive.
Similarly YOFFSET is the distance between the bottom of the second
image and that of the first. If the second array will be above the
first then YOFFSET will be positive.

Note: In this case the program assumes that the input arrays have the
same intrinsic pixel size and the same orientation to North.

\subsection{Method}
The program uses the first input datafile to define the output
datafile in terms of its pixel size and orientation to North.
The datafiles to be mosaiced with this one are given offsets from
the first one in pixels, either by user input or calculation using
astrometry data in the datafiles. The images are then overlayed to
produce a (generally larger) output image. Where the input images
overlap and two or more input pixels are used to make up one output
pixel, the output is averaged by default. To just add up pixel
contributions without averaging, specify 'AVERAGE=N' on the command
line.

\subsubsection{Astrometry}
If astrometry information is present in the first input datafile and
a subsequent one then the program will calculate the offsets of the
second file from the first using the RA, DEC, ROLL and axis
information.

The second file is rebinned and rotated such that it has the same pixel
size as the first file in axis coordinates and has the same orientation
to North. It can then be added onto the first using the calculated
offsets.

\subsubsection{Quality}
A quality array is always produced in the output file. The quality of
the input pixels are checked and output pixels produced from one or
more 'good' input pixels have their quality byte set good. If an output
pixel is bad then its quality is set to 'missing'.

\subsubsection{Variance}
A variance array is produced in the output file only if all the input
files have variance arrays. The variance at a given pixel is the average
of the variances of contributing input pixels.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 NUMBER    1    INTEGER    Number of input datafiles (1 to 20).
 INP1      2    UNIV       Name of 1st input datafile
 INP2      3    UNIV       Name of 2nd input datafile
 INP3      -    UNIV       Name of 3rd input datafile
 INP4      -    UNIV       Name of 4th input datafile
   .
   .
   .
 INP20     -    UNIV       Name of 20th input datafile
 XOFFSET   -    INTEGER    The number of pixels between the left
                           hand edge of the current datafile and
                           that of the first file input.
 YOFFSET   -    INTEGER    The number of pixels between the bottom
                           edge of the current datafile and
                           that of the first file input.
 OUT       -    UNIV       Name of the output file.
 AVERAGE   -    LOGICAL    Average the pixels where two or more
                           pixels from the input data combine to
    Hidden default = YES   make one output pixel ? If NO then the
                           input pixels are just added. By default
                           averaging takes place.
 OTITLE    -    CHAR       Title of output image structure.
    Hidden default = Mosaic

\end{verbatim}\subsection{Examples}
To combine two images and average the overlapping pixels:
\begin{quote}\begin{verbatim}
ICL> IMOSAIC 2 IMAGE1 IMAGE2 OUT=IMAGE3 AVERAGE
\end{verbatim}\end{quote}
\section{IMULT}
Multiply current image by given image or scalar constant.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   INP       1      CHAR       Image or constant to multiply by

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
  > IMULT 2.0                 mult. all pixels by 2.0

  > IMULT IMG2                mult. by image IMG2
 \end{verbatim}\end{quote}
\section{INOISE}
Artificially adds random noise to the current image. The distribution
of the noise is Gaussian with standard deviation specified.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  SDEV       1      REAL       Standard deviation of noise

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > INOISE 3.0      add noise with std.dev. 3.0
 \end{verbatim}\end{quote}
\section{IPATCH}
Patches pixels with bad quality within the currently selected region
of the image. Patching can be done by linear interpolation from nearby
good pixels or by manual cut-and-paste, depending on the MODE parameter
value. The quality for patched pixels is flagged as 'patched',
but the quality mask is adjusted such that they will be seen as good.
This command may be used in combination with IEXCLUDE to remove sources
or other high spots from an image.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  MODE       1      CHAR       Patching mode - INTERP/PASTE

\end{verbatim}\subsection{Algorithm}
INTERP mode

The image (within the currently selected region) is scanned for bad
pixels. When one is found then six good pixels (this excludes already
patched ones) are looked for in each of the x and y directions,
preferably 3 on either side of the bad bit. A linear fit is performed
and an interpolated value derived separately in the x and y directions.
The value assigned to the bad pixel is the simple mean of these.

The std.dev. of the good points about each fitted line is used as
the interpolated error, given by sig**2 = sum(res**2)/(n-2). These
are combined to give a variance for the patched pixel using:
var = sig**2 = (sigx**2 + sigy**2)/4.

PASTE mode

Using the cursor the user selects a box to copy from. Clicking in
an area of bad QUALITY will then cause the box to be copied, centred
around the cursor position.

\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IREG CIRC          - mark a circular region
 > IPATCH             - patch within that region by interpolation

 > IPATCH PASTE       - patch by cut-and-paste
 \end{verbatim}\end{quote}
\section{IPEAKS}
Finds peaks above a specified threshold in the currently selected
region of the image, lists positions on terminal and marks positions
on image display.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  THRESH     1      REAL       Threshold value

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> IPEAKS 100.0         find peaks above 100
\end{verbatim}\end{quote}
\section{IPLOT}
Plot current 1D data if there are any. By default the data will be
plotted in the next clear zone, but may optionally be plotted in the
current zone after clearing it.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   NEXT       -      LOGICAL   Plot in next zone
        Hidden default = YES

\end{verbatim}\section{IPIXEL}
Plot image as a pixel plot only. This doesn't cancel any contours
etc. which may be in effect and these can be replotted with IDISP.

\section{IPOLAR}
A general command which produces azimuthal and radial distributions
from an image or cube. It can produce 1D radial and azimuthal
distributions or 2D radius versus azimuth distributions. If a cube
is supplied as input, the first 2 dimensions are considered to be
the spatial, and the 3rd dimension is propogated to the output
unchanged.

This command is not part of the interactive image processing system
and does not require it to be active.

\subsection{Method}
The value of a given polar bin is the sum of the values of the image
pixels which lie within it, divided by the number of such image pixels
and normalised by the axis units. e.g. if 10 image pixels lie in a
particular polar bin and each pixel has an area of 2 arcmins by 2 arcmins,
then the value of the polar bin will be - the sum of the pixel values,
divided by (10 * 2 * 2) and will be in units of 'cnts/square
arcminute'.

If a variance array is present in the input file, the output
variance of a polar bin is the mean of the variances of the image pixels
within that bin. If input variances don't exist then the output
variances are calculated from counting statistics.

NB: The algorithmn used relies on the fact that the image pixels are
square. If the pixels in the image are not square then the program
prints a warning message and continues executing.

By default azimuthal distributions are taken such that looking right
along the X axis is azimuth=0 and increase moving anti-clockwise.
The initial position to call azimuth zero can be specified on the
command line by AZSTART = 90, which means start from the North
position.

\subsection{Quality Checking}
BAD pixels are ignored in the polar binning process. The quality of
an output polar bin is GOOD if ANY good image pixels fall in that
bin and BAD if none do.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type       Description
 -------  ----  ----       -----------
 INP       1    UNIV       Name of input datafile.
          GLOBAL.BINDS (Default binned dataset).
 OUT       2    UNIV       Name of output datafile.
 XCENT     3    REAL       Central X position of slice.
 YCENT     4    REAL       Central Y position of slice.
 RBIN      5    REAL       Size of the radial bins in axis units.
 ABIN      6    REAL       Size of the azimuthal bins in degrees.
     Default = 360.0
 NORM      -    LOG        Output normalised to axis units
     Hidden default = Y
 AZSTART        REAL       Initial azimuth (degrees). Right is zero,
                           top is +90, left is 180 and South is -90.
     Hidden default = 0.0

\end{verbatim}\subsection{Examples}
To find a radial distribution of a 2-d data array in IMAGE.SDF
\begin{quote}\begin{verbatim}
 ICL> IPOLAR IMAGE
 Name of output file > POL2
 X Axis range: 1.133336 to -1.132224
 Y Axis range: -1.133336 to 1.132224
 XCENT-Centre of cut in horizontal axis > 0.1
 YCENT-Centre of cut in vertical axis > 0.1
 RADBIN-Radial binsize > 0.02
 AZBIN-Azimuthal binsize > 360.
 Ouput data will be in Counts / (degrees squared)
\end{verbatim}\end{quote}
To find an azimuthal distribution of a 2-d data array in IMAGE.SDF
starting from North.
\begin{quote}\begin{verbatim}
ICL> IPOLAR IMAGE POL2 0.1 0.1 2.26 10.0 AZSTART=90.0
\end{verbatim}\end{quote}
\section{IPOSIT}
This command has two basic modes of operation. In default mode it
sets the current position either by cursor or parameter, depending on
the current system mode. In keyboard mode the position may be
specified in a number of specified coordinate frames.

Alternatively, by use of command line switches it can be made to
manipulate lists of RA/DEC positions. The keyword 'ENTER' will
put it into entry mode, in which a list of positions may be
entered in one of three ways: positions may be entered at the
keyboard with '~', '-' or '\verb+\+' indicating continuation; a text
file of positions may be entered; or a PSS results file may be
given. Positions currently held in the system may be listed
using the 'SHOW' keyword, and a position may be selected from
the list to be the current position by using the 'SEL' keyword.

If entering RA/DEC see the section in 'User\_interface' which
explains the formats that may be used.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  ENTER      -      LOGICAL    Switch to entry mode
      Hidden default = N
  LIST       -      CHAR       List of positions
  APPEND     -      LOGICAL    Append to existing list
  SHOW       -      LOGICAL    Show current list
      Hidden default = N
  SEL        -      LOGICAL    Select position from list
      Hidden default = N
  NUM        -      INTEGER    Position number to select
  FRAME      1      INTEGER    Coordinate frame
                                1 = RA/DEC
       Default = 1              2 = Ecliptic
                                3 = Galactic
                                4 = axis coordinates
                                5 = pixel coordinates
  RA         2      CHAR       RA
  DEC        3      CHAR       DEC
  ELON       -      DOUBLE     Ecliptic longitude (dec. deg.).
  ELAT       -      DOUBLE     Ecliptic latitude    "    "
  L          -      DOUBLE     Galactic longitude   "    "
  B          -      DOUBLE     Galactic latitude    "    "
  X          -      REAL       X-coord (axis units).
  Y          -      REAL       Y-coord   "     "
  XPIX       -      REAL       X-pixel   "     "
  YPIX       -      REAL       Y-pixel   "     "

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IPOS 1 "12h45m35s" "-30d30m"   set position by RA/DEC

 > IPOS 4  X=45.0 Y=100.5         set position by x/y

 > IPOS                           set by cursor (CURS mode)

 > IPOS ENTER                     entry mode
    LIST> 22 20 50 -24 47 00 ~    positions entered
    LIST> 22 21 10 -24 37 00 ~    directly at keyboard
    LIST> 22 20 10 -24 42 00

 > IPOS ENTER LIST=pos.txt        enter positions from file

 > IPOS SEL NUM=2                 select position 2 as current pos.

 > IPOS SHOW                      list positions
\end{verbatim}\end{quote}
\section{IPSF}
Fits the PSF to a radial distribution produced by IRADIAL and displays
the result. The fit is a weighted least-squares - the reduced chi-
squared and equivalent normal z is output. The latter is not useful
unless the number of radial bins is > $\sim$ 10. The first time IPSF is
invoked with a particular image, the user is required to choose the
psf to use. The choice will be 'instrument-specific' or 'analytical'.
Once chosen, this will be used by default with subsequent invocations.
The value and error of the background may also be specified.

There is an option to choose a new PSF.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  NEW        -      LOGICAL    Choose a new PSF.
     Hidden default = NO
  PSF        1      CHAR       PSF type.
  MASK       2      CHAR       Mask type for analytical PSFs.
  AUX        3      CHAR       Auxilliary info. eg mask width.
  BGND       -      REAL       Value of background in data units per
                               data pixel.
     Default = !
  BGNDERR    -      REAL       Error on BGND value.
     Default = 0.0
  PMAX       -      REAL       Max value for centre of PSF.
     Hidden default = !
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IPSF \                      plot PSF using default
   ...
   ********** Fit of psf model to data **********

         Scaled psf flux     : 101.243
         Reduced chi-squared : 24.34644
         Equivalent normal z : 16.81098

 > IPSF                        plot PSF using current one

 > IPSF NEW ANAL GAUSSIAN 5.0  plot PSF using a Gaussian of FWHM 5.0

 > IPSF 0.16 0.4               plot PSF using current one, with a
                               background of 0.16+-0.4 per image pixel
\end{verbatim}\end{quote}
\section{IRADIAL}
Specifies a circular region of the image and plots an integrated
radial distribution in the next zone. The bin width of the
distribution is equal to the image pixel size, although this can
be reduced using the hidden parameter SAMPLE (eg. SAMPLE equal to
2 halves the radial bin size). The centre and radius of the circle
are specified either by cursor or parameter input depending on the
current mode (change mode with command IMODE).

The data units are normalised to be per pixel. To convert the
units of a saved output file to counts/square axis unit, do
the following :

\begin{quote}\begin{verbatim}
ICL> ISAVE1D Filename
ICL> HGET Filename.axis(1).data_array.scale VALUE (SCALE)
ICL> SC2 = SCALE * SCALE
ICL> ARITHMETIC filename (SC2) OPER=/ OUT=Ofilename
ICL> HMODIFY OFILENAME.UNITS "Counts/square whatever"
\end{verbatim}\end{quote}
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 XCENT       1      REAL       X-coord of centre (axis units).
 YCENT       2      REAL       Y-coord of centre    "    "
 RAD         3      REAL       Radius of circle (axis units).
 SAMPLE      4      INTEGER    Oversampling factor.
   Hidden default 1

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > IRADIAL SAMPLE=2           Radial profile, oversample by a
                              factor of two.
\end{verbatim}\end{quote}
\section{IREGION}
This allows a spatial region to be defined in a comprehensive and
flexible way. Input to define the region may be by cursor, by
keyboard or by ARD (Ascii Region Definition) text input. Regions
may be defined cumulatively and more complex shapes may be created
by the use of logical operations. The cumulative region is stored
internally as ARD text which may be exported to a file, subsequently
to be used as input to sorting or other applications. The ARD text
or logical mask derived from it is also passed internally to other
image processing commands eg. ISTATS, IEXCLUDE...

The operation of the command is controlled by the MODE parameter, the
value of which may be prefixed with logical operators. Responding
with 'HELP' will list the possible values of both mode and prefix
to the screen. Only the first 3 characters of the mode value is
required and it is not sensitive to case. The following modes are
available:


\begin{description}
\item[BOX]
rectangular region parallel to axes
\item[SLICE]
rectangular slice at any orientation
\item[POLYGON]
any polygon entered via cursor as a series of points
\item[CIRCLE]
circular region
\item[ANNULUS]
annular region
\item[ELLIPSE]
elliptical region
\item[CONTOUR]
select region within a specified contour ie. all
pixels above a given level
\item[XSPOKES]
area corresponding to ribs on ROSAT PSPC images
\item[WHOLE]
select whole image
\item[IMPORT]
import ARD text (see separate section on ARD input)
\item[EXPORT]
export the ARD text for the current region to a file
\item[SHOW]
show the outline of the current region(s) - this may be
a bit inaccurate if the image is very grainy
\item[LIST]
list the ARD text for the current region to the screen
\item[INVERT]
turn the current region inside out
\end{description}
The default operation is for a new region to be created and all
previous ones to be forgotten. To override this the following
prefixes may be added:


\begin{description}
\item[ADD]
add region to existing one
\item[AND]
select only the overlap with the existing region
\item[NOT or EXC]
take the region outside the one given
\item[ADDNOT]
add the area outside the given region to the existing one
\item[ANDNOT]
select the overlap between the area outside the specified
region and inside the existing one.
\end{description}
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   MODE      1      CHAR       Mode control switch
   SUBMODE   2      CHAR       Sub-mode control
   XC        -      REAL       Centre of shape
   YC        -      REAL          "    "   "
   RAD       -      REAL       Radius of circle
   IRAD      -      REAL       Inner radius of annulus
   ORAD      -      REAL       Outer radius of annulus
   XWID      -      REAL       Horiz. width of box
   YWID      -      REAL       Vert. width of box
   LENGTH    -      REAL       Length of slice
   WIDTH     -      REAL       Width of slice
   ANGLE     -      REAL       Angle of slice or ellipse
   MAJOR     -      REAL       Major axis of ellipse
   MINOR     -      REAL       Minor axis of ellipse
   LEV       -      REAL       Contour level
   TEXT      -      CHAR       ARD text or filename
   FILE      -      CHAR       Filename for ARD text output
   AUTO      -      LOGICAL    Generate area automatically from
                               attitude file in XSPOKES mode
   ROOTNAME  -      CHAR       calib file root for above
   ATTFILE   -      CHAR       attitude file for above
   EXTRA     -      LOGICAL    allow extra width in XSPOKES mode
\end{verbatim}\subsection{Examples}
Get statistics for background region...
\begin{quote}\begin{verbatim}
 > IREG ANN                - define annular region
 > ISTATS                  - get stats for that region
\end{verbatim}\end{quote}
Remove instrumental artifacts from image...
\begin{quote}\begin{verbatim}
 > IREG NOTCIRC            - select region outside given circle
 > IREG ADDANN             - add an annular region (mirror support?)
 > IREG ADDSLI             - add rectangular area (rib?)
 > IREG ADDSLI             - repeat for other ribs
 > IDISP                   - clear away construction lines
 > IREG SHO                - show outline of accumulated region
 > IREG EXP FILE=RIBS.ARD  - save ARD text to file
 > IEXC REG CURR           - set quality for this region
 > IREG WHO                - reselect whole image for further
                             processing
\end{verbatim}\end{quote}
Exclude identical region from another image...
\begin{quote}\begin{verbatim}
 > ILOAD new-image DISP \
 > IREG IMP TEXT=RIBS.ARD  - use ARD text saved earlier
 > IEXC REG CURR
\end{verbatim}\end{quote}
Do the above automatically
\begin{quote}\begin{verbatim}
 > IREG XSP AUTO
\end{verbatim}\end{quote}
Remove an extended source...
\begin{quote}\begin{verbatim}
 > IREG CIR                - select area of image containing source
 > IREG ANDCON LEV=x       - select pixels within this area greater
 > IEXC REG CURR           - than given level and remove
\end{verbatim}\end{quote}
\subsection{ARD input}
The syntax of ARD (ASCII Region Definition) text is as described in
SUN 183. It can be entered in one of two ways - as direct text or
as a file containing one or more lines of ARD description. When
entering text directly, multiple lines may be entered by terminating
the line with '~', '-' or '\verb+\+'. The description string may be
broken anywhere except in the middle of a number. Note that these
continuation characters are stripped out and are not needed in files
of multiple lines. When giving a filename, then the name by itself
should be sufficient, although it can be prefixed by the indirection
symbol '\verb+^+'.

\section{ISAVE}
Save the currently selected region of the image to file in its
current state of processing. It does not release the current
image and further processing may be done.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   OUT       1      UNIV       File to save image to,
              -> GLOBAL.BINDS (Default binned dataset)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ISAVE IMG3           save current image to file IMG3
 \end{verbatim}\end{quote}
\section{ISAVE1D}
Save current 1D data produced from image eg. radial profile.
The data are still retained after saving.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   OUT       1      UNIV       File to contain saved data.
              -> GLOBAL.BINDS (Default binned dataset)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> ISAVE1D RADIAL            save data to file RADIAL
\end{verbatim}\end{quote}
\section{ISCALE}
Set the scaling algorithm and optionally the outer scaling limits.
Data values which fall outside the limits are plotted in the extreme
colours of the colour table. The following scalings are available:

\begin{verbatim}
    LINear
    LOGarithmic
    SQRt
    HIStogram equalisation
    CYClic
 \end{verbatim}
The image may be displayed immediately with the new scaling by using
the DISP keyword.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   SCALING   1      CHAR       Scaling type
   MIN       2      REAL       Minimum scaling level
   MAX       3      REAL       Maximum scaling value
   CYCLE     -      INT        For cyclic scaling
   DISP      -      LOGICAL    Display image?
       Hidden default = NO
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ISCALE LIN 0.1 0.9      scale linearly between 0.1 and 0.9

 > ISCALE LOG DISP \       log scaling and redisplay

 > ISCALE CYC CYCLE=3 \    cycle through colour table 3 times
 \end{verbatim}\end{quote}
\section{ISEP}
Gives the angular separation between two points. The points
may be specified by cursor or parameter according to current
mode. If specifying by parameter, a number of coordinate
frames are available.

This command does not require the image processing system to
be active, in which case it defaults to keyboard mode and only
allows frames 1 to 3.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  FRAME      1      INTEGER    Coordinate frame
                                1 = RA/DEC
       Default = 1              2 = Ecliptic
                                3 = Galactic
                                4 = axis coordinates
                                5 = pixel coordinates
  RA1        2      CHAR       RA of first point
  DEC1       3      CHAR       DEC     "     "
  RA2        4      CHAR       RA of second point
  DEC2       5      CHAR       DEC     "     "
  ELON1      -      DOUBLE     Ecliptic longitude (dec. deg.)
  ELAT1      -      DOUBLE     Ecliptic latitude    "    "
  ELON2      -      DOUBLE     Ecliptic longitude (dec. deg.)
  ELAT2      -      DOUBLE     Ecliptic latitude    "    "
  L1         -      DOUBLE     Galactic longitude of first point.
  B1         -      DOUBLE     Galactic latitude  "    "     "
  L2         -      DOUBLE     Galactic longitude of second point.
  B2         -      DOUBLE     Galactic latitude  "     "      "
  X1         -      REAL       X-coord of first point.
  Y1         -      REAL       Y-coord "    "     "
  X2         -      REAL       X-coord of second point.
  Y2         -      REAL       Y-coord "     "     "
  XPIX1      -      REAL       X-pixel of first point.
  YPIX1      -      REAL       Y-pixel "    "     "
  XPIX2      -      REAL       X-pixel of second point.
  YPIX2      -      REAL       Y-pixel "     "     "

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> ISEP 1 "12h34m30s" "50d35m20s" "12h40m30s" "50d36m"
\end{verbatim}\end{quote}
\section{ISLICE}
Specifies an arbitrary slice through the image and displays the
projected profile in the next zone. The slice is specified by
its centre, extent, width and angle, either by cursor or parameter,
depending on the current mode (change with command IMODE).

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 XCENT       1      REAL       X-coord of centre (axis units).
 YCENT       2      REAL       Y-coord of centre   "     "
 ANGLE       3      REAL       Angle anti-clockwise from horiz.
 LENGTH      4      REAL       Full length of slice (axis units).
 WIDTH       5      REAL       Full width of slice    "     "

\end{verbatim}\section{ISTATS}
Displays basic statistics on the currentl region of the current
image or of a specified box or circle by using the CIRC or BOX
options. Selected statistics may also be extracted into
variables. Using the SUPPRESS option will suppress output to
screen.

\subsection{Shortforms}
IBOXSTATS = ISTATS BOX
ICIRCSTATS = ISTATS CIRC
\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   BOX       -      LOGICAL    specify box
      (Hidden default = N)
   CIRC      -      LOGICAL    specify circle
      (Hidden default = N)
   XC        -      REAL       centre of box or circle
   YC        -      REAL       if in keyboard mode
   XWID      -      REAL       box width
   YWID      -      REAL        "    "
   RAD       -      REAL       radius of circle
   MIN       1      REAL       minimum data value.
   MAX       2      REAL       maximum data value.
   MEAN      3      REAL       mean of data values.
   TOT       4      REAL       total of data values.
   NPIX      5      INTEGER    number of good pixels used.
   XCENT     6      REAL       x-coord of centroid.
   YCENT     7      REAL       y-coord of centroid.
   SUPPRESS  -      LOGICAL    suppress screen output.
          Hidden default = NO

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ISTATS TOT=(D_TOTAL)      extract total into variable D_TOTAL
 \end{verbatim}\end{quote}
\section{IBOXSTATS+}
A special invocation of ISTATS whereby a box is specified
by cursor of keyboard without changing the current region.

\section{ICIRCSTATS+}
A special invocation of ISTATS whereby a circle is specified
by cursor of keyboard without changing the current region.

\section{ISUB}
Subtract another image, or a scalar constant, from the current
image.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   INP       1      CHAR       Image or scalar constant

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ISUB 1.5           subtract 1.5 from all pixels

 > ISUB IMG2          subtract IMG2
 \end{verbatim}\end{quote}
\section{ISURFACE}
Draw the image data as a pseudo 3D surface.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
   ANGLE      1      REAL      Viewing angle above horizontal.
        Default = 45.0

\end{verbatim}\section{ITEXT}
Puts text onto the current image or plot at arbitrary position
and orientation defined either by cursor or parameter, depending
on the current mode. In keyboard mode the user is prompted for
the text start position in axis coordinates. To specify the
position in other coordinates (eg celestial), use IPOSIT to
set current position and then accept the default offered with
ITEXT.

In cursor mode pressing the centre/right mouse button or \verb+<RETURN>+
will accept current position and give horizontal text when
requesting position and orientation respectively.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  TEXT       1      CHAR       Text string.
  XPOS       2      REAL       X-coord of start of string.
  YPOS       3      REAL       Y-coord of start of string.
  ANGLE      4      REAL       Angle in deg. anti-clockwise
                               from horiz.
       Default = 0.0

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> ITEXT "Some text"                  cursor mode

> ITEXT "Some text" 25.5 56.7 45     keyboard mode
\end{verbatim}\end{quote}
\section{ITITLE}
Change the title at the top of the image or plot displayed in the
current zone. This title will become the title of the dataset
(ie the TITLE component - not the name) if the data are subsequently
saved to file.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 TITLE       1      CHAR       Text of new title

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 > ITITLE "New title"
 \end{verbatim}\end{quote}
\section{IUNDO}
Discard the last process performed on the image and go back to
previous state. This only applies to processes which change
the image itself, not to processes which produce 1D data
products.

\section{IWHOLE}
Reselects the whole image, ie. cancels any previous invocation of
IBOX or ICIRCLE.

\section{IXCONV}
Perform a 2D convolution of a pair of images using FFTs. If both
images have the same dimensions then the user has a choice of
cyclic or non-cyclic convolution - non-cyclic is the only option
for datasets of different shape. In cyclic convolution the output
image has the same dimensions as the inputs, in non-cyclic the
dimensions are the sum of the dimensions of the inputs.

\subsection{Method}
\begin{itemize}
\item Move the first input into the centre of the real part of an array X
and remove its average. Zero the imaginary part.
\item Move the second input into the centre of the real part of an array Y
and remove its average. Zero the imaginary part.
\item Find the FFT of X and Y, FX and FY
\item Compute the FFT of the convolution of FX and FY, FC
\item Untransform FC to find the convolution C
\item Restore DC level if required and rotate so that zero lag is in the
centre of the output
\end{itemize}
\subsection{Deficiencies}
Does not take account of data quality or variance in either input.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  INP1       1      CHAR       First image in convolution
           -GLOBAL.BINDS (Default binned dataset)
  INP2       2      CHAR       Second image in convolution
  OUT        3      CHAR       Name of output convolution image
           ->GLOBAL.BINDS (Default binned dataset)
  CYCLIC     -      LOGICAL    Perform cyclic convolution?
    Default = YES
  DC_RESTORE -      LOGICAL    Restore DC level in output image?
    Default = YES

\end{verbatim}\subsection{Example}
Example of two images of the same dimensions,
\begin{quote}\begin{verbatim}
 csh> ixconv img1 img2
 IXCONV Version 1.8-0
 The dimensions of both inputs are 20 by 20
 CYCLIC - Do a cyclic deconvolution? /YES/ >
 OUT - Output dataset > corr
 Transforming first input...
 Transforming second input...
 Cross-correlating...
 Untransforming...
 The average of dataset 1 was 0.011572
 The average of dataset 2 was 0.01182
 DC_RESTORE - Add back in DC level? /YES/ >
 \end{verbatim}\end{quote}
Example of two images of different dimensions,
\begin{quote}\begin{verbatim}
 csh> ixconv img1 img2_bit
 IXCONV Version 1.8-0
 Dimensions of dataset 1 are 20 by 20
 Dimensions of dataset 2 are 20 by 13
 Dimensions are not same, so convolution will be non-cyclic
 OUT - Output dataset > icor
 Transforming first input...
 Transforming second input...
 Cross-correlating...
 Untransforming...
 The average of dataset 1 was 0.003507
 The average of dataset 2 was 0.003047
 DC_RESTORE - Add back in DC level? /YES/ > n
 \end{verbatim}\end{quote}
\section{IZED}
This command can only be used when the image being displayed
has been loaded from a cube. It allows a circle to be defined,
eg around a source, and then bins in the "z" direction collecting
only events within the "cylinder" defined by the circle. The
resulting 1D data are displayed and may be saved to a file.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
    X        1      REAL       x-pos of circle centre (keyboard mode)
    Y        2      REAL       y-pos
    RAD      3      REAL       radius of circle in axis coords

\end{verbatim}\section{IZOOM*}
A macro command which selects a box, clears the current zone and
redisplays the zoomed-in section of the image.
\section{IUNZOOM*}
A macro command which selects the whole image, clears the current
zone and redisplays the full image.

\chapter{Statistical analysis commands}
\section{BINSUM}
Integrates values in a dataset, which can be either structured
or primitive.

If the dimensionality of the dataset is greater than 1, the
dataset is treated as if it is a series of 1D strips having
a length the same as the first dimension of the dataset.

Use BINSUM after FREQUENCY to produce a cumulative
frequency distribution. The REVERSE keyword permits data to
be summed back to front - ie. from high "X" to low "X".

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type      Description
 -------  ----   ----      -----------
 INP         1    UNIV      The input dataset (Primitive/Structured).
            -GLOBAL.BINDS (Default binned dataset)
 OUT         2    UNIV      Dataset to receive output. Becomes the
                            default analysis object.
            ->GLOBAL.BINDS (Default binned dataset)
 REVERSE     -    LOGICAL   Perform summation from back to front?
        Hidden default =  NO

\end{verbatim}\section{COMPARE}
Makes a chi-squared comparison of a model and dataset, and
computes the renormalization factor which brings the model into
closest agreement with the data.

If the model and data both contain error components then these
are added in quadrature - hence COMPARE can be used to compare
two datasets for similarity.

If the data contain no error component then the user is given the
options of assuming Poisson data errors (i.e. data error = sqrt
(data value)), or unit data errors (i.e. all 1.0) (answer Y to
POISS\_ERR for the former).

\subsection{Input output data}
Input: binned dataset containing data
binned dataset containing model
Output: alphanumeric display device
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type        Description
 -------  ----   ----        -----------
 DATA       1    UNIV        Dataset containing data values
           GLOBAL.FITDAT (Default fit data)
 MODEL      2    UNIV        Dataset containing model values
           GLOBAL.FITMOD (Default fit model)
 POISSON    -    LOGICAL     Poisson data errors to be assumed?
        Default = YES

\end{verbatim}\subsection{Method}
Chi-squared is calculated as Sum[(data-model)**2/variance], where
variance is the variance of the data, unless the model array also
has a VARIANCE component, in which case it is the sum of the
model and data variances. i.e the program can be used to compare
two datasets for compatibility. If the data have no error
component then either Poisson or unit errors may be The number of
degrees of freedom is taken to be the number of data points (i.e.
model is assumed to be independent of data). Bad quality data
points are excluded from the statistic, but the model array is
assumed to be 100\% good.

\section{FREQUENCY}
Obtains the relative frequency of occurrence of items in a data
object.

Items are collected into regularly spaced output bins whose size
is specified by the user. The first bin starts at the minimum of
the data in the input object so its centre is at
$min + 0.5 binsize$ and so on.

The normalisation option, when used, makes the sum (not a
numerical integral) of the frequency bins equal to unity.

Data quality is taken into account if present in the input data
object: items in the input whose quality is not perfect are
ignored.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type      Description
 -------  ----   ----      -----------
 INP         1    UNIV      The input dataset (Primitive/Structured).
           - GLOBAL.BINDS (Default binned dataset)
 REG         -    LOGICAL   Are output bins to be regularly
                            spaced?
    Hidden default = YES
 OUT         2    UNIV      Dataset to receive output. Becomes the
                            default analysis object.
           -> GLOBAL.BINDS (Default binned dataset)
 SPACING     3    REAL      The bin spacing (only used if
                            REG is YES).
 BOUNDARIES  4    CHAR      Bin boundaries (only used if REG is NO).
 NORM        -    LOGICAL   If specified then the "area" under the
                            curve will be normalised to unity.
    Default = !

\end{verbatim}\subsection{Irregular Bins}
The specification for irregular bins is by specifying the
bounds. The full syntax is described under 5\_User\_Interface
but the most useful form is for example,
\begin{verbatim}
     1:10:20:30:40:100:500:1000
\end{verbatim}
Which would specify 7 bins with centres 5.5, 15, 25,35,70,300 and 750.

\subsection{Examples}
An example producing regular frequency bins :
\begin{quote}\begin{verbatim}
> frequency mydata
FREQUENCY Version 1.2-1
Regular bin sizes can up to 67.72251 (sigma)
The data range is -2.894102 to 64.82841
REG - Are the output bins to be regularly spaced /YES/ >
SPACING - Enter the bin spacing > 1.3
This will give 52 bins
NORM - Normalise (Y/N) /YES/ >
OUT - Dataset to receive distribution > F_REG
\end{verbatim}\end{quote}
The same file binning into 9 irregular bins :
\begin{quote}\begin{verbatim}
> frequency mydata
FREQUENCY Version 1.2-1
Regular bin sizes can up to 67.72251 (sigma)
The data range is -2.894102 to 64.82841
BOUNDARIES - Enter the bin boundaries > ?
A sequence of bounds is required, eg. 1:10:20:40:55:60:65
BOUNDARIES - Enter the bin boundaries > *:5:10:20:30:35:40:45:50:*
This will give 9 bins
NOTE - receptor bins assumed contiguous
REG - Are the output bins to be regularly spaced /YES/ > N
NORM - Normalise (Y/N) /YES/ >
OUT - Dataset to receive distribution > F_IRREG
\end{verbatim}\end{quote}
\section{KSTAT}
Calculates Kendall's K statistic for a 1D binned dataset between
the axis and data components, or between 2 primitive 1 dimensional
objects.

The K statistic is a distribution-free test for correlation - see
Ponman, {\em MNRAS, 201, p769, 1982}.

In the absence of any correlation the K statistic is
approximately unit normally distributed (so long as there are at
least 10 data values), so a value K = 3.0 constitutes three sigma
evidence for positive correlation. Negative values imply a
negative correlation.

Bad quality data are excluded from the statistic. Variance is not
used.

\subsection{Input output data}
Input: 1D binned dataset (test performed between data and
axis values)
Or
Input: Primitive 1 dimensional object
Input2: Primitive 1 dimensional object of same length
Output: Terminal
\subsection{Ties}
The K statistic is an "order statistic" - it is based on comparing
the order of the values in the two arrays - and it needs correction
if there are many identical values in either array. This is achieved
by counting the number of "tied pairs", which can be larger than the
number of array elements. For example the 10 element array: 1 2 2 3
7 7 7 7 7 9, contains a total of 11 ties (one tie in 2, and ten in 7).

KSTAT corrects for such ties automatically, and informs the user of
the number of ties in each input array. The correction factor applied
is
\begin{verbatim}
N_p/sqrt((N_p-N_t1)*(N_p-N_t2))
\end{verbatim}
where N\_p is the total number of pairs ( = 0.5*N*(N-1), where
$N$ is the array length) and N\_t1 and N\_t2
are the number of tied pairs in each array. So long as the number of ties
is substantially smaller than the total number of pairs this correction
will be small (ie. near unity).

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type        Description
 -------  ----   ----        -----------
 INP        1    UNIV        Input data object (1D).
            GLOBAL.BINDS (Default binned dataset)
 INP2       2    UNIV        Second input data object.

\end{verbatim}\section{SCATTERGRAM}
SCATTERGRAM provides, as the name suggests, a plot of one
set of data values against another. Two input objects are
required, either of which may be structured datasets or
primitive data objects. Each input object must have the
same number of values (but not necessarily the same
dimensionality).

Up to 10000 items may be handled. If the input data objects
have more than this, a subset can be selected. The spacing
of items in the input dataset is also selectable : by
default, 1 is used, but any positive value is allowed.

Error data is propogated if present.

NOTE : This routine does not actually plot the output,
but merely prepares a file for plotting with the
graphics commands.

\subsection{Input output data}
Input: 1D to 7D binned dataset or primitive data object
Output: 1D binned dataset
\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 INP1        1   CHARACTER  First input dataset
 INP2        2   CHARACTER  Second input dataset
 SUBSET      3   INTEGER    2 or 3 numbers indicating which
                            input items are to be used.
 OUT         4   CHARACTER  Output dataset
   -> GLOBAL.BINDS (the current binned dataset)
 MARKER      -   LOGICAL    Set output graphics style to
                            markers.
   Hidden default Y

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
    > SCAT
    First input dataset > X.X_RAW
    Second input dataset > X.Y_RAW
    There are 65536 items
    Subset and spacing of items to be used /1,10000,1/ > <RETURN>
    Output dataset > SCATTER1

    > SCATT
    First input dataset > 1,2,3,4,5
    Second input dataset > IMAGE1.AXIS1_DATA
    There are 5 items
    Subset and spacing of items to be used /1,5,1/ > <RETURN>
    Output dataset > SCATTER2

    > SCATTER
    First input dataset > IMAGE1.DATA_ARRAY
    Second input dataset > IMAGE2.DATA_ARRAY
    There are 16384 items
    Subset and spacing of items to be used /1,10000,1/ > 1,16384,2
    Output dataset > SCATTER3
\end{verbatim}\end{quote}
\section{STATISTIX}
Calculates the mean, sum, standard deviation and the maximum and
minimum values of a binned dataset or primitive data object. The
dataset is vectorized, so may have any number of dimensions.

If variance is used then reduced chi-squared and equvalent normal
Z values are calculated, and the mean is a weighted one.

STATISTIX can be made to find the kurtosis and skewness in the
dataset by specifying the SIMPLE keyword false.

By using the LOOP parameter, values deviating from the current
mean by more than n sigma, may be ignored, with the statistics
being recalculated for the reduced sample. The ignored values
may be displayed at the terminal by using the DISP keyword.

When used with a binned dataset VARIANCE and QUALITY are used by
default if possible. This may be prevented by use of the WEIGHT
and QUALITY keywords.

\subsection{Input output data}
Input: Binned dataset or primitive data object. Note that when a
slice is supplied, the pixel positions of minimum and
maximum value refer to positions within the slice, and not
the array from which the slice was drawn.
Output: Alphanumeric display device
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Input data object (Primitive/Structured).
           - GLOBAL.BINDS (Default binned dataset)
 DEV       2    CHAR        text output device. Can take values of
                            (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                            or (N)EWFILE, otherwise is interpreted
                            as a filename.
   Hidden default 'CONSOLE'
 SIMPLE    -    LOGICAL     Only find simple statistics, ie. omit
                            kurtosis and skewness.
   Hidden default = YES
 WEIGHT    -    LOGICAL     Make use of VARIANCE information?
   Hidden default = YES
 QUALITY   -    LOGICAL     Make use of QUALITY information?
   Hidden default = YES
 LOOP      -    LOGICAL     Loop over data again allowing
                            recalculation of the statistics?
   Hidden default = NO
 DISPLAY   -    LOGICAL     Display ignored data values?
   Hidden default = YES
 SIGMA     -    DOUBLE      Critical number of sigma from mean

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
 ICL> STATISTIX TTT NOSIMPLE

   Statistics of DISK$WORK1:[ASTERIX88]TTT.SDF;1

             400 points excluded by bad data quality

   ***************************************************************
   *                                                             *
   *       16384 data points.          15984 points used.        *
   *                                                             *
   *  Sum of valid data points =    9964.946                     *
   *  Weighted mean =   0.6241636     +/-   6.1838025E-03        *
   *                                                             *
   *  Standard deviation =   0.3831711     +/-   2.1430636E-03   *
   *                                                             *
   *  Coefficient of skewness =    29.56415                      *
   *  Coefficient of kurtosis =    874.7267                      *
   *                                                             *
   *  Minimum value =    0.5513638  @ [12,100]                   *
   *  Maximum value =     12.00000  @ [123,4]                    *
   *                                                             *
   *  Goodness of fit of the mean value to the data:             *
   *     Reduced chi squared =   0.2402088                       *
   *     Equivalent normal Z =   -91.16045                       *
   *                                                             *
   ***************************************************************
\end{verbatim}\end{quote}
\chapter{Instrument interface commands}
An ASTERIX instrument interface consists of a set of commands to
manipulate data in an instrument dependent manner. These programs
do not have any application for general data processing.

\section{EXOSAT}
The old EXOSAT interface has not been ported to UNIX, and it is not
anticipated that it will be.

\section{SLOTMRG}
A routine to merge two ASCII time lists.

The data sorting programs, WFCSORT and XSORT are capable of using a
file containing a series of ON-OFF times in MJD format to sort the data.
SLOTMRG allows two such files to be merged to produce a file containing
the overlapping time regions. A common use would be in simultaneous
work with Rosat WFC and XRT data. The XRT routine, PREPXRT, produces a
file, Rootname.ONOFF, which contains the ON-OFF times as MJDs for a
particular XRT observation. The WFC interface produces a similar file
for the WFC observation. These files can be merged using SLOTMRG to
produce a single file containing the times when both instruments were
collecting data. This resultant file may than be used in both of the
sort routines.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 SFILE1      1      CHAR       Name of an ASCII time window file
 SFILE2      2      CHAR       Name of another ASCII time window file
 SOFILE      3      CHAR       Name of merged file

\end{verbatim}\subsection{Example}
To merge the time window files from the XRT and WFC interfaces:
\begin{quote}\begin{verbatim}
> SLOTMRG CYGX1.ONOFF CYGX1_WFC.TIMES CYGX1_MERGE.TIMES
\end{verbatim}\end{quote}
\section{WFC ROSAT}
A package of ROSAT wide field camera specific procedures. There are two
sets of applications to handle the survey and pointed phases of the
mission. The applications beginning with S2 manipulate S2 catalogues,
those beginning with WFC are used to process pointed mode data.

Many of the WFC* routines are contained within a separate package, WFCSORT.
If installed, this may be started up by typing "WFCSTART" from the
command line.

\subsection{Getting started}
Raw WFC data customarily arrives in backup format on a
magnetic tape or Exabyte. The initial stages of processing this data are
described in the Starlink document, SUN62, which should be available from
your system manager.

\subsection{S2 Catalogues}
The S2 format catalogue is a binary file containing data about sources
found in the S2 pipeline processing. It is accompanied by a description
file which enables it to be read by SCAR, the StarLink Catalogue Access
and Reporting System.

However, for reasons of efficiency, much of the data is compressed and
is not immediately understandable using the SCAR reporting program,
CAR\_REPORT. For reporting purposes the S2CATLIST application should be
used.

In all other respects S2 catalogues can be manipulated by SCAR
commands.

\subsection{S2ADDPFLAG}
Adds a processing flag to an S2 format catalogue. Valid flags are
\begin{verbatim}
 CATALOGUING  -  Indicates catalogue cross-correlation complete
 OPTICAL      -  Indicates optical identification done
 XRT          -  Indicates XRT cross-correlation done
\end{verbatim}
\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      CHAR       Name of input S2 catalogue
      -GLOBAL.S2CAT ( Default S2 catalogue )
 FLAG        2      CHAR       Name of processing flag to
                               add.
 OUT         3      CHAR       Name of output S2 catalogue
      ->GLOBAL.S2CAT ( Default S2 catalogue )
\end{verbatim}\subsection{S2CATCLEAN}
Copies the input catalogue to the output removing those sources
which the specified quality flags set.

In addition, the XCOR parameter can be set which resets all
catalogue correlation data.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      CHAR       Name of first S2 catalogue
      -GLOBAL.S2CAT ( Default S2 catalogue )
 OUT         2      CHAR       Name of second S2 catalogue
      ->GLOBAL.S2CAT ( Default S2 catalogue )
 DELETE      -      LOGICAL    Remove sources with DELETE flag set
 DOUBTFUL    -      LOGICAL    Remove sources with DOUBTFUL flag set
 SPURIOUS    -      LOGICAL    Remove sources with SPURIOUS flag set
 USER1       -      LOGICAL    Remove sources with USER1 flag set
  ...       ...       ...               ...        ...
 USER8       -      LOGICAL    Remove sources with USER8 flag set
 XCOR        -      LOGICAL    Reset all catalogue correlation data

 All logicals have a default of N.

\end{verbatim}\subsection{S2CATLIST}
Dumps the contents of an S2 format catalogue to an ascii device in
one of two formats - a few lines per source, or one page.

Packed catalogue correlation data is unpacked to provide names,
positions and miss-distances of all correlations.
\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      CHAR       Name of input S2 catalogue
      -GLOBAL.S2CAT ( Default S2 catalogue )
 DEV         2      CHAR       text output device. Can take values of
                               (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                               or (N)EWFILE, otherwise is interpreted
                               as a filename.
      Default 'TERMINAL'
 HEADER      -      LOGICAL    Display an informative
                               header?
      Hidden default YES
 TAB         -      LOGICAL    Display data in compact
                               format ( 2 lines per source )
      Hidden default NO
 FULLCAT     -      LOGICAL    Dump all correlated catalogue data. If
                               specified false, only the number of
                               matches into each catalogue is output.
      Hidden default YES
\end{verbatim}\subsubsection{Shortform}
S2CATTAB = SCATLIST TAB
\subsection{S2CATMERGE}
Merges results of two S2 catalogues. New data is created -
objects which have a match are passed through a test which
compares their signal-to-noise. The rejected object is
omitted from the output catalogue. The better object is
simply copied to the output.

By specifying CREREJ true, rejected objects can be output
to their own catalogue.

\subsubsection{Algorithm}
The following criterion is used to decide whether two sources
are the same.
great\_circle < max( MINSEP, RMULT*(err1+err2) )
where 'great\_circle' is the angular separation of the sources,
MINSEP is the estimate of the systematic error in the posi-
tions of the sources, RMULT is a multiplicative fudge factor,
and err are the positional errors of the sources.

If two sources satisfy this criterion, then one of them is
chosen for the output catalogue ( the other may be output
to a rejects catalogue if that option is selected ).

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP1        1      CHAR       Name of first S2 catalogue
      -GLOBAL.S2CAT ( Default S2 catalogue )
 INP2        2      CHAR       Name of second S2 catalogue
 MINSEP      -      REAL       Estimate of systematic error
                               in source positions
 RMULT       -      REAL       Multiplicative factor to
                               operate on pos. errors. See
                               algorithm for more details.
     Hidden default 1.0
 CREREJ      -      LOGICAL    Create a rejects catalogue?
     Hidden default N
 OUT         3      CHAR       Output merged catalogue
      ->GLOBAL.S2CAT ( Default S2 catalogue )
 REJECTS     4      CHAR       Name of catalogue to contain rejects
                               from match. Only created if CREREJ
                               is specified and there are objects
                               to put in it.

\end{verbatim}\subsection{S2CATQUAL}
Interactive editor for S2 catalogue quality information. The input
catalogue is read into memory where the user can inspect or alter
the contents of the quality word for each S2 source. On exit, an
output catalogue may be specified.

\subsubsection{Commands}
S2CATQUAL is controlled by the following 10 commands
? : Brief list of commands
EXIT or QUIT : Leave S2CATQUAL and output catalogue
HELP : Bring up this help page
LIST  : List current record selection
QHELP : List valid quality flag names
RECS  : Select records by number
RESET  : Reset bits of selected records
SET  : Set bits of selected records
SIF  : Select records with specified bits set
SNIF  : Select records with specified bits not set
A  is a list of record numbers. Eg. 1,2,3 or 1:4. Use
ALL to specify all records.
A  is a list of one or more quality flag names, separated by
commas
\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      CHAR       Input S2 catalogue
      -GLOBAL.S2CAT ( Default S2 catalogue )
 COM         -      CHAR       S2CATQUAL command
 OUT         -      CHAR       Output S2 catalogue
      ->GLOBAL.S2CAT ( Default S2 catalogue )
\end{verbatim}\subsubsection{Example}
\begin{quote}\begin{verbatim}
> S2CATQUAL ANY48

S2CATQUAL Version 1.3-0
COM - Command > help

Commands available :

  ?                : Brief list of commands
  EXIT or QUIT     : Leave S2CATQUAL and output catalogue
  HELP             : Bring up this help page
  LIST   : List current record selection
  QHELP            : List valid quality flag names
  RECS   : Select records by number
  RESET    : Reset bits of selected records
  SET      : Set bits of selected records
  SIF      : Select records with specified bits set
  SNIF     : Select records with specified bits not set

A  is a list of record numbers. Eg. 1,2,3 or 1:4. Use
            ALL to specify all records.
A  is a list of one or more quality flag names, separated by
            commas

COM - Command > LIST 1:5

  N  Name                Map    Sig1  Sig2       Spr Dbt DEL  User1..8

  1  WFC180023+383633  006,089   7.8   1.7        -   -   -   --------
  2  WFC184508+382301  002,088   9.6   6.9        -   -   -   --------
  3  WFC171131+264656  012,098   4.3  -0.4        -   -   -   --------
  4  WFC160155+183341  053,083   6.5   3.7        -   -   -   --------
  5  WFC145746+142248  050,078   4.8  -1.7        -   -   -   --------

COM - Command > SET USER4
 5 records modified
COM - Command > LIST

  N  Name                Map    Sig1  Sig2       Spr Dbt DEL  User1..8

  1  WFC180023+383633  006,089   7.8   1.7        -   -   -   ---Y----
  2  WFC184508+382301  002,088   9.6   6.9        -   -   -   ---Y----
  3  WFC171131+264656  012,098   4.3  -0.4        -   -   -   ---Y----
  4  WFC160155+183341  053,083   6.5   3.7        -   -   -   ---Y----
  5  WFC145746+142248  050,078   4.8  -1.7        -   -   -   ---Y----

COM - Command > REC 3
COM - Command > SET SPURIOUS
 One records modified
COM - Command > LIST 1:5

  N  Name                Map    Sig1  Sig2       Spr Dbt DEL  User1..8

  1  WFC180023+383633  006,089   7.8   1.7        -   -   -   ---Y----
  2  WFC184508+382301  002,088   9.6   6.9        -   -   -   ---Y----
  3  WFC171131+264656  012,098   4.3  -0.4        Y   -   -   ---Y----
  4  WFC160155+183341  053,083   6.5   3.7        -   -   -   ---Y----
  5  WFC145746+142248  050,078   4.8  -1.7        -   -   -   ---Y----

COM - Command > REC 2
COM - Command > RESET USER4
 One records modified
COM - Command > LIST 1:5

  N  Name                Map    Sig1  Sig2       Spr Dbt DEL  User1..8

  1  WFC180023+383633  006,089   7.8   1.7        -   -   -   ---Y----
  2  WFC184508+382301  002,088   9.6   6.9        -   -   -   --------
  3  WFC171131+264656  012,098   4.3  -0.4        Y   -   -   ---Y----
  4  WFC160155+183341  053,083   6.5   3.7        -   -   -   ---Y----
  5  WFC145746+142248  050,078   4.8  -1.7        -   -   -   ---Y----

COM - Command > EXIT
OUT - Output catalogue (! for none) > TEST
\end{verbatim}\end{quote}
\subsection{S2GIDECODE}
Unpacks a coded cross-correlation information inside an S2
format catalogue into a SCAR global index.

The user is presented with a table of the catalogues coded
in the input S2 catalogue. A list of those catalogues to be
unpacked is then entered followed by an output file name.

By entering 999 for the catalogue numbers, all cross reference
data will be extracted.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      CHAR       Name of input S2 catalogue
      -GLOBAL.S2CAT ( Default S2 catalogue )
 CATNOS      2      INTEGER[]  List of numbers of catalogues
                               to be extracted. Enter 999
                               to select all cross references
 GINDEX      3      CHAR       The output global index name
\end{verbatim}\subsection{S2GIENCODE}
Encodes a SCAR global index into an S2 format catalogue.

A data compression program, which can achieve compression
ratios of several hundred by making (valid) assumptions
about the nature of the catalogues with which it is dealing.

If the global index has so many records per S2 source that
S2GIENCODE runs out of space, the user is informed of the
fact. The best solution here is to tighten the criterion
by which the global index was created.

The global index can be recovered later using S2GIDECODE.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      CHAR       Name of input S2 catalogue
      -GLOBAL.S2CAT ( Default binned dataset )
 GINDEX      2      CHAR       The global index to encode
 RADIUS      -      REAL       The cross-correlation radius
                               used to create the global
                               index. Not compulsory.
 OUT         3      CHAR       Output S2 catalogue with
                               encoded index included.
      ->GLOBAL.S2CAT ( Default S2 catalogue )

\end{verbatim}\subsection{S2CATTAB+}
Shortform of S2CATLIST - output in TAB mode

\subsection{WFCAUTO*}
An ICL procedure which works through all the RWODs located on a
requested directory producing hard copy images for each separate filter
configuration.

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> WFCAUTO (BASE) (RWOD) (PRINT) (IERR)
\end{verbatim}\end{quote}
The online parameters are
\begin{verbatim}
   BASE    CHAR     The directory containing the RWODs
   RWOD    CHAR     The RWOD number to process (this is a six digit
                    number unique to an observation)
   PRINT   LOGICAL  Requests hardcopy of the images produced
   IERR    INT      Returned Error status
\end{verbatim}
\subsection{WFCBOP*}
An ICL procedure to aid in optimising the background rate (LEVS)
threshold selection when producing WFC pointed images. WFCBOP computes
(in a field-averaged sense) the optimum LEVS threshold (optimum for
maximising S:N for a point source, assuming Gaussian statistics).

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> LEVS=10000.0
ICL> WFCBOP (ROOT) (FILT) (LEVS)
\end{verbatim}\end{quote}
The online parameters are
\begin{verbatim}
   ROOT    CHAR     The root name of the data files, eg. 170096
   FILT    CHAR     The required WFC filter, eg. S1A, S2B, P1, P2
   LEVS    REAL     Contains the computed optimum LEVS limit (ct/s)
\end{verbatim}
The procedure uses WFCSORT and ASTERIX routines. The procedure creates
several temporary, working files, named BOPTEMP*.SDF, on the current
default directory. Typical disk space occupied is 1k blocks. These
files are deleted on successful completion.

\subsection{WFCCOUNTS*}
An ICL procedure which works interactively on the currently displayed
image and produces statistics at a requested source location. The
backgrouns subtracted source counts, significance of detection above the
background and the PSF correction for the user selected source box are
displayed to the user.

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> wfccounts
\end{verbatim}\end{quote}
\subsection{WFCDBM}
Maintains a database of all RWODs (ROSAT WFC observation datasets) held
at an institute. The database must first be initialised using the
(C)reate command, thereafter the database may be updated with
information for an RWOD after first copying the latter from exabyte to
disk (see WFCDISK in this help).

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 CMND        -      CHAR       May be one of :
                               (C)reate an empty index file
                               (U)pdate with a new RWOD
                               (S)earch on a set of criteria
                               (H)ead to display the event file header
                               (G)roup a number of associated
                               observation slots
 EVEF        -      CHAR       The name of an input event file (default
                               extension is .EVE)
 LISF        -      CHAR       The file to hold listing o/p (default is
                               SYS$OUTPUT)
 INDF        -      CHAR       The index file which holds the RWOD
                               database.
 SMJD        -      REAL*8     The start MJD of a search window
 EMJD        -      REAL*8     The end MJD of a search window
 RAL         -      REAL       The RA lower limit of a search (deg)
 RAH         -      REAL       The RA upper limit of a search (deg)
 DECL        -      REAL       The Dec lower limit of a search (deg)
 DECH        -      REAL       The Dec upper limit of a search (deg)
 PI          -      CHAR       Any unique string which identifies the
                               Principal Investigator in a search
 TARGET      -      CHAR       Any unique string which identifies the
                               Target in a search
 SELECTn     -      CHAR       The search is performed on a series of
                               selection criteria in response to the
                               SELECTn prompt. The valid requests are
                               (M)JD        Select an MJD window
                               (R)A         Select an RA window
                               (D)EC        Select a DEC window
                               (P)I         Select by Principal
                                            Investigator
                               (T)ARGET     Select by Target name
                               (E)XIT       Perform the search
 GROUP       -      INT        The sequence number of a group of slots
                               to be associated.
 SLOTS       -      CHAR       The observation slots which have been
                               associated by the group command.

\end{verbatim}\subsection{WFCDISK*}
The procedure copies the files from a user specified RWOD save set
to the users target directory.

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> WFCDISK (SAVESET) (TARGET)
\end{verbatim}\end{quote}
The online parameters are
SAVESET CHAR This specifier must include both device and file
names (eg MUB0:123456.BCK). Each RWOD contains a
copy of the WFC calibration file and, after
recovery to disk, a check is made of the creation
date of this file. If the file is more recent than
the default version, referenced by the
CAL\_WFC\_MASTER logical, then this is replaced.
TARGET CHAR The target directory to which to copy the RWOD
\subsection{WFCEXP}
This program takes as input an ASTERIX binned dataset (created by the
WFCSORT program) and converts the counts in the data\_array component
into flux normalised to the on axis response of the WFC. (ie it
produces a flux which would have obtained had each element of
data\_array been located on the optical axis of the instrument. The flux
calculation takes into account the following instrument dependencies:
\begin{itemize}
\item Instrument dead time effects (the event rates needed for the
calculation are stored in the .HKR file which is part
of the standard observation dataset).
\item Instrument vignetting effects. The Instrument pointing is derived
from the .ATT file. The vignetting at points within the
field is derived from the filter and detector information stored
in the WFC Calibration File (with the logical name CAL\_WFC\_MASTER).
\item Optionally the program may correct the fluxes in a dataset for the
degradation of the detector efficiency. The time history of the
efficiency is stored in the calibration file and is used to
determine an interpolated value at the epoch appropriate to the
dataset. The user is warned if the dataset epoch is beyond the
end of the current time history in the calibration file. In this
event the latest value for detector efficiency is used.
\item Optionally the program may correct the fluxes in a one dimensional
dataset for the fraction of the point response function which fell
outside the source centred circle from which events were
originally accepted by the WFCSORT program.
\end{itemize}
\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         -      CHAR       The name of an input ASTERIX dataset. The
                               dataset may be either a time series or
                               an image.
 OUT         -      CHAR       The name of an output ASTERIX dataset.
                               The dataset is cloned from the input and
                               then the raw count array is replaced by
                               an exposure corrected count/sec array.
 EFFCOR      -      LOGICAL    Requests correction of fluxes for the
                               degradation of detector efficiency
                               post-launch.
 PSFCOR      -      LOGICAL    Requests correction of fluxes
                               for the point spread fraction (use on
                               spectra or time-series datasets only)
\end{verbatim}\subsection{WFCHK}
Extracts the counters present in a reduced housekeeping rates file
.HKR together with the pointing information present in the aspect
file .ATT into a series of arrays in an HDS file.
The following arrays are stored at the top level of the o/p HDS file

\begin{description}
\item[TEVS]
Total events output by the detector preamplifiers
\item[VEVS]
Valid events, i.e. TEVS within pulse height limits
\item[LEVS]
Limited events, i.e. VEVS within the electronic f.o.v.
\item[AEVS]
Accepted events, i.e LEVS entered into telemetry queue.
\item[FEVS]
Final events, i.e. AEVS telemetered to ground
\item[RA]
Pointing direction RA (degrees)
\item[DEC]
Pointing direction DEC (degrees)
\end{description}
The program gives the option of binning directly from an HKR file {i.e.}
using the file start and end MJDs and intrinsic bin size of 8 secs) or
using the bin structure from an existing time series dataset.
In this latter mode the user may create arrays of background counts
aligned with an existing bin structure.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 USE         -      LOGICAL    Requests that the binning information
                               should be extracted from an existing
                               time series.
 INPUT       -      CHAR       If USE is selected the user is prompted
                               for the name of a time series dataset.
 HKR         -      CHAR       The name of an HKR reduced rates file.
                               The .HKR extension is optional.
 OUTPUT      -      CHAR       The name of the o/p HDS file.

\end{verbatim}\subsection{WFCIMAGE*}
The ICL procedure creates a full field, 256 pixel, counts image together
with the corresponding exposure corrected image and exposure field
images.

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> WFCIMAGE (RWOD) (FILT) (OUTF)
\end{verbatim}\end{quote}
The online parameters are
\begin{verbatim}
   RWOD    CHAR     The full RWOD description including device and
                    directory
   FILT    CHAR     The WFC filter
   OUTF    CHAR     The root name for the output files. The counts
                    image has the name , the exposure corrected
                    image _C and the exposure field image
                    _X
\end{verbatim}
\subsection{WFCLIGHT*}
The ICL procedure is intended for interactive use only. It requests the
location of source and background boxes from the currently displayed
image and produces a background subtracted, exposure corrected, light
curve for the source.

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> WFCLIGHT (OUTF) (TBIN)
\end{verbatim}\end{quote}
The online parameters are
\begin{verbatim}
   OUTF    CHAR     The o/p light curve dataset
   TBIN    REAL     The time bin length in seconds
\end{verbatim}
\subsection{WFCPOSX}
This program takes as input an ASTERIX image dataset created by WFCSORT
and reports exposure information for any location within the image.
The quoted effective exposure is the value by which a raw count derived
at the location should be divided in order to produce a count rate
corrected to the axis of the instrument at the epoch of the data set.
In addition the program displays the efficiency of the detector for the
epoch of the data set relative to the launch efficiency. This relative
efficiency may be used to increase a measured flux to that appropriate
to a non-degraded detector. The exposure calculation works in a similar
manner to that in the WFCEXP program described above.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         -      CHAR       The name of an input ASTERIX image
                               dataset.
 RA          -      REAL       The right ascension of the point in the
                               FOV at which exposure information is
                               required (deg).
 DEC         _      REAL       The declination of the point in the FOV
                               at which exposure information is required
                               (deg).

\end{verbatim}\subsection{WFCSLOTS}
Select time slots when the WFC was in a particular filter configuration.
The time slots are written to a text file with each record containing
one pair of start and end times (MJDs). The file may be merged (using
the WFCSLMRG program) with a similiar file created from XRT data and in
this way common time slots, when the two instruments were simultaneously
collecting data, may be defined and used to control event selection in
the WFCSORT program.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 EVEF        -      CHAR       The name of the input event file (default
                               extension is .EVE).
 SHOW        -      LOGICAL    Requests that a sub-set of the information
                               from the header records of the event
                               file is displayed.
 SLOTS       -      CHAR       A text string which specifies which slots
                               to include in the sort. (These are the
                               slots which are displayed when SHOW = Y
                               is selected).
                               Specify the slots in the format
                               i-j,k,l-m. The slots selected should all
                               have the same filter at the focus. If a
                               slot value of '0' is selected the user
                               is prompted for the filter type instead.
 SFILE       -      CHAR       The name of a text file into which the
                               time windows will be written. Each record
                               in the file contains a pair of start and
                               stop MJDs separated by a space and with
                               each MJD preceded by the letter 'M'.
 FILTER      -      CHAR       A text string definition of the filter
                               to use.

\end{verbatim}\subsection{WFCSLMRG}
The program is used to merge time slot files into a single file of common
time slots. The merged file may then be used to control event selection
in the WFCSORT program.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 SFILE1      -      CHAR       The 1st time slot file to merge.
 SFILE2      -      CHAR       The 2nd time slot file to merge.
 SOFILE      -      CHAR       The output merged time slot file.

\end{verbatim}\subsection{WFCSORT}
Sorts the pre-processed event files from an RWOD into ASTERIX datasets.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 EVEF        -      CHAR       The name of the input event file (default
                               extension of .EVE).
 SHOW        -      LOGICAL    Requests that a sub-set of the
                               information from the header records of
                               the event file is displayed.
 OUTPUT      -      CHAR       The name of the output dataset.
 DTYPE       -      CHAR       The type of dataset to be produced by the
                               sort. Valid types are

                               (I)mage   ASTERIX image dataset
                               (R)aw     detector coord image
                               (L)in     linearised detector coord image
                               (T)ime    time series dataset
                               (E)vent   event dataset.

 RA          -      REAL       The Right Ascension of the sort field
                               centre (deg). By default this is set to
                               the axis RA value  given in the header.
 DEC         -      REAL       The Declination of the sort field centre
                               (deg).
 DAZ         -      REAL       The half extent of the field to be sorted
                               (deg), the default value is 3 degrees.
 INR         -      REAL       In time series mode INR specifies the
                               inner radius of an annulus from which to
                               select events. The radius should be
                               specified in arcminutes and may be set to
                               zero in order to select from a circular
                               region.
 OUTR        -      REAL       In time series mode OUTR specifies the
                               outer radius of an annulus from which to
                               select events.
 NXPIX       -      INT        The number of pixels along a side of the
                               image. For the default field half extent
                               of 3 degrees NXPIX = 256 would give a
                               pixel width of approx 1.4 arcminutes.
 SLOTS       -      CHAR       A text string which specifies which slots
                               to include in the sort. (These are the
                               slots which are displayed when SHOW = Y
                               is selected). Specify the slots in the
                               format i-j,k,l-m. The slots selected
                               should all have the same filter at the
                               focus. If a slot value of '0' is selected
                               the user is prompted for the filter type
                               instead.
 START_T     -      REAL*8     The start time of a selection window
                               which will be combined with the slot and
                               discrimination windows. The unit may be
                               either seconds from the BASE_MJD given in
                               the EVE file header or an absolute MJD
                               (in the latter case the MJD value should
                               be preceded by an 'M').
 END_T       -      REAL*8     The end time of a selection window (see
                               above)
 USEF        -      LOGICAL    Requests that events will be selected
                               from time windows specified in a simple
                               text file.
 SFILE       -      CHAR       The name of a text file from which the
                               time windows will be read. Each record in
                               the file should contain a pair of start
                               and stop MJDs separated by a space and
                               with each MJD preceded by the letter 'M'.
 NBINS       -      INT        The number of time bins. If a value '0'
                               is given the program prompts for the bin
                               duration instead.
 TBIN        -      REAL       If NBIN is specified as 0 then TBIN
                               specifies the bin duration (secs)
 FILTER      -      CHAR       The WFC filter to use.
 REJECT      -      CHAR       Requests rejection of events by one of
                               the  'LEVS', or 'VIEW' options. If VIEW
                               event rate thresholding is selected all
                               events from periods when the
                               velocity-view (or ram) angle .OR. the
                               solar zenith angle was below certain
                               limits will be rejected. The optimum
                               values for these angles are set by the
                               program. Alternatively the parameter may
                               be set to 'NONE'.
 LEVS        -      REAL       If LEVS event rate thresholding is
                               selected all events from periods when the
                               LEVS rate exceeds this threshold will be
                               rejected (suggested start value is 50 c/s)
 ROOT        -      CHAR       The File root which defines the location
                               of the .HKR file used for LEVS
                               discrimination.
\end{verbatim}\subsection{WFCSPC*}
The procedure is intended for interactive use only. It requests the
location of source and background boxes from the currently displayed
image and produces a background subtracted, exposure corrected, spectral
dataset for the source.
The procedure may optionally arrange to collect data from periods only
when the WFC and XRT systems were collecting data simultaneously.

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> WFCSPC (OUTF) (MERGE) (XTIM)
\end{verbatim}\end{quote}
The online parameters are
\begin{verbatim}
   OUTF    CHAR     The o/p spectral dataset
   MERGE   LOGICAL  Requests WFC and XRT commonality
   XTIM    CHAR     If MERGE is set this is the name of a simple text
                    file in which each record contains a pair of MJDs
                    between which the XRT was collecting data. The MJDs
                    should be separated by a space and each preceded by
                    the letter 'M'.
\end{verbatim}
\subsection{WFCSPEC}
Creates a Wide Field Camera `spectrum' (which has only one
channel) complete with energy response.

The program takes as input either a WFC datafile (an NDF)
containing only a single data value (it does not have to
be 1-dimensional though), or a source search
results file (from PSS). In the latter case, all sources
in the file will be listed, and the user selects which
is to be used.

The filter in use is looked up in the INSTRUMENT.FILTER
component (which should be present in any WFC dataset)
and the appropriate instrument energy response is attached
to the output spectrum.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      CHAR       Name of input dataset
      -GLOBAL.BINDS ( Default binned dataset )
 AUX         -      CHAR       Name of file searched to create INP.
                               Only used if INP is a source search
                               results file, and the image searched
                               cannot be extracted.
 OUT         2      CHAR       Name of output spectrum
      ->GLOBAL.BINDS ( Default binned dataset )
 SRC                INTEGER    Number of source in SSDS
 OVERRIDE    -      LOGICAL    Override observation date.
   Hidden default = NO         Enables study of time evolution
                               of WFC efficiency.
 OBSMJD      -      DOUBLE     MJD of observation. Default
                               offered is that from the
                               dataset header
\end{verbatim}\subsubsection{WFC Efficiency}
In normal operation WFCSPEC attaches a response which takes into
account any loss of efficiency in the instrument over time. If
the aim of the analysis is to investigate this loss, then OVERRIDE
mode may be used to force WFCSPEC to attach a response from a
given date.

\subsection{WFCSUB*}
The ICL procedure subtracts a background light curve from a source light
curve. The two input light curves must have been exposure corrected. The
fluxes are normalised by the areas of the two original boxes from which
events were sorted, then subtraced.

To run (from within ICL):
\begin{quote}\begin{verbatim}
ICL> WFCSUB (SRCF) (BKDF) (OUTF)
\end{verbatim}\end{quote}
The online parameters are
\begin{verbatim}
   SRCF    CHAR     The source light curve dataset
   BKDF    CHAR     The background light curve dataset
   OUTF    CHAR     The subtracted light curve dataset
\end{verbatim}
\section{XRT ROSAT}
A package of XRT specific routines. See user\_005 in AST\_DOCS:
(VMS) or \$AST\_DOCS (UNIX) for an overview of XRT processing
\subsection{Getting started}
So you've got an XRT data tape - now what ?

Print-off or display a copy of the XRT users guide, user\_005. This
is in AST\_DOCS: (VMS) or \$AST\_DOCS (UNIX). This gives an
overview of the programs needed to produce Asterix datafiles and process
them.

The first thing you have to do with you data is to convert the files to a form
that can be understood by asterix. This only has to be done once - see the
help on XRTCONV.

The remainder of this help facility gives information on the particular
difficulties of reducing XRT data; (e.g. the 'Time\_series\_analysis' section)
and explains the functions of the various applications in the XRT suite.
It's not a bad idea to at least skim through the help entry for a given
program before running it for the first time.

Occasionally, important bugs found in the XRT routines or new calibration
information are entered into the "Latest news on XRT routines" topic
on the XRAYSIG\_NEWS conference. This should be regularly monitored.
This facility will be replaced in the near future with a WWW (World Wide
Web) service.

\subsection{The HRI}
HRI analysis is still in its infancy in Asterix. XSORT will sort the
data into the usual Asterix datafiles e.g. an image or time series
and will ignore events from known hotspots/deadspots. (currently only
MPE HRI data) These datafiles may then be further analysed by the
Asterix image processing or time series analysis packages just as for
the PSPC.

The correction and response programs will not currently
work on these HRI products. Further calibration information is awaited
before these routines can be written. The main obstacle at the moment
is the lack of pulse height information in the data received; when this
is available, the pulse heights may prove to be useful in reducing the
contamination of the data by the UV leak. It is also hoped that in due
time the pulse heights may give scientifically useful spectral
information.

\subsection{The Boron filter}
The filter only covers the region of the image within the central ring.
The region outside is totally unfiltered and therefore
should NOT be used as background for a source within the ring.

NB: There is currently a problem with analysing sources outside
the central ring in images which have been filtered. The software
currently treats these sources as though they have been filtered.
A workaround is to type
\begin{quote}\begin{verbatim}
> hmodify filename.more.asterix.instrument.filter val="OFF"
\end{verbatim}\end{quote}
before correcting the file with XRTCORR.

This data exterior to the filter was only provided by some early versions of
the SASS.

\subsection{Time series analysis}
The analysis of time series produced with the XRT PSPC is severely
complicated by the window structure in front of the detector.
This window support structure consists of 3 major components.

\begin{enumerate}
\item Wide ribs which reduce the transmission for a point source to zero
\item Thick wires with a 4 arcminute spacing and a width of about 12
arcseconds
\item Thin wires with a spacing of 50 arcseconds and a width of 3 arcsecs
\end{enumerate}
Further to this the spacecraft is usually wobbled over a distance of 3
arcmins in a time of 3 minutes. So the effect on a time series is the
convolution of the window structure and this wobble. The mirror structure
causes the beam incident on the detector to be focused to a HWHM of $\sim$ 3
arcseconds on axis hence all the structural components can have a
significant effect on the time series.

The three effects will cause.

\begin{enumerate}
\item Nulls in the time series when a source passes under a rib (NB:
most sources will not lie near a rib.
\item A dip in the time series every time the source passes beneath a
thick wire. This may occur 0,1 or 2 times every wobble (e.g. every
6 minutes) depending on the proximity of the source to a wire and
the angle of the wobble relative to the wires. The intensity of this
dip will be dependent on the beam size from the mirror at the
off-axis angle of the source. A source at a large off-axis angle
will be spread out by the mirror and so a small fraction of the
total counts from the source will be obscured by each pass under
a thick wire. A source near the centre of the field of view,
however, will have most or all of its counts obscured at each pass
under the thick wire.
\item A smaller dip every time the source passes under a thin wire.
This will occur upto 8 times a wobble cycle. Because these wires
are only 3 arcsecs in diameter, at the centre of the field
the reduction in count rate will be about 50 percent. Away from the
inner ring these wires may not contaminate a time series at all.
\end{enumerate}
The software at present makes no attempt to compensate for these effects
within a time series. The user should be aware that these effects exist
and take them into account when performing time series analysis.

\subsubsection{Barycentric correction}
To barycentrically correct XRT data, use the following procedure:
\begin{itemize}
\item Create an EVENT dataset with XSORT: select a small box around
your source and choose the option DATASET=E
\item Convert the orbit file into the correct format with XRTORB. See
help for XRTORB for more details.
\item Barycentrically correct the datafile using BARYCORR.
\item Bin the event file into a time series using EVBIN
\end{itemize}
\subsubsection{Processing sequence}
Once the data file conversions have been performed (using XRTCONV), a
typical sequence of commands would be:

\begin{enumerate}
\item Create an image of the whole field using XSORT
\begin{quote}\begin{verbatim}
ICL> XSORT SRCFILE=CYGX1_IMAGE
\end{verbatim}\end{quote}
\item Display the image using the 'I' routines.
\begin{quote}\begin{verbatim}
ICL> ILOAD  CYGX1_IMAGE
ICL> IDISPLAY
\end{verbatim}\end{quote}
\item Choose source and background box centres and sizes
\begin{quote}\begin{verbatim}
ICL> XRTBCKBOX
\end{verbatim}\end{quote}
This will allow you to use a cursor on the image to select
the position and radius of a source and background box.
It then sorts events from within these boxes into a source
and background time series.
\item Background subtract the source file
\begin{quote}\begin{verbatim}
ICL> XRTSUB
\end{verbatim}\end{quote}
This corrects the background time series to that which would have
been seen had this background been at the position of the source
box. It then subtracts it from the source time series.
\item Correct the subtracted time series
\begin{quote}\begin{verbatim}
ICL> XRTCORR
\end{verbatim}\end{quote}
This will correct the time series for dead time, vignetting,
scattering and some overall wire correction. It will not,
however, attempt to remove the features imposed onto the time
series by the window structure of the PSPC.
\item Analyse the result, for example,
\begin{quote}\begin{verbatim}
ICL> POWER
\end{verbatim}\end{quote}
Performs an FFT on the subtracted time series.
\end{enumerate}
\subsection{Spectral analysis}
The PSPC detector on the XRT has 256 corrected pulse height channels
which can be used in spectral fitting. In reality the resolution of
the detector gives about 5 independent energy points which will affect
the outcome of any spectral fit.

The Asterix system uses a detector matrix, Rootname\_DRPM.SDF, which
contains the relative contributions of 729 trial energies in each
of the 256 corrected pulse height bins. This needs to be multiplied
by the effective area at each energy before it can be used in a
spectral fitting program. The effective area depends on the position
of the source in the field of view. Asterix writes an array containing
the effective area at each of the 729 trial energies for the position
of a given source into the source file. (Actually in a structure
MORE.ASTERIX.INSTRUMENT.ENCORR.DATA\_ARRAY). This is then multiplied by
the detector matrix prior to spectral fitting. So the detector response
is unique to each source and hence is stored in the datafile itself.
This can make each spectral datafile large, especially if the full 256
bin resolution has been used.

\subsubsection{Point sources}
The processing sequence for performing a spectral fit on a point
source (once the data files have been converted) is as follows.

\begin{enumerate}
\item Create an image of the whole field using XSORT
\begin{quote}\begin{verbatim}
ICL> XSORT  SRCFILE=CYGX1_IMAGE
\end{verbatim}\end{quote}
\item Display the image using the 'I' routines.
\begin{quote}\begin{verbatim}
ICL> ILOAD  CYGX1_IMAGE
ICL> IDISPLAY
\end{verbatim}\end{quote}
\item Choose source and background box centres and sizes
\begin{quote}\begin{verbatim}
ICL> XRTBCKBOX
\begin{quote}\begin{verbatim}

This will allow you to use a cursor on the image to select
the position and radius of a source and background box.
It then sorts events from within these boxes into a source
and background spectrum.

\item Background subtract the source file

\begin{quote}\begin{verbatim}
ICL> XRTSUB
\begin{quote}\begin{verbatim}

This corrects the background spectrum to that which would have
been seen had this background been at the position of the source
box. It then subtracts it from the source spectrum.

NB: the correction of the background spectrum to the source
position is a vignetting correction which is performed on a
pulse height channel basis. In reality vignetting corrections
are only calibrated for photon energies. So a small third order
error is introduced by assuming that we know the "energy" of
each corrected pulse height bin.

\item Correct the subtracted spectrum

\begin{quote}\begin{verbatim}
ICL> XRTCORR
\end{verbatim}\end{quote}
This will correct the spectrum for the energy independent
effects, dead time and wires. In addition it produces an
array of energy dependent correction factors within the
datafile. These are used as the effective area at each energy
for this datafile.
\item Produce a detector response matrix
\begin{quote}\begin{verbatim}
ICL> XRTRESP
\end{verbatim}\end{quote}
This multiplies the detector matrix file with the energy
corrections array found in the corrected datafile. The
resultant energy response structure is stored in the datafile.

\item Set up a spectral model and fit the data
\begin{quote}\begin{verbatim}
ICL> SMODEL
ICL> SFIT
\end{verbatim}\end{quote}
\end{enumerate}
\subsubsection{Extended sources}
XSORT may be used to generate a file containing a series of radially
binned spectra or time series. This file may be propogated through the
subtraction and correction stages of the system to enable spatially
resolved spectral or temporal analysis to be easily achieved. See the
help entries for individual programs for more details.

\subsection{Image analysis}
Images can be corrected with the current software for dead time,
vignetting, wires, point spread and the boron filter. See the help on
XRTCORR for further information.

Background subtraction of images may be performed with XRTSUB.

\subsubsection{Processing sequence}
\begin{quote}\begin{verbatim}
 ICL> XSORT

 ICL> XRTSUB

 ICL> XRTCORR
\end{verbatim}\end{quote}
\subsection{Distributed data conversion}
Data obtained in MPE,US and RDF FITS format may be processed
using the XRTCONV program. (FITS to HDS converter).

To process the data follow the following procedure :

\begin{enumerate}
\item Put the FITS files into a directory. The important files are listed
in the following sections, marked with a "*"
\item Convert the files with:
\begin{quote}\begin{verbatim}
csh> xrtconv
\end{verbatim}\end{quote}
\end{enumerate}
This is significantly simpler than the previous method, using
PREPXRT, USPREPXRT and XRTDISK.

\subsubsection{MPE MIDAS FITS}
PSPC data sets
\begin{verbatim}
Name                  Description
====                  ===========
events.tfits         *Events
eventrates.tfits     *Event rates
attitude.tfits       *Attitude
orbit.tfits          *Orbit
mexmap.ifits          Exposure map
image1.ifits          Broad band image
image2.ifits          Hard band image
image3.ifits          Soft band image
imageec.ifits         Coded energy image
difevents.tfits      *Differential events
quality.tfits         Quality
moimp.ifits           Modified instrument map
drmpspc.ifits         Detector response
effarea.tfits         Effective area
\end{verbatim}
HRI data sets
\begin{verbatim}
Name                  Description
====                  ===========
events.mt            *Events
eventrates.mt        *Event rates
attitude.mt          *Attitude
orbit.mt             *Orbit
difevents.mt         *Differential events
hotmap.mt            *Hotspots/deadspots
qefficiency.mt        Qefficiency
quality.mt            Quality
qualitylimits.mt      Quality limits
\end{verbatim}
\subsubsection{GSFC PROS FITS}
PSPC data sets
\begin{verbatim}
Name                  Description
====                  ===========
.fits          *Events
.asp           *Aspect quality
.cas            Corrected aspect
.evr           *Event rates
.hkb            Binned housekeeping
.mds            Master data set
.par            Parameter set
.rcr            Radiation calibration
.sa            *Split aspect
.sas            Smoothed aspect
.so            *Split orbit
.sta            Housekeeping status
_im1.fits       Broad band image
_im2.fits       Hard band image
_im3.fits       Soft band image
_mex.fits       Exposure map
_sky.fits       Sky catalogue table
_smx.asc        Sky catalogue table
.moi            Modified instrument map
.dmp            Detector response
.oar            Effective area
\end{verbatim}
HRI data sets
\begin{verbatim}
Name                  Description
====                  ===========
.fits          *Events
_img.fits       Image
.ah             Aspect histogram
.anp            Processing parameters
.ao            *Aspect offsets
.cpb            Charged particle bgnd map
.cps            Complete photon store
.dbm            Detector background map
.dms            Cts/pix**2/s histogram
.htl            Science frame qualities
.ltf            Live-time correction factors
.mob            Merged OBI TSH
.oan            Observation angles
.obi            Bright earth intervals
.obt            Bright earth TOE
.ots            TSH by OBI
.par            Output parameter set
.qeg            Quantum efficiency map
.s1d            Samping histogram
.saa            SAA rates
.sgi            Standard good intervals
.so            *Split orbit information
.sps            Science rates
.ssc            Standard screen results
.tsh            Temporal status history (TSH)
_sky.asc        Sky catalogue table
_sky.fits       Sky catalogue table
_sky.plt        Postscript
_str.plt        Postscript
\end{verbatim}
\subsubsection{RDF FITS}
PSPC data sets
\begin{verbatim}
Name                  Description
====                  ===========
_bas.fits      *Events and good times
_anc.fits      *Aspect and ancilliary data
_im1.fits       Broad band image
_im2.fits       Hard band image
_im3.fits       Soft band image
_bk1.fits       Broad band background
_bk2.fits       Hard band background
_bk3.fits       Soft band background
_mex.fits       Merged exposure map
_ime.fits       Energy coded image
_src.fits       Source extraction information
_his.fits       Processing parameter and log files
\end{verbatim}
HRI data sets
\begin{verbatim}
Name                  Description
====                  ===========
_bas.fits      *Events and good times
_anc.fits      *Aspect and ancilliary data
_im1.fits       HRI image
_bkg.fits       Backgound image
_src.fits       Source extraction image
_his.fits       Processing parameter and log files
\end{verbatim}
\subsection{New to Version 1.8}
The changes in version 1.8 are...
\subsection{FITS2HDS}
Converts FITS table or image data into HDS files.

A HDS file is produced for each image or table contained in the
FITS file. IF the FITS keyword "EXTNAME" is present this is
appended to the output filename supplied, otherwise a number is
added for each extra table. Similar tables are joined together
if they have the same column names and extension name.

FITS2HDS will attempt to create the standard NDF axis structure
for image data using information contained in the keywords, unless
the hidden parameter "ASTERIX" is set false.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type          Description
 -------  --------  ----          -----------
 INPUT       1      CHARACTER   Name of input FITS datafile
 OUTPUT      2      CHARACTER   Rootname of HDS output file.
 ORIGIN             CHARACTER   Origin of dataset (MPE, US, RDF)
 JOIN               LOGICAL     Join similar tables together
                                (Default = TRUE)
 ASTERIX            LOGICAL     Attempt to extract axis information
                                and create NDF structure
                                (Default = TRUE)

\end{verbatim}\subsubsection{Examples}
To convert a FITS file containing an image dataset into a HDS file.
\begin{quote}\begin{verbatim}
csh> FITS2HDS
FITS2HDS version 1.7-2
INPUT - Name of FITS file > RH201756N00_IM1.FITS
FITS file type is RDF
OUTPUT - Generic name of HDS output files > rh201756_im1
Writing to rh201756_im1
\end{verbatim}\end{quote}
Produces:
\begin{quote}\begin{verbatim}
FITSCONV

  DATA_ARRAY(512,512)     0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
                                 ... 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
  AXIS(2)        <AXIS>          {array of structures}

  Contents of AXIS(1)
     UNITS                'degrees'
     DATA_ARRAY     <ARRAY>         {structure}
        VARIANT               'SPACED'
        BASE                    0.5688889
        SCALE                   -2.2222223E-03
        DIMENSION            512

  MORE                      {structure}
     ASTERIX                   {structure}
        HEADER              {structure}
           TARGET               'RE0537+02'
           OBSERVER             'JEFFRIES, ROBIN,DAVI'
           OBSERVATORY           'ROSAT'
           INSTRUMENT            'XRT'
           AXIS_RA               84.44
           AXIS_DEC              2.52
           FIELD_RA              0.E00
           FIELD_DEC             0.E00
           EQUINOX              2000
           POSITION_ANGLE        0.E00
           BASE_MJD             49420
           BASE_DATE            '1994-Mar-09'
           BASE_UTC              48831.000000014
           BASE_TAI              8103.5653703704
           OBS_LENGTH              2356.136
           EXPOSURE_TIME           2356.136

        PROCESSING          {structure}
           BGND_SUBTRACTED      FALSE
           CORRECTED           {structure}
              DEAD_TIME            FALSE
              VIGNETTING           FALSE


        INSTRUMENT          {structure}
           PIXEL_SIZE              8
           FILTER               'OFF'
           DETECTOR             'HRI'
           SC_BASE               118945621
           SC_CONV               1
           RAWDATA               '     '
           SASS_VERSION         '                         '


     FITS(79)             'SIMPLE  =                    T / fi...'
                                    ... 'HISTORY     image(x,y)...','HISTORY'

  HISTORY               {structure}
     CREATED              '18-May-94 14:39:09'
     UPDATE_MODE          'NORMAL'
     EXTEND_SIZE          10
     CURRENT_RECORD       1
     RECORDS(10)          {array of structures}

     Contents of RECORDS(1)
        DATE                 '   18-May-1994 14:39:09'
        COMMAND              'FITS2HDS Version 1.7-2'
        HOST                 'LTVS4'
\end{verbatim}\end{quote}
To convert a FITS file containing a number of tables to a set of HDS
files.
\begin{quote}\begin{verbatim}
csh> FITS2HDS
FITS2HDS version 1.7-2
INPUT - Name of FITS file > RH201756N00_BAS.FITS
OUTPUT - Generic name of HDS output files > rh201756
Writing to rh201756_stdgti
Converting 3 columns and 33 rows
Reading/mapping from 1 to 33
Writing to rh201756_stdevt
Converting 8 columns and 12784 rows
Reading/mapping from 1 to 10000
Reading/mapping from 10001 to 12784
Writing to rh201756_rejevt
Converting 8 columns and 3108 rows
Reading/mapping from 1 to 3108
Writing to rh201756_tsi
Converting 18 columns and 286 rows
Reading/mapping from 1 to 286
Writing to rh201756_stdqlm
Converting 33 columns and 1 rows
Reading/mapping from 1 to 1
Writing to rh201756_allqlm
Converting 33 columns and 1 rows
Reading/mapping from 1 to 1
\end{verbatim}\end{quote}
Produces 6 HDS files.

\subsection{XRTCONV}
XRTCONV is a FORTRAN wrapup of FITS2HDS, XRTINDEX and XRTHOT. It
knows which files to look for, for any given dataset type (MPE, US and RDF)
and converts them into HDS. XRTINDEX is run on the events data to
extract useful information into a HDS header file. If hotspot/deadspot
information is available (currently only MPE HRI data) XRTHOT is run
and the data added to the HDS header file.

XRTCONV will automatically detect the origins of a dataset. It does this
using the following rules:
\begin{itemize}
\item MPE dataset, if any files ending in ".*fits" or ".mt".
\end{itemize}
otherwise the first file ending in ".fits" is opened
\begin{itemize}
\item MPE if keyword ORIGIN exists
\item US if keyword QPOENAME or XS-CNTRY exists
\item RDF if keyword RDF\_VERS exists
\end{itemize}
The automatic detection can be overridden by specifying one of the
following:

\begin{quote}\begin{verbatim}
csh> XRTCONV origin=
\end{verbatim}\end{quote}
\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type          Description
 -------  --------  ----          -----------
 RAWDIR             CHAR      Directory of raw FITS files
 FITSROOT           CHAR      Rootname of FITS files
 ORIGIN             CHAR      Origin of datafiles (MPE,US,RDF)
   Default = AUTO
 ROOTNAME           CHAR      Rootname of output HDS files, this can
                              include a directory specification

\end{verbatim}\subsubsection{FITS file contents}
The following are a mapping of FITS files to the HDS names used in
the various datasets. Not all the files listed are used by the XRT
interface.

\paragraph{MPE MIDAS FITS}
PSPC dataset
\begin{verbatim}
FITS filename             HDS filename
=============             ============
events.tfits              .sdf
difevents.tfits           _dif.sdf
attitude.tfits            _att.sdf
eventrates.tfits          _evr.sdf
orbit.tfits               _orb.sdf
quality.tfits             _qua.sdf
\end{verbatim}
HRI dataset
\begin{verbatim}
FITS filename             HDS filename
=============             ============
events.mt                 .sdf
difevents.mt              _dif.sdf
attitude.mt               _att.sdf
eventrates.mt             _evr.sdf
orbit.mt                  _orb.sdf
quality.mt                _qua.sdf
hotspot.mt                _hot.sdf
\end{verbatim}
\paragraph{GSFC PROS FITS}
PSPC dataset
\begin{verbatim}
FITS filename             HDS filename
=============             ============
.fits               _gti.sdf
                          _tsi.sdf
                          _events.sdf
.asp                _aspqu.sdf
.evr                _evrat.sdf
.sa                 _spasp.sdf
.so                 _sporb.sdf
_im1.fits           _im1.sdf
_im2.fits           _im2.sdf
_im3.fits           _im3.sdf
\end{verbatim}
HRI dataset
\begin{verbatim}
FITS filename             HDS filename
=============             ============
.fits               _gti.sdf,
                          _tsi.sdf,
                          _events.sdf
_img.fits           _img.sdf
.ao                 _ao.sdf
.so                 _sporb.sdf
\end{verbatim}
\paragraph{RDF FITS}
PSPC dataset
\begin{quote}\begin{verbatim}
FITS filename             HDS filename
=============             ============
_bas.fits           _stdgti.sdf
                          _stdevt.sdf
                          _rejevt.sdf
                          _tsi.sdf
                          _stdqlm.sdf
                          _allqlm.sdf
_anc.fits           _obstable.sdf
                          _ephem.sdf
                          _aspect.sdf
                          _hkp.sdf
                          _evrate.sdf
_im1.fits           _im1.sdf
_im2.fits           _im2.sdf
_im3.fits           _img.sdf
_bk1.fits           _bk1.sdf
_bk2.fits           _bk2.sdf
_bk3.fits           _bk3.sdf
\end{verbatim}\end{quote}
HRI dataset
\begin{quote}\begin{verbatim}
FITS filename             HDS filename
=============             ============
_bas.fits           _stdgti.sdf
                          _stdevt.sdf
                          _rejevt.sdf
                          _tsi.sdf
                          _stdqlm.sdf
                          _allqlm.sdf
_anc.fits           _obstable.sdf
                          _ephem.sdf
                          _aspect.sdf
                          _hkp.sdf
                          _evrate.sdf
                          _gstar.sdf
_im1.fits           _bk1.sdf
_bkg.fits           _im1.sdf
\end{verbatim}\end{quote}
\subsection{XRTINDEX}
XRTINDEX extracts essential information from the FITS header of the event
dataset and stores them in a HDS datafile. XRTINDEX is wrapped up in the
XRTCONV program. The header file is used by all the XRT interface
programs and contains information on exposure time, data origins, PI name,
obervation title and observation dates.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type          Description
 -------  --------  ----          -----------
 INPUT       1      CHAR      Name of FITS event datafile
 OUTPUT      2      CHAR      Rootname of output HDS files
 ORIGIN             CHAR      FITS file type (US/RDF/MPE)

\end{verbatim}\subsection{XRTHOT}
XRTHOT extracts hotspot and deadspot information from MPE HRI hotspot
FITS file and adds them to the header file produced by XRTINDEX. This
information is then used in XSORT to exclude events falling in these
regions. XRTHOT is wrapped up in the XRTCONV program.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type          Description
 -------  --------  ----          -----------
 INPUT       1      CHAR      Name of HOTSPOT fits file
 ROOTNAME    2      CHAR      Rootname of HDS datafiles

\end{verbatim}\subsection{OMD2HDS}
OMD2HDS converts binary event datafiles (.OMD) and the associated text
header file (.HDR), previously converted using PREPXRT and XRTDISK into
the HDS format used in ASTERIX version 1.7. This program is necessary
for porting data from VAX VMS machines onto Unix, the other HDS files
are directly transferable.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type          Description
 -------  --------  ----          -----------
 RAWDIR      1      CHAR      Location of datafile
 ROOTNAME    2      CHAR      Rootname of datafile
 DIR                LOGICAL   Display contents of the raw directory
 HDSROOT            CHAR      Rootname of output HDS datafile

\end{verbatim}\subsection{SASSBIN+}
An invocation of REBIN which automatically selects the recommended
SASS binning. If the dataset being rebinned is not just a spectrum
then the energy axis will have to be specified and on that axis may
be rebinned. The bins are as follows:
\begin{verbatim}
   bin       compressed channels
    1               8-10
    2              11-13
    3              14-16
    4              17-19
    5              20-23
    6              24-27
    7              28-31
    8              32-36
    9              37-41
   10              42-46
   11              47-51
   12              52-57
   13              58-63
   14              64-69
   15              70-76
   16              77-83
   17              84-90
   18              91-98
   19              99-106
   20             107-114
   21             115-122
   22             123-130
   23             131-139
   24             140-148
   25             149-158
   26             159-168
   27             169-178
   28             179-189
   29             190-200
   30             201-211
   31             212-222
   32             223-234
   33             235-246
\end{verbatim}
\subsection{SHOWXRT}
Produces a 1 page summary of a Rosat XRT observation from the
header file. It may also be used to generate a file containing the
ON-OFF times used in the SASS pre-processing, as MJDs. The
name of the ON-OFF times file produced by SHOWXRT is XRT\_TIMES.LIS.

\subsubsection{Method}
This routine works on the header data for the observation.

The header data is contained in the file \_hdr.sdf, which is
produced by the applications XRTINDEX inside XRTCONV, which must
be run before SHOWXRT is executed.

The header file is a HDS file containing the FITS header for the
FITS event file. Records are read from this file and re-formatted
into a one page summary of the observation. This may either be sent
to a file or displayed on the screen.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 RAWDIR       1     CHAR      Directory containing header file
 ROOTNAME     2     CHAR      Rootname of header file - only asked
                              if more than one *_HDR file on directory.
 SUMFILE      3     CHAR      File to contain observation summary
 TIMLIST            LOGICAL   Produce a file of ON-OFF times
                              called XRT_TIMES.LIS ?
   Default = NO
 SHOWOBS            LOGICAL   Produce observation summary ?
   Default = YES
 DIR                LOGICAL   List the .OMD files on directory ?

   Default = YES

\end{verbatim}\subsubsection{Examples}
To produce a summary of an observation.
\begin{quote}\begin{verbatim}
ICL> SHOWXRT
SHOWXRT - verison 1.7-0
RAWDIR - Raw data directory /'DISK$SCRATCH:[ASTERIX88]'/ >
ROOTNAME - Rootname of observation /sa/ >
SUMFILE - Name of obs. summary file (TT for terminal) > TT
\end{verbatim}\end{quote}
To just produce a file containing the time windows used in the preprocessing
\begin{quote}\begin{verbatim}
ICL> SHOWXRT TIMLIST=YES SHOWOBS=NO
SHOWXRT - version 1.7-0
\end{verbatim}\end{quote}
\subsection{XPSF*}
A graphical wrapup of the XRAD application. It allows the user to
select a position on a previously displayed image and then asks for
the fraction of enclosed counts required and the mean photon energy
to use. A circle is drawn denoting the radius containing that fraction
of counts.

\subsection{XPSSCORR}
Converts raw flux in a source search results file produced by PSS to
counts per second by correcting for vignetting as a function of both
position and energy, and presence of wires.

The new information added can be listed using SSDUMP.

\subsubsection{Input output}
The primary input file, which is updated, is the PSS source search
results file. XPSSCORR also requires

\begin{verbatim}
   Rootname_DRPM.SDF     Detector response matrix
   Rootname_EFF.SDF      Effective area of the instrument
\end{verbatim}
although the name of the first of these two can be overridden.

\subsubsection{Method}
There are two methods which may be used. Firstly, 'Calculation' mode,
where the exposure time is calculated using the vignetting and wire
corrections, essentially in the same way as in XRTCORR.

The corrections applied to the measured flux are :

\begin{enumerate}
\item Divide by the over all exposure time
\item Vignetting correct to the optical axis
\item Apply a correction of 1.0/0.79 for absorption by the
thin and thick wires in the PSPC support structure
\end{enumerate}
The other method is to use the exposure map supplied with
the observation; in this case exposure times near the ribs and
central circle can be accurately calculated, however, the map is fixed
over all times and all energies, so it will not be accurate if either
the time axis or energy axis has been subset.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  INPUT      1      UNIV       Name of input source search dataset
                                Defaults source search dataset
  AUX        -      CHAR       Name of file searched to create INP.
                               Only used if INP is a source search
                               results file, and the image searched
                               cannot be extracted.
  MODE       2      CHAR       'Calculate' or 'Exposure map'
  WIRES      -      LOGICAL    Apply a correction for wire absorption
  ENERGY     -      REAL       Mean photon energy in searched image
  EFFILE     -      CHAR       Name of effective area file
  RESPFILE   -      CHAR       Name of detector matrix file
  EXPMAP     -      CHAR       Name of exposure map (if this
                               option has been chosen)

\end{verbatim}\subsection{XRAD}
Calculates the radius of a circle which encloses a certain fraction
of the counts in a PSPC source, given an off-axis angle and mean
photon energy. It uses an implementation of the PSPC PSF developed
by I. Georgantopolous (LTVAD::IG) from in-flight calibration data.

To display the result on an image use the procedure XPSF90.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  OFFAX      1      REAL       OFF-axis angle (arcminutes)
  ENERGY     2      REAL       Mean photon energy (keV)
  PFRAC      3      REAL       Fraction of counts wanted (e.g. 0.9)
  DISP              LOGICAL    Display the result on the screen ?
  PSFRAD            REAL       Output parameter containing the
                               90% radius in DEGREES.
\end{verbatim}\subsubsection{Examples}
To calculate the radius containing 95\% of the counts of a fairly soft
source at an off-axis angle of 40 arcminutes
\begin{quote}\begin{verbatim}
ICL> XRAD OFFAX=40 ENERGY=0.2 PFRAC=0.9
\end{verbatim}\end{quote}
\subsection{XRTBOX*}
This is an ICL procedure which allows the user to interactively select
a circular box on an image. Raw XRT data is then sorted within this
box. The processing sequence should be:

\begin{enumerate}
\item Use XSORT to create an image from the raw data.
\item Display the image with the 'I' routines.
\begin{quote}\begin{verbatim}
ICL> ILOAD image_name
ICL> IDISPLAY
\end{verbatim}\end{quote}
\item Execute XRTBOX
\begin{quote}\begin{verbatim}
ICL> XRTBOX
\end{verbatim}\end{quote}
This leads to:

\begin{enumerate}
\item Select the centre of the source box with a cursor
\item Select the box radius with the cursor
\item Sort the data within the box.
\end{enumerate}
\end{enumerate}
\subsubsection{Graphics}
The procedure works on the image displayed in the current plotting zone,
remember you {\em cannot} select the source box centre from an image which
has been plotted in a zone other than the current one.

\subsubsection{Input \& outpu}
The input file to this procedure is an HDS image datafile. The output
will be an HDS datafile.

\subsubsection{Wrapups}
This procedure wraps up the applications ICIRCLE, ICURRENT and XSORT. See
the help entries for these for further information.

\subsection{XRTBCKBOX*}
This is an ICL procedure which allows the user to interactively select
a source box and a background box, within which to sort XRT data.

Optionally an optimisation algorithm may be used to
select the radius of the source box such that the signal to noise ratio
of the source is maximised (default is no optimisation).
This optimisation is turned on by specifying :

\begin{quote}\begin{verbatim}
    ICL> XRTBCKBOX OPT
\end{verbatim}\end{quote}
{\em YOU SHOULD NOT OPTIMISE WHEN PRODUCING A SPECTRUM OF
AN ON-AXIS SOURCE CONTAINING SIGNIFICANT SOFT (0.25 KEV) COUNTS.
THIS IS BECAUSE THE PSF CORRECTION IN XRTCORR CANNOT COPE WITH THE
'ELECTRONIC GHOST IMAGE' EFFECT IN LOW ENERGY DATA.}
The algorithm uses a default energy of 0.5 keV. For sources which may have
considerably harder or softer spectra than this, the energy may be set
on the command line e.g. to set the energy to 0.2 keV.

\begin{quote}\begin{verbatim}
ICL> XRTBCKBOX 0.2
\end{verbatim}\end{quote}
The optimisation should only be used on point sources.

\subsubsection{Method}
The background box may be either a circle away from the source box,
in which case the user is asked to enter a background centre and
radius, or it may be an annulus around the source box. To avoid the
possibility of contaminating a background annulus with counts from
the extended wings of the point spread function, the box is defined
by an inner and outer radius. The user should ensure that the inner
radius is beyond the point where the PSF of the source has decayed to
$\sim$ zero.

\subsubsection{Procedure}

\begin{enumerate}
\item Use XSORT to create an image from the raw data.
\item Display the image with the 'I' routines
\begin{quote}\begin{verbatim}
ICL> ILOAD image_name
ICL> IDISPLAY
\end{verbatim}\end{quote}
\item Type XRTBCKBOX
\begin{quote}\begin{verbatim}
ICL> XRTBCKBOX
\end{verbatim}\end{quote}
this leads to:

\begin{enumerate}
\item Select the centre of the source box with a cursor
\item Select an initial trial source box radius - (should be large)
\item Select the centre of the background box with a cursor
\item Select the background box radius or radii - (should be large)
\item The procedure then displays information about the source and
background boxes.
\item The background subtracted counts are then calculated along
with the optimum source box radius (if not turned off).
This radius together with the fraction of the PSF contained
within it, is displayed on the screen. A dashed line
representing the source box selected by the procedure is
drawn on the screen.
\item Finally, the data is sorted within both boxes.
\end{enumerate}
\end{enumerate}
The background box may be either a circle away from the source box or
be an annulus enclosing the source box entirely. The boxes shouldn't
overlap, unless the source box is totally enclosed within the bckgnd
box.

\subsubsection{Graphics}
The procedure works on the image displayed in the current plotting zone,
remember you CANNOT select the source box centre from an image which
has been plotted in a zone other than the current one.

\subsubsection{Input \& outpu}
The input file to this procedure is an HDS image datafile. The output
will be two HDS datafiles, containing the source and background data.

\subsubsection{Wrapups}
This procedure wraps up the applications ICIRCLE, ICURRENT, RADOPT and XSORT.
See the help entries on these for further information.
\subsection{XRTCORR}
Incorperates instrument specific corrections in XRT binned dataset files.
e.g. exposure, vignetting, etc.

\subsubsection{Input \& outpu}
The input file should be an Asterix binned XRT dataset, e.g. time series,
spectrum or image etc... This will most likely have a data array in counts
per bin. The output file will contain the corrected data array, now in
counts per second and in some cases an entry in the instrument box
within the header giving an array of energy dependent corrections which
need to be applied to the detector matrix rather than the data array.

\subsubsection{Error messages}
"Warning: insufficient eventrate data to calculate dead time correction
for nn timebins"

This means that the eventrate file, which is used to calculate dead times,
contains no data for nn timebins.

"Warning: bad eventrate data for nn timebins"

This means that the eventrate file contains a silly value for the
number of events recorded by the detector in a given time bin. This
number can either be zero, negative or ridiculously high.

Both of these messages suggest some problem with the eventrate, *\_EVR,
file. The dead time correction is typically only about 3 percent, so
unless you are interested in very accurate time series work, these messages
can be safely ignored. It may be better to perform the corrections again
with the dead time correction turned off by putting DCORR=NO on the
command line.

\subsubsection{Method}
The program corrects for the following effects with the following
assumed dependencies:
\begin{verbatim}
   Correction        Pos. dependent     Energy dept.    Time dept.
  ------------      ----------------   --------------  ------------
   DEAD TIME              NO                NO             YES
   WIRES                  NO                NO             NO
   VIGNETTING             YES               YES            NO
   POINT SPREAD           YES               YES            NO
   BORON FILTER           NO                YES            NO
\end{verbatim}
Different types of files are treated differently:

\begin{description}
\item[Time series:]
A time series will be corrected for all the above effects. For the
energy dependent corrections a mean photon energy of 0.2 keV will be
assumed. This may be overridden on the command line by e.g.
\begin{quote}\begin{verbatim}
  ICL> XRTCORR ENERGY=0.4
\end{verbatim}\end{quote}
The final data array is normalised by dividing by the exposure time.
\item[Images]
Images are corrected for dead time and vignetting.
The final data array is normalised by the exposure time.
The vignetting correction is only an approximation to allow
subsequent image processing work to be performed. It works
be calculating the vignetting at each pixel at a default energy of
0.2 kev (changed by specifying ENERGY=nn on the comand line).
\item[Spectra]
Spectra are treated differently. The data are corrected for the
energy independent factors and then normalised by the exposure time.
Energy dependent factors are combined and written into an extension
within the instrument box of the datafile,
\begin{verbatim}
       MORE.ASTERIX.INSTRUMENT.ENCORR(1).DATA_ARRAY
\end{verbatim}
This array containing correction factors for 729 energies is used by
the detector response program XRTRESP to modify the detector matrix
so that it is appropriate for the particular datafile being used.
NB: RADIAL\_SPECTRA contain one such array for each radial bin.

\item[Spectral images:]
These are corrected for dead time and vignetting. The vignetting
is performed separately for each spectral bin by calculating the
approximate mean energy of the bin and then using the effective
area curve relevant to this energy. Note this is an appoximation
only ! {\em Files which have been corrected like this cannot be used for
spectral fitting!} This option is an image analysis tool only.
\end{description}
\paragraph{RADIAL BINS}
RADIAL\_SPECTRA or RADIAL\_TIME datasets are corrected for all the
instrumental effects except the point spread function.

\paragraph{Dead time}
At present dead time is corrected for using the pre-flight formula
given in the MPE document TN-ROS-ME-ZAOO-025.

It is a function of the PSPC live time,

\begin{verbatim}
  PLIVE = SQRT ( 1.0 - 2.0E-06 * A1LL * DEADTP )
\end{verbatim}
(where A1LL is the event rate seen by the instrument and DEADTP is
the dead time parameter in microseconds. This is in reality a
function of photon energy but for now the program uses a value of
DEADTP=250 microseconds for all events.)
and the telemetry livetime,
TLIVE = AEXE / AXE
(where AEXE is the accepted and evaluated event rate and AXE is the
accepted event rate)
Making the dead time correction factor
DEADFACT = 1.0 / (PLIVE * TLIVE)
\paragraph{Wires}
Built into the effective area values is an energy independent correction
of 79\% to take account of the wires. This corrects for the thin and thick
wires but NOT for the large ribs. This can optionally be removed by
specifying "wires=no" on the command line.

\paragraph{Effective Area \& Vignettin}
Vignetting calibrations are stored in the effective areas file
(Rootname\_EFF.SDF). This file contains the effective area as a
function of photon energy for a series of off-axis angles. The
correction for given datafile is obtained by linearly interpolating
between the two effective area curves with the closest off-axis angle
to the source box. The effective area appropriate to the PSPC detector
(B or C) is automatically included.

Time series are corrected by assuming a mean photon energy and finding
the ratio of the on axis response at this energy to the response at the
position of the datafile. The data array is then multiplied by this
ratio. Spectra are left unchanged by the vignetting correction, however
an array containing the effective area for each energy at this position
is inserted into the datafile and used to modify the detector response
matrix by XRTRESP.

For a circular source box centred on the optical axis, the vignetting
is calculated using an off-axis angle of half the outer radius. The
optimum vignetting to use in the case of a large source box is dependent
on the photon ditribution within the box; for instance a box with most
of its counts in the centre should really have an off-axis angle of 0.0.
On the other hand a box with evenly distributed pohotons should have the
area weighted mean off-axis angle of that box,
i.e. SQRT((IRAD**2 + ORAD**2)/2), where IRAD ,ORAD are the inner and outer
box radii. The default off-axis angle may be overridden on the command
line by setting OFFAX1=??. For radial files, an off axis angle may be
specified for each bin, by OFFAX1=??, OFFAX2=?? etc...

\paragraph{Point spread}
The correction factor for source photons which lie outside the source
box used is energy dependent. An array of corrections, one for each of
the 729 trial energies, is calculated and used to produce the full
energy dependent correction array stored in the datafile as:
\begin{verbatim}
   MORE.ASTERIX.INSTRUMENT.ENCORR(1).DATA_ARRAY
\end{verbatim}
NB: The point spread function is only defined for a circular box.
It can be used reasonable accurately with an elliptical or rectangular
box, but NOT with an annulus. When files containing an annular source
box are corrected, point spread corrections are turned off.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
  INP        1      UNIV       Name of input file
  OUT        2      UNIV       Name of output file
  RTNAME     3      CHARACTER  Rootname of calibration files
  EFFILE            CHARACTER  Name of the effective area file
  RESPFILE          CHARACTER  Name of the detector response file
  ENERGY            REAL       Mean photon energy (default 0.2 keV)
  OVER              LOGICAL    Overwrite the input file (default=NO)
  AXTYPE            INTEGER    Axis type if label is unrecognised
  DCORR             LOGICAL    Perform the dead time correction ?
    Default = YES
                               Put "DCORR=NO" on the command line
                               to NOT do dead time corrections.
  WCORR             LOGICAL    Perform correction for the wires?
    Default = YES
  VCORR             LOGICAL    Perform the vignetting correction ?
    Default = YES
  PCORR             LOGICAL    Perform the point spread correction ?
    Default = YES
  OFFAX1            REAL       Off-axis angle for vignetting corrections
                               of the first radial bin or whole box
                               (Units = arcminutes)
  OFFAX2            REAL       Off-axis angle for vignetting corrections
                               of the second radial bin.
                               (Units = arcminutes)
    .
    .
  OFFAX10           REAL       Off-axis angle for vignetting corrections
                               of the tenth and subsequent radial bins.

\end{verbatim}\paragraph{RTNAME}
This is the rootname of the raw data files. e.g. the eventrate file
for MPE data is Rootname\_EVR.SDF. Just enter the rootname here, this may
include a directory spec. e.g. RTNAME > DISK\$SCRATCH:[RDS]3C273
The effective area and detector matrix files will ordinarily be
picked up from a default directory XRTCAL:

\paragraph{ENERGY}
If the datafile does not contain a corrected pulse height axis then
all energy information about the incoming photons has been lost.
Some of the corrections, e.g. vignetting, are energy dependent. So
to calculate the correction a value of the mean photon energy
reaching the detector is used. The default is 0.2 keV.

\paragraph{OVER}
To overwrite the input datafile rather than creating a seperate
output file enter OVER=YES
\paragraph{AXTYPE}
If the program does not recognise one of the axis labels then the
user is asked to identify the axis type. Enter an integer from the
following:
\begin{verbatim}
   X axis : 1
   Y axis : 2
   Time axis : 3
   Corrected pulse height axis : 4
   Anything else e.g. uncorrected pulse height axis : 5
\end{verbatim}
\subsubsection{Examples}
To correct a spectral dataset for everything
\begin{quote}\begin{verbatim}
ICL> XRTCORR
INP - Input dataset name > SUB
OUT - Output dataset name > SUB_CORR
RTNAME - Rootname for calibration files > ARLAC
Mean dead time correction : 1.0398
Mean vignetting correction : 51.6259
Mean scattering correction :1.00001
\end{verbatim}\end{quote}
To correct a 3 bin radial\_spectral dataset for everything,
using the area weighted off-axis angles:
\begin{quote}\begin{verbatim}
ICL> XRTCORR OFFAX1=0.71 OFFAX2=1.6 OFFAX3=2.55
\end{verbatim}\end{quote}
\subsection{XRTEXPMAP}
Constructs a PSPC exposure map for a given energy band. The program
uses the header file created by XRTCONV plus the attitude and event
rate files. The output is a 512 square exposure map of the whole
field. This can be transformed using REBIN using the axis cloning
option to trim the map to any particular size.

The program is essentially unchanged from the Steve Snowden original,
apart from adopting Asterix conventions on use of good time slots.
This means that there is a rather limited selection of energy bands
available and it is important only to apply the exposure maps to data
XSORTed using the same band.

\subsubsection{Method}
The program works by dithering a copy of the detector background for
a given energy band using the pointing information supplied in the
attitude file, resulting in a map of exposure as a function of
position on the sky. The choice of the particular detector map to use
is left to the user.

The algorithm does {\em not} take particles into account.

\subsubsection{References}
A full description of the algorithm can be found in
"Generation of Energy Dependent Exposure Maps for
ROSAT PSPC analysis", Mendenhall, J.A., Snowden, S.L., Burrows, D.N.
presented at AAS meeting, Summer 1992, Columbus Ohio.
A discussion of the accuracy of the algorithm can be found in
"The Flattening Accuracy of the Exposure Map Software",
Schlegel, E.M., OGIP Calibration Memo, CAL/ROS/93-026
\subsubsection{Parameters}
\begin{verbatim}
  Keyword  Posn     Type     Description
  -------  ----     ----     -----------
  RAWDIR     1      CHAR     Raw data directory. Default is the
                             current directory.
                              The directory must contain no more than
                              500 files.
  ROOTNAME   2      CHAR     Rootname of the raw data files. (The
                                              one used in XRTCONV)
  OUT        3      UNIV     Output exposure map.
  IFILE      -      INTEGER  Detector map to use. Choose from
                             list.
  TIMRANGE   -      CHAR     Filename containing list of good time
    Default = !              slots. Create this file using XRTHK.
                             Default assumes all times are ok.
  DETMAP     -      CHAR     Name of detector map. Creates an HDS
                             copy of the detector map selected
    Default = !

\end{verbatim}\subsection{XRTMSPEC*}
This is an ICL procedure which creates spectra for all the sources
in a ROSAT XRT field found by PSS. To run this procedure, you need
to be in the directory containing the raw data, and have a source
list available.
To invoke it type,
\begin{quote}\begin{verbatim}
 XRTMSPEC [srclist] [encpsf] [emean]
\end{verbatim}\end{quote}
This will produce a number of spectral files called SPEC1 -
SPEC$n$
where $n$ is the number of the source found by PSS.

The procedure adjusts the size of the box used to extract events
to account for the variation of the size of the psf across the field.

\subsubsection{Method}

\begin{enumerate}
\item Set your default directory to the directory which contains
the raw XRT data
\item Use XSORT to create an image from the raw data.
\item Run PSS on this image and produce a file containing the
positions of the sources in the image.
\item Execute XRTMSPEC by:
\begin{quote}\begin{verbatim}
ICL> XRTMSPEC
\end{verbatim}\end{quote}
\end{enumerate}
\subsubsection{Parameters}
\begin{verbatim}
  Parameter     Description

  srclist       The name of the PSS results file.
     Default = srclist
  encpsf        Fraction of enclosed energy which defines the
                extraction radius used for each source.
     Default = 0.95
  emean         The energy at which the psf info is calculated
     Default = 0.5 keV

\end{verbatim}\subsubsection{Wrapups}
This procedure wraps up the application XSORT.

\subsection{XRTMTIME*}
This is an ICL procedure which creates time series for all the sources
in a ROSAT XRT field found by PSS. It is identical to XRTMSPEC but
creates time series instead. To run this procedure you must be in the
directory containing the raw data, and must have a PSS results file.

To invoke it type,
\begin{quote}\begin{verbatim}
ICL> XRTMTIME [srclist] [encpsf] [emean] [tbin]
\end{verbatim}\end{quote}
This will produce a number of time series files called TIME1 -
TIME$n$ where $n$ is the number of the source found by
PSS.

\subsubsection{Method}

\begin{enumerate}
\item Set your default directory to the directory which contains
the raw XRT data
\item Use XSORT to create an image from the raw data.
\item Run PSS on this image and produce a file containing the positions
of the sources in the image.
\item Execute XRTMTIME by:
\begin{quote}\begin{verbatim}
ICL> XRTMTIME
\end{verbatim}\end{quote}
\end{enumerate}
\subsubsection{Parameters}
\begin{verbatim}
  Parameter     Description

  srclist       The name of the PSS results file.
     Default = srclist
  encpsf        Fraction of enclosed energy which defines the
                extraction radius used for each source.
     Default = 0.95
  emean         The energy at which the psf info is calculated
     Default = 0.5 keV
  tbin          Time bin size in seconds

\end{verbatim}\subsubsection{Wrapups}
This procedure wraps up the application XSORT.

\subsection{XRTHK}
This is a general program which allows data to be excluded
according to the values of house-keeping parameters. It works
by creating a simple textfile containing the times (MJD format) when
the housekeeping parameters were WITHIN the tolerances set.
This file may then be used in XSORT to control the times
over which data is sorted. (It may also be used in WFCSORT
for simultaneous data extraction).

\subsubsection{Advice}
The technique of subsetting the data according to housekeeping
parameters is in its infancy. The advice for now is :

Background reduction
--------------------
To minimise the number of particles in the background -
select EE\_MV (for MPE PSPC) to be between 0 and 170, this tends
to lose $\sim$10\% of the data.

NB: This parameter is only recorded for PSPC
data. No information is currently available on minimising the
HRI background.

Aspect improvement
------------------
Some on-axis sources exhibit strange PSFs, with a characteristic
lump in the bottom right quadrant. This can be improved by
setting constraints on the aspect error - set ASP\_ERR to be
between 0 and 2 (the units of ASP\_ERR are arcsecs/2). This
parameter may be used for both PSPC and HRI data.

NB: Data received in US FITS format isn't quite as useful.
The aspect error varies between 0 and 5, with everything over
1 being a large error (I think, info. is hard to come by !).
They also use -33 (and/or 9) to mean missing aspect info.

\subsubsection{Corrupt HK files}
Occasionally the calibration files included with the observation are
corrupt and don't cover the full time interval over which the
observation was taken. In this case, the times when HK data is missing
are INCLUDED in the output times file. i.e. these times are deemed to be
good, no matter what HK selection has been made. An appropriate message
is usually output if the HK data is incomplete.

\subsubsection{Error messages}
The error message "Maximum number of windows exceeded", means that
the program has exceeded a maximum number (currently 2000) of time
windows. The solution is probably to increase the INTERVAL value.

If NO bad time windows are found from the parameter values set, a
message is output to this effect and an output file is {\em not} created.

\subsubsection{Input \& outpu}
The program uses the Aspect/Attitude and Eventrate calibration files.
At least one of these must be present for the program to work.
In addition the program obtains timing information from the file
\_hdr.

The output is a textfile containing a list of ON-OFF MJDs.

After each parameter selection, a message indicating the exposure
time lost due to that selection is output. When the program terminates
a message indicating the total exposure time now available and the
percentage excluded is output.

\subsubsection{Method}
The Aspect/Attitude and Eventrate files contain a set of
housekeeping parameters as a function of spacecraft clock time.
The names of the parameters are detailed in the EXSAS users guide,
if you can get hold of a copy. Some of the more important
ones are:
\begin{verbatim}
  EE_MV* - the Master veto rate
  EE_AXE* - the accepted event rate
  ASP_ERR* - the aspect error
\end{verbatim}
*These names are for MPE PSPC data files.
Each of these are stored as HDS arrays in the above files.
Limits may be set on up to ten of these parameters. The program
merges the time ranges excluded by each of these limits and
inverts to produce a file of "GOOD" time windows over which the
data can be sorted. The file produced, contains a list of MJDs and
should be entered as the response to the prompt for TIMRANGE in
XSORT.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 RAWDIR        1     CHAR       Directory containing cal. files
 ROOTNAME      2     CHAR       Rootname of cal. files
 HKPAR1        3     CHAR       Name of housekeeping parameter
 PMIN1         4     REAL       Minimum value of this parameter
 PMAX1         5     REAL       Maximum value of parameter
 FNAME               CHAR       Name of output textfile
 DIR                 LOGICAL    Display .OMD files on directory ?
 INTERVAL            REAL       Smallest length of a "GOOD" window

 HKPAR2              CHAR       Name of housekeeping parameter
                                Enter "!" at any of the HKPAR prompts
                                if no more parameters are required
 PMIN2               REAL       Minimum value of this parameter
 PMAX2               REAL       Maximum value of parameter
 HKPAR3              CHAR       Name of housekeeping parameter
                                Enter "!" at any of the HKPAR prompts
                                if no more parameters are required
 PMIN3               REAL       Minimum value of this parameter
 PMAX3               REAL       Maximum value of parameter
  .
  .
  .
 HKPAR10             CHAR       Name of housekeeping parameter
                                Enter "!" at any of the HKPAR prompts
                                if no more parameters are required
 PMIN10              REAL       Minimum value of this parameter
 PMAX10              REAL       Maximum value of parameter

\end{verbatim}\paragraph{INTERVAL}
Because the housekeeping parameters follow the usual laws of
statistics it is common to have small chunks of "GOOD" data in a
large section of "BAD" data. e.g. if you are selecting Master veto
rate data between 0 and 170 only, then there may be a five minute
period where the Master veto rate is constantly above 170, but then
drops to 160 for a 2 second period before rising to above 170 for
another 5 minutes. Rather than include a large number of these
small and probably spurious "GOOD" time windows, a minimum length
of a time window is set by the program. The default is 10 seconds
but this may be overridden on the command line.

\subsubsection{Examples}
To select data with a Master veto rate of less than 170 (MPE PSPC)
\begin{quote}\begin{verbatim}
> XRTHK
RAWDIR - Raw data directory > DISK$SCRATCH:[RDS.HERX1]
ROOTNAME - Rootname of the calibration files > HERX1

Typical parameters to sort on :

    Master veto rate : EE_MV
    Aspect error     : ASP_ERR
    Accepted event rate : EE_AXE

HKPAR1 - Name of HK parameter (! to finish) > EE_MV
EE_MV : ranges from 0 : 603
PMIN1 - Minimum value for this parameter > 0
PMAX1 - Maximum value for this parameter > 170
18 BAD windows - excluding 10.29 % of the data
HKPAR2 - Name of HK parameter (! to finish) > !
\end{verbatim}\end{quote}
\subsection{XRTORB}
Converts the XRT orbit datafile, into a text format
which can be used by the barycentric correction program, BARYCORR.
This program can currently only be used on VAX VMS systems since
it produces ISAM files, not supported by Unix.

The output filename is ATT\_FILE.DAT

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 RAWDIR            CHAR       Raw data directory
 ROOTNAME          CHAR       Rootname of data files
 DIR               LOGICAL    Display directory listing
 OUTPUT            CHAR       Name of input orbit file


\end{verbatim}\subsection{XRTRESP}
Attaches a detector response structure for the ROSAT XRT
PSPC (or HRI). It uses instrument information calculated by
XRTCORR. Hence XRTCORR MUST be run before XRTRESP.

The matrix used should depend on the date of the observation.
Following current advice, the matrix DRMPSPC\_036 should be used for
observations made after mid 1991. The matrix DRMPSPC\_06 should be used
for observations before that date. These defaults is The latest
response matrix. This can be overridden by use of the RESPFILE
parameter.

It can also be used to attach an HRI response matrix. This is a
single channel matrix, just containing effective areas at a number
of different energies. To invoke this mode include the keyword
HRI on the command line.

\subsubsection{Input \& outpu}
The input datafile should be an Asterix spectral dataset.
The response structure is written into the input datafile.

\subsubsection{Radial spectra}
The program processes RADIAL\_SPECTRA by calculating a separate
response for each radial bin using the series of energy corrections
arrays calculated by XRTCORR. Each response is then written into
the output file. This tends to make the output file very large;
each response array takes up about 3000 blocks. If a file with
more than 10 radial bins is used a warning message showing the
approximate size of the output file is issued and the user is
asked whether she wishes to continue.

\subsubsection{Method}
The program uses a detector matrix file, default XRTCAL:DRMPSPC,
to calculate the basic response. To convert this response into an
effective area the response from the matrix is multiplied by the
energy dependent correction array written into the datafile by
XRTCORR. The corrections program should have written an array
MORE.ASTERIX.INSTRUMENT.ENCORR(n).DATA\_ARRAY into the datafile. If
this isn't found then the program will exit with a message to that
effect.

\subsubsection{Scope}
The program can cope with any spectral file as long as it can find
an axis containing corrected pulse height channel information. It
will process datasets which have irregularly spaced pulse height
channel bins.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INPUT       1      CHAR       Input datafile
    -GLOBAL.BINDS (Default binned dataset)
 RESPFILE    2      CHAR       Name of detector response file
 ERASE              LOGICAL    If an energy response structure
                               already exists in the input datafile,
                               do you want to overwrite it ?
 PH_DIM             INTEGER    If the program can't automatically
                               find the pulse height axis, the user
                               is prompted for which axis contains
                               the PH information.
 CONTINUE           LOGICAL    If the output file is going to be larger
                               than 30,000 blocks, do you want to
                               continue ?
 HRI         -      LOGICAL    Attach an HRI response

\end{verbatim}\subsection{XRTSUB}
Subtracts an XRT background file from a source file.

\subsubsection{Changes since V1.6}
The particle model has been updated following the release of a draft paper,
Plucinsky et al. This is valid for observations up to Feb 1992.
The effect is to increase particle counts by $\sim$30\% from previous invocations
of XRTSUB.

\subsubsection{Advice}
Expertise in subtraction techniques is being gained all the time, this
is a summary of current knowledge.

\paragraph{On AXIS point source}
If you are background subtracting a point source close to the optical
axis then you can't go far wrong. Select a background area,
preferably in an annulus around the source, but if the field is too
crowded, in any blank region of the image. The program corrects for
the vignetting effects between the source and background boxes. The
only possible problem is if the background box has to be taken at a
large distance from the source and if there is a strong gradient in
the background. Such a gradient, may be due to either galactic
background effects (for sources at low galactic latitudes) or solar
scattered X-rays. It is trivial to determine whether
such gradients exist, by performing an ICIRCSTATS at various points
on the image. If you HAVE got a gradient, then you may produce two
background files at THE SAME OFF-AXIS angle from the centre and with
the SAME BOX SIZE , but on opposite sides of your source and average
them using MEANDAT. Before using the result in XRTSUB.

\paragraph{Extended sources}
Performing an accurate subtraction of a large source area, such as an
off-axis source or an extended source, needs a bit of care.
The following technique, estimates the particle component of the
background, subtracts this and then vignets the remainder to the
position of the source box using an energy dependent vignetting
correction. This energy dependent correction can only be performed
at the spectral resolution of the datafiles, so consequently the
background dataset should contain several spectral channels to give
an accurate vignetting correction.

The particle models have been parameterised as a function of Master
Veto Rate (MVR). They are valid for MVR values between 0 and about
170. Therefore you should produce a set of "clean" times, when your
data has an MVR between 0 and about 170, using XRTHK. This will reduce
the exposure time by about 10\%, but gets rid of periods of high
particle activity. If you find that too much data is being lost by
excluding the high MVR values, then the high limit may be increased
somewhat. NB: The particle model has only been validated for
observations upto the end of April 1991; for observations after
Sept 1991 it will certainly be inaccurate.

Then produce a source file, using the output file from XRTHK to
define the sort times. The effect of particles is well determined
between pulse height channels 8 and 249 (essentially no photons are
recorded outside this range), so when sorting, you should restrict
the corrected PH range to be between these values. It should be
noted, however, that observations taken after the PSPC HT voltage
was reduced, have useful data between channels 11 and 249. There is a
certain amount of particle leakage at the edge of the detector where
the vetoing is inefficient. Unless absolutely necessary, source boxes
should be wholly within 57 arcminutes of the optical axis. When
subtracting full images, you should be aware that regions close to
the edge are liable to be poorly subtracted. This is the official
line, in practise it seems that there are problems at greater than
fifty arcminutes from the centre. The source file may be a spectrum,
an image or a spectral image (a time axis may be added to any of
these files, however a source time series may only be subtracted
using a background time series, in which case spectral information
is lost and so the vignetting correction will be less accurate.

Next produce a background file, sorted over the XRTHK times.
The background area needs to be as large as possible, to give
good statistics, but should cover only a narrow range of off-axis
angles, to give a well defined position correction . These
contrasting requirements can best be met by using an annular
background box of large radius (an inner radius of 0.5 degrees and
outer radius of 0.6 degrees has been shown to give good results).
The problem then, is that such a box will be disected
by the PSPC support structure and will doubtless include
several bright sources. One solution is to sort all the data
into a spectral image or temporal\_image and than set the quality of
bright sources and the ribs bad using a combination of WELLARD and
ARDQUAL (and possibly IGNORE, CIGNORE and CRESTORE). The program will
then correctly calculate the area of the background region. NB: This
technique minimises the effect of gradients in the background,
however, you should check your subtraction using a local background
if possible.

Now subtract the two files using XRTSUB. The effect of particles is
taken into account by default, although this may be turned off by
setting PARTICLES=NO on the command line. A file containing the
calculated particle spectrum can be produced by setting
PART\_FILE=filename on the command line.

In summary the following may be of use in subtracting a spectrum,
image or spectral image.


\begin{enumerate}
\item Produce a background spectral image. Unfortunately it isn't
correct to produce an annular spectral image with XSORT and then
use this as the background file, because the pixels at the inner and
outer edges of the annulus, which are bisected by the circle are
not totally filled and lead to an incorrect calculation of the area
of the background box. The solution is to sort over a rectangular
field and then mask out certain pixels such that an annular field
of view is achieved. i.e.

\begin{enumerate}
\item Use XSORT to produce a SQUARE spectral image of the field. This
must use the same pulse height channel range as the source file.
If subtracting an image, use about 10 spectral bins, so that the
vignetting function is adequately sampled. NB: If you are
subtracting a spectrum using this technique, the background file
can potentially get very large (e.g. 128*128*240).
\item Mask out the bits of the background file that you don't want.
e.g. if you wanted to use a background annulus with an inner
radius of 0.5 degrees and an outer radius of 0.6 degrees you
should do the following.

\begin{enumerate}
\item Set the whole file bad using IGNORE
\item Set all pixels within the outer radius GOOD using CRESTORE
\item Use WELLARD to define an ARD file containing a description
of the PSPC ribs, the sources that fall within the bckgnd
annulus and a circle centred on 0,0 with a radius of 0.5
degrees. You can toggle the IMODE button to define the
circle using keyboard entry and sources using cursor entry.
\item Set the region specified, bad, by using the created ARD file
in ARDQUAL.
\end{enumerate}
\item At this stage you should check that the masking operation has
worked by looking at the background file using IQDISP.
\end{enumerate}
\item subtract the two files using XRTSUB.
\item You can then set pixels in the subtracted image, bad, by
using WELLARD and ARDQUAL as before.
\end{enumerate}
\subsubsection{Error messages}
The message "Warning: energy of channel n out of bounds" ,
means that the cal. files for your observation dont have an entry
for the channel in question. This is {\em perfectly normal} for channels
1-7 which are {\em dud} channels. If the channel number is different
from 1-7 then there may be some problem with the auxilliary files
in the observation.

\subsubsection{Input \& outpu}
There are several input files:
\begin{itemize}
\item An Asterix binned dataset, containing data taken within a box
centred on the source.
\item A binned dataset containing data taken from a background box
\item Cal files: The effective area file and detector response file
are needed. The default at the prompt line will be the standard
files which are contained in the XRT calibration data directory.
\item If particle rates are being calculated, the program needs the header
file and the eventrate file.
\end{itemize}
The output file contains the source data minus the corrected
normalised background data.

If requested, an HDS file containing the calculated background in the
source box (i.e. source file - subtracted file) may be produced.

\subsubsection{Method}
The details of the method vary slightly, depending on whether the
source file is an image or not, but in all cases the procedure
is as follows:

\begin{enumerate}
\item Open the source and background files and check that they
are compatible. They are compatible if they have been taken
over the same pulse height range and over the same times.
\item Sum the background counts. Background pixels with bad quality
are ignored.
\item Calculate the area of the source and background boxes. If the
background file contains image axes, its area is calculated
by adding up the number of GOOD image pixels and multiplying
by the area of a pixel. Otherwise the area is calculated
using the SPATIAL information in the SORT box of the file.
\item Calculate the expected particle count (as a function of PH
channel) in the background box. This calculation uses the
formula of Steve Snowdon (Ap.J. submitted). It turns out to
be a function of Master Veto Rate which is a function of
time and is contained in the eventrate (\_EVR) file.
\item Calculate the photons in the background box by subtracting
the particles.
\item 'Correct' the background photon counts to the source box
position, using the vignetting functions contained in the
effective area file and do the same for the particles but
using the spatial form in Steve Snowdon's paper. Both of
these corrections are energy dependent and are applied
separately for each spectral bin in the background file.

\item Normalise the background counts for the difference in area
between the two boxes and subtract from the source counts.
\end{enumerate}
Photon vignetting is a function of energy. If the background file
hasn't got an energy axis then by default the program assumes a mean
photon energy of 0.2 keV when it produces the background image.
This may be overridden on the command line. However, if the background
file contains a spectral axis, then each pulse height bin is vignetted
individually to produce the output image. This means that it is
good practise to use either a spectrum or a spectral image as the
background file when subtracting an image (see the `advice' section)
\paragraph{Images}
The background file may consist of any of the X,Y,time or corrected
PH axes. The counts from within the background area are summed and
convolved with the vignetting to produce a background image file.
This image is then subtracted from the source image.

NB: The struts are not included in the background image, but may be
set bad using XSPOKES etc...

\paragraph{Others}
The program will subtract a file containing any of the X,Y, time
or corrected pulse height axes.

Note: to avoid correcting the background for the vignetting difference,
enter PCORR=NO on the command line.

Files containing radial bins have each bin corrected independently
for the vignetting difference from the background box and also for
the different area normalisations.

\subsubsection{Particles}
The program calculates the particle rate using a formula developed
by Steve Snowdon et al ({\em Ap.J. 393,819 (1992)}) and Pluchinsky et al
({\em Ap.J. 418,519 (1993)}). There are three particle components;
external particles, internal detector noise and flourescent Aluminium
X-rays produced within the spacecraft. Each of these components have
different temporal, spectral and positional dependencies, but can be
modelled accurately as a function of a single parameter. This parameter
is the Master veto rate, which is recorded as a function of time in the
eventrate file which comes with each observation. This is only
really important for background subtraction of extended sources.

The program outputs the fraction of the background counts which are
particles. This should be about 1-2\% for open observations and
around 4\% for observations taken with the boron filter.

To create a file containing the particles in the BACKGROUND box
({\em not} the source box), put PART\_FILE=$filename$ on
the command line.

\subsubsection{The HRI}
XRTSUB works on HRI data, but isn't capable of vignetting the
background data to the source position. So XRTSUB should be
invoked with PCORR=NO on the command line.

The program doesn't currently cope with the fact that the area of the
source and background boxes is reduced by hotspots.

\subsubsection{Variance}
XRTSUB will produce a variance array in the output file.
The formula is:

\begin{quote}\begin{verbatim}
VAR(IX,IY,IT,IP) = SVAR(IX,IY,IT,IP) + BVAR(IX,IY,IT,IP) *
                                   (CFACTOR**2 + PFACTOR(IP)**2)
\end{verbatim}\end{quote}
Where VAR is the output variance, SVAR is the source variance, BVAR is
the background variance, CFACTOR is the area normalisation factor and
PFACTOR is the positional correction factor. IX,IY,IT and IP denote the
pixel position in the X,Y,Time and corrected PH axes respectively.

Typically the source and background files won't contain variance arrays
by this stage. In which case poissonian statistics are assumed and the
source and background data values are used in place of their variances
to calculate the output variance array. If a source pixel has a value
of zero then by default its variance will be assumed to be zero, however,
it is possible to override this by setting
SDEFVAR=1.0 (for instance) on the command line. Similarly, empty
background pixels may have their variance set by specifying BDEFVAR=0.8
(for instance) on the command line. BDEFVAR is the default variance for
the background which is actually going to be subtracted from the source
data, e.g. if the background file is a spectral image, it will be binned
into a spectrum before being normalised to the source box position and
subtraction. In this case BDEFVAR will only be used if a bin in the
background spectrum is zero.

\subsubsection{Warning}
XRTSUB assumes that neither the source box nor the background box
overlap any of the struts or the central circle. If this isn't true,
e.g. if your background file is a spectrum which has been taken in
an annulus around the whole image at a large radius, then the program
will overestimate the area of the background box. This problem may
be alleviated by using a spectral image as the background file (see
the advice section on extended objects).

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 SOURCE      1      UNIV       Name of source file
 BCKGND      2      UNIV       Name of background file
 OUT         3      UNIV       Name of output file
 RTNAME             CHARACTER  Rootname of calibration files
 ENERGY             REAL       Mean photon energy
 UPPEN              REAL       Maximum energy for applying position
                               corrections (keV)
 AXTYPE             INTEGER    Type of axis  e.g. X,Y,time,PH
                               Only if an axis is not recognised.
 EFFILE             CHARACTER  Effective area file.
 RESPFILE           CHARACTER  Detector response matrix file
 PCORR              LOGICAL    Correct the background to the source
                               position before subtracting ?
    Hidden default = YES
 PARTICLES          LOGICAL    Calculate the particles and modify the
                               vignetted counts accordingly ?
    Hidden default = YES
 PART_FILE          CHARACTER  Name of output particle spectrum file
                               "!" means dont produce a file
                               NB: This produces the particle spectrum
                               in the background box
    Hidden default = !
 SDEFVAR            REAL       Variance to assign to empty source
                               pixels
    Hidden default = 0
 BDEFVAR            REAL       Variance to assign to empty background
                               pixels
    Hidden default = 0
 BGMODEL            CHARACTER  Name of file to contain the actual
                               background subtracted from the source
                               file. "!" means dont produce a file.
                               Useful if maximum likelihood fitting.
    Hidden default = !

\end{verbatim}\subsubsection{Examples}
To background subtract a time series
\begin{quote}\begin{verbatim}
ICL> XRTSUB
SOURCE - Name of source file > SRC
BCKGND - Name of background file > BACK
OUT - Name of output file > SUB
RESPFILE - Name of response file /'XRTCAL:DRMPSPC'/ >
EFFILE - Name of effective area file /'XRTCAL:PSPCB_EFF'/ >
RTNAME - Rootname for calibration files > QFIELD
Mean position correction factor: 1.01
\end{verbatim}\end{quote}
To subtract an HRI spectrum - MUST turn off the positional correction.
\begin{quote}\begin{verbatim}
ICL> XRTSUB PCORR=NO
SOURCE - Name of source file > SRC
BCKGND - Name of background file > BACK
OUT - Name of output file > SUB
\end{verbatim}\end{quote}
To subtract a PSPC image
\begin{quote}\begin{verbatim}
ICL> XRTSUB
SOURCE - Name of source file > IMA2
BCKGND - Name of background file > BACKANN
OUT - Name of output file > IMSUB
RESPFILE - Name of response file /'XRTCAL:DRMPSPC'/ >
EFFILE - Name of effective area file /'XRTCAL:PSPCB_EFF'/ >
RTNAME - Rootname for calibration files > QFIELD
BIMAGE - Name of Background image > BACKIM
Estimated 222.3 particles in the background box: 1.5 percent of total
background
\end{verbatim}\end{quote}
To produce a particle spectrum file
\begin{quote}\begin{verbatim}
ICL> XRTSUB PART_FILE=PFILE
\end{verbatim}\end{quote}
To produce a file containing the actual background removed and
set the miminum variance from the input background file to be
one.
\begin{quote}\begin{verbatim}
ICL> XRTSUB BGMODEL=REALBGD BDEFVAR=1.0
\end{verbatim}\end{quote}
\subsection{XSORT}
Takes raw XRT event data produced by XRTCONV and sorts them into either
a Binned dataset or an event dataset.
Note these data have been pre-processed into image coordinates
such that north is always up.

\subsubsection{Background subtraction}
Because the XRT is an imaging instrument, it is possible to select
a background area of sky. This may be a rectangle or circle away from
the source box or an annulus around it.

In the annulus method the background RA and DEC should be set to
the same as that of the source box. To avoid including source photons
in the background box the user is prompted for an inner and outer
radius. The inner radius should be made slightly larger than the
source box radius and the outer radius should be made as large as
possible without including any other sources or overlapping a rib
or the central ring structure. It is recommended that you use the
procedure XRTBCKBOX to define a source and background box.

\subsubsection{Changes since V1.6}
PSPC data has been changed to contain PH channels 1-512. XRTSUB, XRTCORR
etc. cannot cope with spectra having channels greater than 256 and so XSORT
now sets the default channel range to 1-256 if a spectrum is being produced
from this data.

Event datasets have their timelists in double precision now.

\subsubsection{Corrections}
All datasets produced will be totally uncorrected. i.e. no exposure
or vignetting correction is applied. Run XRTCORR to perform instrument
corrections.

\subsubsection{Input \& outpu}
If a binned dataset is wanted, the user has to choose which axes are
needed in the output file. For instance, for a time series of images
enter the codes for the X axis, Y axis and Time axis at the BIN\_AXES
prompt. Any combination of the eight axes may be entered. The XDET,
YDET and Uncorrected pulse height axes are of no use in subsequent
processing and should only be used for diagnostic purposes.

\subsubsection{Selecting data ranges}
The user is initially asked to define a source box in celestial
coordinates. This may be either a circular or rectangular box centred
at a particular RA and DEC. Most of the calibration data is defined
assuming a circular source box so ordinarily if a time series or
spectrum of a point source is being produced the source box
should be circular.

The data may then be further subsetted on any of the following five data
properties, X detector coord, Y detector coord, time, uncorrected
pulse height channel or corrected pulse height channel.
The default ranges of these five properties are shown initially
and the user is invited to enter the codes of the ranges to be changed.

NB: X and Y detector coordinates and uncorrected pulse height spectra
are present for diagnostic purposes only. The full range of these
properties would ordinarily be used when sorting the raw data.

The commands XRTBOX and XRTBCKBOX may be of use in selecting source and
background box positions

\subsubsection{The HRI}
HRI observations contain a hotspot/deadspot file, Rootname\_HOT.SDF.
XSORT ignores events which occur in hotspot/deadspot regions.

\subsubsection{Time ranges}
Up to 250 time ranges may be used to sort the data. Times may be input
as either offset times or modified Julian date. MJDs must be prefixed
with an 'M'; times may be seperated by either a space or a colon.
The following are acceptable:

\begin{quote}\begin{verbatim}
  TIMRANGE> 102.3 405.6 1230.0 1500.0
  TIMRANGE> 102.3:405.6 1230.0:1500.0
  TIMRANGE> M48401.3356789 M48401.3367890 M48401.4 M48402.0
  TIMRANGE> M48401.3356789:M48401.3367890 M48401.4:M48402.0
\end{verbatim}\end{quote}
Up to 132 characters may be used on the prompt line to specify time
ranges. Alternatively the name of a file may be given e.g.
\begin{quote}\begin{verbatim}
  TIMRANGE> TIMES.DAT
\end{verbatim}\end{quote}
The file TIMES.DAT must then contain a list of start and stop times.
The recommended format is:
\begin{quote}\begin{verbatim}
  M48401.335678   M48401.336890
  M48401.535678   M48401.636890
  M48401.735678   M48401.936890
         etc....
\end{verbatim}\end{quote}
The same file may be used to select time ranges in the WFC sorting
program, WFCSORT, so that extracting simultaneous data from the
two instruments becomes trivial. In this case the above format for
the time windows, i.e. MJDs seperated by a space, MUST be used.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 RAWDIR       1      CHAR       Raw data directory. Default is the
                                current directory.
                                The directory must contain no more than
                                500 files.
 ROOTNAME     2      CHAR       Rootname of the raw data files. (The
                                                  one used in PREPXRT)
 DATASET      3      CHAR       Type of output files required
                                      Event(E) Binned(B) or both(E,B)
 SRCFILE      4      UNIV       Name of output source file
 BCKFILE      5      UNIV       Name of output background file
 QCHANGE             LOGICAL    Change the default quality limits
    Default = NO
 PROPERTY            CHAR       Quality limits to change
 TEMPMIN             INTEGER    Minimum temperature quality accepted
 TEMPMAX             INTEGER    Maximum temperature quality accepted
 GAINMIN             INTEGER    Minimum gain quality accepted
 GAINMAX             INTEGER    Maximum gain quality accepted
 RANGES              CHAR       Codes of the ranges to change.
                                      (xdet=3,ydet=4,time=5,PH=6,
                                       corrected PH=7).
 XSTART              REAL       Minimum X pixel in source box
 XEND                REAL       Maximum X pixel in source box
 YSTART              REAL       Minimum Y pixel in source box
 YEND                REAL       Maximum Y pixel
 XDSTART             REAL       Minimum X detector pixel in source box
 XDEND               REAL       Maximum X det. pixel in source box
 YDSTART             REAL       Minimum Y det. pixel in source box
 YDEND               REAL       Maximum Y det. pixel
 TIMRANGE            CHAR       Time ranges to use - may be a text file
                                (see TIME_RANGES)
 MINPH              INTEGER     Minimum value of uncorrected PH channel
 MAXPH              INTEGER     Maximum value of uncorrected PH channel
 MINEN              INTEGER     Minimum value of corrected PH channel
 MAXEN              INTEGER     Maximum value of corrected PH channel
 BIN_AXES            CHAR       Axes required in data array (X=1,Y=2,
                                XDET=3,YDET=4,T=5,PH=6,CORR PH=7)
 SHAPE               CHAR       Shape of source area
                                   Circular(C),Rectangular(R),
                                   Annular(A) or Elliptical(E)
 GRP_RA              REAL       RA of the centre of the source box
 GRP_DEC             REAL       DEC of the centre of the source box
 GRP_SWIDTH          REAL       Width of source box (degrees)
 GRP_SHEIGHT         REAL       Height of source box (degrees)
 RADIUS              REAL       Radius of source box in degrees
 RADINN              REAL       Inner radius of source box in degrees
 RADOUT              REAL       Outer radius of source box in degrees
 ORIENT              REAL       Orientation of the ellipse
 EXINN               REAL       Inner X axis of the ellipse
 EXOUT               REAL       Outer X axis of ellipse
 EYOUT               REAL       Outer Y axis of ellipse
 NXBIN              INTEGER     Number of pixels in output X axis
 NYBIN              INTEGER     Number of pixels in output Y axis
 NXDBIN             INTEGER     Number of bins in output det. X axis
 NYDBIN             INTEGER     Number of bins in output det. Y axis
 TIMBIN              REAL       Width of bins in the time axis (seconds)
 PHBIN              INTEGER     Bin width of pulse height axis (channels)
 ENBIN              INTEGER     Bin width of corr. PH axis (channels)
 NRBIN              INTEGER     Number of radial bins
 BACK               LOGICAL     Is a background file wanted (Y/N)
 GRP_RAB             REAL       RA of the centre of the bckgnd box
 GRP_DECB            REAL       DEC of the centre of the bckgnd box
 GRP_BWIDTH          REAL       Width of bckgnd box (degrees)
 GRP_BHEIGHT         REAL       Height of bckgnd box (degrees)
 BRADINN             REAL       Inner radius of bckgnd box in degrees
 BRADOUT             REAL       Outer radius of bckgnd box in degrees
 BORIENT             REAL       Orientation of the bckgnd ellipse
 BEXINN              REAL       Inner X axis of the bckgnd ellipse
 BEXOUT              REAL       Outer X axis of bckgnd  ellipse
 BEYOUT              REAL       Outer Y axis of bckgnd ellipse

\end{verbatim}\paragraph{GRP RA}
The RA of the centre of the source box. The default is the centre
of the XRT field of view (decimal degrees).

\paragraph{GRP DEC}
The DEC of the centre of the source box. The default is the centre
of the field of view (decimal degrees).

\paragraph{GRP RAB}
The RA of the centre of the background box. The default is the same
centre as was selected for the source box.

\paragraph{GRP DECB}
The DEC of the centre of the background box. The default is the same
centre that was selected for the source box.

\paragraph{RANGES}
By default the full range of X,Y detector pixel,time and pulse height
data will be used.
To select data from a subset of any of these properties enter the integer
code for that axis (xdet=3, ydet=4, t=5, PH=6, corr. PH=7). Enter "!" to
accept all the ranges shown.

\paragraph{BRADINN}
Inner radius of the background annulus. The software works by first
testing if an event lies within the source box and IF NOT then testing
if it lies in the background box. Therefore the inner radius of the
background box MUST be bigger than the radius of the source box.

\paragraph{BRADOUT}
Outer radius of the background annulus. You should try and ensure that
no sources are included in the background annulus and that the box
does not include any portion of the central ring or supporting rib
structure.

\subsubsection{Examples}
The following example produces a source spectrum and a background
spectrum taken from an annulus about the source.
\begin{quote}\begin{verbatim}
ICL> XSORT
XSORT Version 1.7-0
RAWDIR - Raw data directory > DISK$SCRATCH:[ASTERIX88.ARLAC.DATA]
Found 2 data files on  DISK$SCRATCH:[ASTERIX88.ARLAC.DATA]
DISK$SCRATCH:[ASTERIX88.ARLAC.DATA]ARLAC.SDF;1
DISK$SCRATCH:[ASTERIX88.ARLAC.DATA]ARLAC_HDR.SDF;1

ROOTNAME - Rootname of files to sort /'ARLAC'/ >

OBSERVATION  DATA-STARTS   DATA-ENDS   EVENTS   FILT_STAT
                 53.0        6150.0     30611     OFF

DATASET - Output files required Event(E) Binned(B) /'B'/ >

Binned dataset selection
------------------------
   1) XPIX
   2) YPIX
   3) XDET
   4) YDET
   5) Time
   6) PHA channel
   7) Corrected PHA channel

BIN_AXES - Enter integer codes of axes required in data array /'5'/ > 7

SHAPE - Shape of source area Circular(C),Rectangular(R) > C
GRP_RA - RA of the centre of the source box /332.02334594727/ > 332.173
GRP_DEC - DEC of the centre of the source box /45.501556396484/ > 45.7393
Source box centre  azimuth =   -752  elevation =  -1712

RADIUS - Radius of source box (degrees) /1.171111/ > 0.05
  Data array axes
*******************
   3) XDET:      0 to    8192 (pixels)
   4) YDET:      0 to    8192 (pixels)
   5) Time:      53.000 to     6150.000 (seconds)
   6) PHA channel:      1 to     256 (chn.)
   7) Corr. PHA chan.      1 to     256 (chn.)

RANGES - Property ranges to change (enter numbers) /'7'/ > 7

MINEN - Minimum value of energy /8/ >
MAXEN - Maximum value of energy /256/ >
ENBIN - Bin width of corrected pulse height axis /1/ > 1
There will be 249 energy bins, each of 1 raw channel, starting from 8
BACK - Is a background file wanted (Y/N) /TRUE/ >
GRP_RAB - RA of centre of bckgnd box (dec. degrees) /332.173/ >
GRP_DECB - DEC of the centre of the bckgnd box /45.7393/ >
BRADINN - Inner radius of bckgnd box (degrees) /5.0000001E-02/ > 0.1
BRADOUT - Outer radius of bckgnd box (degrees) > 0.2

SRCFILE - Output source filename /@SRCS/ >
BCKFILE - Output background filename /@BGDS/ >

Processing observation DISK$SCRATCH:[ASTERIX88.ARLAC.DATA]ARLAC.SDF
\end{verbatim}\end{quote}
\subsection{XSPOKES}
An application which defines the spoke/rib structure on an XRT
image in a spatial description (ARD) text file.
This file may subsequently be used to set the quality of the pixels under the
spoke structure bad. Essential if analysis (e.g. radial profiles)
of structures extending outside the central ring is being undertaken.

Optionally the region, outside the circular field of view may be
included in the file produced by XSPOKES.

\subsubsection{Method}
There are two methods of operation. For most observations an attitude
file will be available and will give an unambiguous value for the
roll angle. The program can then use this value, together with
the geometry of the window support structure to reconstitute the spokes
on the image. The routine uses the mean roll angle contained in the
attitude file (but excludes zero rolls from the mean calculation as
these are usually corrupt entries). This is MODE=C.

Occasionally, the attitude file is useless for defining
the roll angle. This can be either because it is corrupt or because
the observation has been completed in more than one session so that
more than one roll angle is present in the data. In this case the
spoke structure may be defined using the cursor, MODE=I, as below :

\begin{enumerate}
\item Display the XRT image of interest on an interactive graphics device
with IDISPLAY.
\item Type XSPOKES from within ICL.
\item Select whether to include the region outside the circular field
of view of the PSPC in the spatial file. By default
the radius of the circular field is set to 0.95 degrees. This
may be altered on the command line.
\item Use the cursor to select the centre of an intersection of
a spoke and the central ring.
\item Use the cursor to select the centre of the intersection of
the opposite spoke and the central ring.
\item The routine will now use these two points to reconstruct the
XRT support structure. It will draw an outline of the
structure on the image.
\item A textfile will now be produced containing the spatial
definition of the support structure.
\end{enumerate}
This file may be used to set the quality of the pixels
under the support structure bad.

\subsubsection{Input \& outpu}
Input is either an attitude file or an image displayed on an
interactive graphics device by a PGPLOT based routine such as IDISPLAY,
depending on the run mode.

Output is a textfile containing a spatial description of the region
selected (an ARD file).

\subsubsection{Widening the pattern}
The width of each spoke and the central ring is set to 0.06
degrees (3.6 arcminutes) by default. These can be enlarged by setting
the EXTRA\_WIDTH parameter on the command line.

\subsubsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 ROOTNAME            CHAR       Rootname of datafiles
 ARDFILE             CHAR       Name of output text file
 NEW                 LOGICAL    Create a new file (Y) or add to
                                an existing ARD file (N)
 OUTSIDE             LOGICAL    Include region outside f.o.v. ?
 MODE                CHAR       'C'alculate or 'I'nteractive mode
 ATTFIL              CHAR       Name of attitude file.
 TRAD                REAL       Radius of field of view in degrees
    Hidden default = 0.95
 DOPLOT              LOGICAL    Plot the spoke structure on a
                                displayed image ? (MODE=C only)
    Hidden default = YES
 EXTRA_WIDTH         REAL       Width to extend the ring and spoke
                                widths by (degrees)
    Hidden default = 0.0
 TIMRANGE            CHAR       Filename containing list of good time
                                slots. Create this file using XRTHK.
                                (see TIME_RANGES)

\end{verbatim}\paragraph{EXTRA WIDTH}
The default diameter of the central ring and width of the
spokes is 0.06 degrees. In some cases the apparenmt width can be greater
than this value. Enter EXTRA\_WIDTH=nnn on the command line to
broaden the pattern. This is in degrees, so e.g. EXTRA\_WIDTH=0.016
will broaden the pattern by 1 arcminute.

\chapter{Textual display commands}
These commands display information about or in datasets, in a
neat textual format.

See also General\_HDS\_commands for additional ways to look at your
raw HDS datasets, and Graphics\_display\_commands for plotting them.

All these commands use the DEV parameter to direct their output.
See text output for more information.

\section{AXSHOW}
Displays the attributes of axes of a dataset.

AXSHOW uses the DEV parameter to direct output. See text output
for more information.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type      Description
 -------  ----   ----      -----------
 INP       1     UNIV      Input binned dataset
           GLOBAL.BINDS (Default binned dataset)
 DEV       -     CHAR      text output device. Can take values of
                           (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                           or (N)EWFILE, otherwise is interpreted
                           as a filename.
    Hidden default 'TERMINAL'

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> AXSHOW CCF
Axis Label           Size  Range                         Reg Widths

  1  X_CORR            85  69.5 to -14.5 arcmin           Y  Absent
  2  RAW_TIMETAG       94  86424.66 to 1169395 second     N  Non-uniform
  3  Y_CORR            65  -14.5 to 49.5 arcmin           Y  Absent
\end{verbatim}\end{quote}
\section{BINLIST}
Lists the data contained in a 1D binned dataset.

A subset may be specified, although the default is the whole
dataset (with large datasets it is probably advisable to use the
PROMPT keyword).

BINLIST uses the DEV parameter to direct output. See text output
for more information.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type      Description
 -------  ----   ----      -----------
 INP       1     UNIV      Input binned dataset
          GLOBAL.BINDS  (Default binned dataset)
 DEV       -     CHAR      text output device. Can take values of
                           (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                           or (N)EWFILE, otherwise is interpreted
                           as a filename.
    Hidden default 'TERMINAL'
 SLICE     -     CHAR      Subset specification
    Hidden default '*'

\end{verbatim}\subsection{Examples}
List elements 1 to 50 for dataset DS
\begin{quote}\begin{verbatim}
> BINLIST  DS SLICE=1:50    -
\end{verbatim}\end{quote}
Send whole of DS to printer
\begin{quote}\begin{verbatim}
ICL> BINLIST DS DEV=PRINTER
\end{verbatim}\end{quote}
\section{EVLIST}
Displays the DATA\_ARRAY component of all the LISTs in an event
dataset.

EVLIST uses the DEV parameter to direct output. See text output
for more information.

\subsection{Input output data}
Input: Event dataset
Output: to user-specified device
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn   Type       Description
 -------  ----   ----       -----------
 INP        1    UNIV       Input dataset.
            GLOBAL.EVDS (Default event dataset)
 SUBSET     2    INTEGER    Subset of items to be printed.
 DEV        -    CHAR       text output device. Can take values of
                            (T)ERMINAL,(C)ONSOLE,(P)RINTER,(O)LDFILE
                            or (N)EWFILE, otherwise is interpreted
                            as a filename.
    Hidden default 'TERMINAL'
\end{verbatim}\section{HEADER}
Displays and optionally edits the header information within
an event or binned dataset.

HEADER uses the DEV parameter to direct text output

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type      Description
 -------   ----  ----      -----------
 INP         1    UNIV      The input container dataset
 DEV         2    CHAR      Destination of text output
     Hidden default = TERMINAL
 EDIT        -    LOGICAL   Whether editing required
     Hidden default = NO
    (The following all have hidden default=! ie no change)
 TARGET      -    CHAR
 OBSERVER    -    CHAR
 OBSERVATORY -    CHAR
 INSTRUMENT  -    CHAR
 AXIS_RA     -    DOUBLE
 AXIS_DEC    -    DOUBLE
 FIELD_RA    -    DOUBLE
 FIELD_DEC   -    DOUBLE
 POSITION_ANGLE   DOUBLE
 EQUINOX     -    INT
 BASE_DATE   -    CHAR
 BASE_MJD    -    INT
 BASE_UTC    -    DOUBLE
 BASE_TAI    -    DOUBLE
 OBS_LENGTH  -    REAL
 EXPOSURE_TIME    REAL

\end{verbatim}\subsection{Example}
Display header info for dataset TIME
\begin{quote}\begin{verbatim}
> HEAD TIME
\end{verbatim}\end{quote}
Change the OBSERVER component
\begin{quote}\begin{verbatim}
> HEAD IMG EDIT OBSERVER="Me"
\end{verbatim}\end{quote}
Prompt for each component
\begin{quote}\begin{verbatim}
> HEAD IMG EDIT PROMPT
\end{verbatim}\end{quote}
\section{SSDUMP}
Displays the contents of source search results files. A full
description of this command can be found under 'Source Search
Commands'.

\chapter{Time series processing}
These applications are principally designed for the analysis of
data in the form of time series. Many of them are, however,
applicable to any one dimensional dataset.

\section{Advice}
There are several flavours of time series routine for each
application area. Please see USER\_006 in AST\_DOCS: (VMS) or
\$AST\_DOCS (UNIX) for more details of the applicability of
individual routines.
\subsection{POWER SPECTRUM}
If the dataset consists of regularly spaced data with no points
having bad quality then the best results will be obtained using
the standard FFT algorithm in the application POWER. This is quick
and accurate.

For data containing gaps or uneven sampling you must use the
aplication LOMBSCAR, which employs the LOMB-SCARGLE algorithm, or
possibly, SINFIT, which performs a least squares fit of sine waves
to the data.

The document USER\_006 also gives an account of the performance
of a CLEAN algorithm. This is available from Harry Lehto at
Southampton (username: hl@phastr.soton.ac.uk). Interested users should
contact him directly.

\subsection{Folding}
Asterix contains three folding routines, FOLDLOTS, FOLDAOV and
FOLDBIN. FOLDLOTS is a standard period search technique and can
be used on any dataset. FOLDAOV uses the analysis of variance
technique which is theoretically superior at detecting sharp
periodic signals in small sample datasets. Finally if the period
is known, FOLDBIN, will fold an N-dimensional dataset at a given
period.

\section{ACF}
ACF calculates a 1 dimensional autocorrelation function.

If the dataset has more than one dimension, then the user must
select along which axis the autocorrelation is to be performed.
The output will contain an autocorrelation along this axis at
each point specified by the other axes. Thus ACF can be run on an
image series to produce an autocorrelation function at each point
in the spatial plane.

Note that ACF can only autocorrelate along regularly binned axes.

\subsection{Input Output Data}
Input: N dimensional dataset
Output: N dimensional dataset with 1 axis converted to ACF.
\subsection{Method}
For WEIGHTED case uses method given in Weisskopf, Kahn, and
Sutherland ({\em Ap.J. 199, L147 (1975)}). This is corrected for
the extra statistical noise contribution at zero lag, and
can result in estimated ACF values > 1. The {\em unweighted} case
uses a similar method, omitting the weights.

UNBIASSED results are normalized by * 1 / (NPTS - LAG)
BIASSED results are normalized by * 1 / NPTS (suppressing large
lags).

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Input dataset
   - GLOBAL.BINDS the current binned dataset
 OUT       2    CHARACTER   Output dataset
   -> GLOBAL.BINDS the current binned dataset
 AXIS      -    INTEGER     Index number of axis to perform
                            autocorrelation along. Note, only
                            used if dataset has more than 1
                            dimension!
   Suggested default is the TIME axis (if present).
 MXLAG     3    INTEGER     Maximum lag to calculate ACF at.
   Suggested default is the maximum allowed value
 BIAS      -    LOGICAL     Bias the ACF? i.e. multiply the ACF
                            by (N - LAG) / N, where N is the
                            number of values in the axis along
                            which the ACF is being found.
 WEIGHT    -    LOGICAL     Use variance and/or quality information
                            if present?
   Hidden default = Yes

\end{verbatim}\section{BARYCORR}
Performs Barycentric corrections to both binned and event datasets.
A ROSAT style orbit file is required to correct for satellite motion,
(VMS only). The program performs earth centred correction otherwise.
An output dataset is created and the corrections are made to that file,
the input file is unmodified.

\subsection{Simple-Mode}
This option allows one barycentric correction to be calculated and
applied to the header components of the file. The central time of the
observation is used to calculate the appropriate barycentric correction.
The user is warned about the size of the differential barycentric
correction that applies to the dataset. If that uncorrected differential
shift is unacceptably large, then the program should be rerun with
SIMPLE\_MODE returned as false.

\subsection{Earth-centred-corrections}
If earth based corrections are wanted then, the barycentric offset is
calculated as if the observation was made from the centre of the Earth
at the time of the first event in the dataset.

If higher precision corrections are desired, then specify a small
UPDATE\_INT. For guidance, internal consistency of +/- 0.1 second
requires UPDATE\_INT to be set to 1000.0

\subsection{Method}
The barycentric corrections are applied directly to the RAW\_TIMETAG
list in the input file, the time axis or the header components as
appropriate.

The satellite Orbit file should be produced by either
ATT\_POS\_SAT for ROSAT WFC data or XRTORB for ROSAT XRT data.

If the Orbit file is available and used, the default update
period to the calculation is the frequency of the updated
orbit data. This applies to VMS only.

The algorithm used is based on {\em Stumpff Astron \& Astrophys 1979}.
\subsection{The Orbit File}
An orbit file is required if the barycentric corrections are to be
applied to the position of the satellite. The format required is
that produced by ATT\_POS\_SAT (WFC) or XRTORB (PSPC/HRI).
The file must cover the entire range of timetags in the event dataset.

\subsection{Unix-differences}
The orbit file format chosen for Rosat is not supported on UNIX,
consequently the satellite motion cannot be taken into account.
This will be fixed in the future.

\subsection{Event-lists-and-binned-datasets}
If you do not want SIMPLE\_MODE then the barycentric corrections are
applied to the RAW\_TIMETAG list (event dataset) or to the time axis
(binned dataset). If the input dataset is a regularly spaced binned
dataset, then the output will be an irregularly spaced binned dataset,
due to differential barycentric corrections being appropriate
throughout the dataset.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    UNIV        Any binned or event dataset
                            with a time axis or raw_timetag list
     - GLOBAL.EVDS ( Default event dataset )
 OUT       2    UNIV        output dataset
 SATCORR        LOGICAL     Correct for the satellite orbit ?
                            (only available on VMS)
 SIMPLE_MODE    LOGICAL     One correction applied to the header
                            components of the file.
 POS_FILE       CHARACTER   Name of the satellite orbit file
 GO_ON          LOGICAL     Ignore invalid Orbit file and
                            perform Earth-centred correction
 UPDATE_PRD     REAL        Period after which correction is
                            re-calculated.
                            The default is Dynamic and equal
                            to the orbit file update period
 UPDATE_INT     REAL        If there is no orbit file, then
                            a recalculation of the barycentic
                            correction is performed if the time
                            tags change by more than this value.
                            (The MJD changes --- the  Earth moves)

\end{verbatim}\section{CROSSCORR}
CROSSCORR computes the cross-correlation of two equal length series
Y and Z which may both come from the same file or from different
files. Primitive arrays are also accepted.

If the series are not of equal length then the longer is
truncated.

\subsection{Deficiencies}
The FFT is not used; this involves a time penalty for large datasets.

\subsection{Input Output Data}
Input: 2 data arrays

Output: cross-correlation of above

\subsection{Method}
The cross-correlation calculated is the biased version, which has
lower variance than the unbiased estimator. It reduces towards
zero as the lag increases.

If data variances are available then these may be used to weight
the data values, and the expected noise contribution can be
removed from the denominator of the cross-correlation to give an
estimate of the source CCF rather than the data CCF - see e.g.
Weisskopf et al, {\em Ap.J.199, L147}.
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP1      1    CHARACTER   Input object 1
 INP2      2    CHARACTER   Input object 2
 OUT       3    CHARACTER   Output object
    -> GLOBAL.BINDS (the current binned dataset)
 LAG       4    INTEGER     Maximum lag to be computed
 WEIGHT    -    LOGICAL     Use data variances to weight calculation?
 NOISE     -    LOGICAL     Remove noise variance from calculation?

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> CROSSCORR
Input object 1 > LIGHT_CURVE_1
Input object 2 > LIGHT_CURVE_2
Maximum lag to be computed (in bins) > 10
Weighted cross-correlation? > Y
Remove expected noise bias? > Y
Output object > CROSS_CORR_DATA
\end{verbatim}\end{quote}
\section{CROSSPEC}
This computes the cross-spectrum of two 1 dimensional datasets
INP1 and INP2. These may both come from the same file or from
different files.

If the series are not of equal length then the longer is
truncated. The data are then extended by adding zeroes to obtain
a suitable length dataset for the transformation.

The extended data array may be tapered at the ends to reduce
'leakage' in the power spectra. If tapering is requested then,
as a default, a cosine bell is applied to 20\% of the data points
at {\em each} end of the data array. This may be altered using the
appropriate key word. The resulting spectral estimates will
fluctuate wildly unless smoothed. A Gaussian sigma of 2 or 3 may
be suitable for light smoothing.

Alignment of the two input arrays is advisable if one lags the
other. A shift of $+n$ will shift the first series forward with
respect to the second by $n$ elements. This is what is required if
their cross-correlation peaks at positive lags.

\subsection{Deficiencies}
The calculated errors in coherency and phase are underestimated
at both ends of the spectrum (by a factor root 2 at the very
ends) since the assumed symmetry of the spectrum about its ends
results in fewer statistically independent points being smoothed
over in these regions.

The program cannot make allowance for bad data - the user is
warned if any bad quality points are present. Bad values can be
replaced by interpolated values using INTERPOLATE.

\subsection{Method}
See e.g. Bloomfield - "Fourier Analysis of Time Series: An
Introduction" Power and cross-spectra are calculated using a NAG
FFT routine, these are then smoothed with a truncated Gaussian
(truncated at 2.5 sigma). A cosine bell taper can be applied to
the input data to reduce leakage in the spectrum, though it will
also reduce the total power somewhat.

NOTE: The phase spectrum has a negative slope when INP1
leads INP2.
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type  Description
 -------  ----  ----  -----------

 INP1      1    TEXT   First input object
 INP2      2    TEXT   Second input object
 OUT       3    TEXT   Output object
    -> GLOBAL.BINDS (the current binned dataset)
 SIGMA     4    INT    Sigma for smoothing Gaussian
 SHIFT     5    INT    Alignment shift between INP1 and INP2
 TAPER     -    LOG    Taper data?
    Hidden default = YES

 FRAC      -    REAL   Fraction of data tapered at EACH end
    Hidden default = 0.2

\end{verbatim}\section{DIFDAT}
DIFDAT differences adjacent data points in an object, creating
a new dataset with axis and data\_array components. The
aim is to reduce low frequency noise in a time series. If a
quality component exists, DIFDAT will ignore the bad points,
warning the user that bad data is present, calculating the
output data value by difference*(mean spacing/gap). DIFDAT
also works on primitive datastets and HDS files with irregular
axes, although in this case DIFDAT is less applicable and the
user is warned that it is irregular.
\subsection{Input Output Data}
Input: Asterix NDS or primitive
Output: 1D Asterix NDF
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Input data object or structure
 OUT       2    CHARACTER   Output data structure

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> DIFDAT
DIFDAT Version 1.0-1
INP - Enter the name of the input object/file > time_series
OUT - Enter the name of the output file > differenced_data
\end{verbatim}\end{quote}
\section{DYNAMICAL}
DYNAMICAL calculates the power spectrum of successive segments
(of length 2**N) of a 1D data array subclass object (e.g. time
series) and stacks the resulting spectra to form a dynamical
power spectrum.

The normalization of the power spectra is:

\begin{quote}\begin{verbatim}
     power = (wave amplitude/2)**2 = F.T.of autocovariance.
\end{verbatim}\end{quote}
The zero frequency power is omitted from the output array since
it tends to dominate plots if included, and makes taking of logs
difficult if zeroed.

\subsection{Deficiencies}

\begin{enumerate}
\item The data are assumed to be regularly spaced, and
data errors are ignored, as is data quality.
\item Does not work for primitive input objects
\end{enumerate}
\subsection{Input Output Data}
Input: HDS structure containing a 1D DATA\_ARRAY primitive

Output: HDS structure containing power spectrum of input

\subsection{Method}
Fast FFT routine (FFA8) is used for speed. Arrays are mapped so
there is no size limitation.
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP       1    CHARACTER   Input data object
 OUT       2    CHARACTER   Output object
    -> GLOBAL.BINDS (the current binned dataset)
 SECTOR    3    INTEGER     Sector size for FFT

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> dynamical
DYNAMICAL Version 1.0-0
Enter name of input file > TIME_SERIES  ! Enter name e.g. TIME_SERIES
Data object is of type TIME_SERIES      ! Informs user of input type
82 data points entered                  ! Informs user of No of points
Data will be split into sections and transformed - enter number of
points in each section (this will be rounded to nearest N**2)
Enter sector size for FFT > 20          ! Enter No. e.g. 20
Data will be split into 5 sections of length 16
Enter name of output file >
\end{verbatim}\end{quote}
\section{EVPHASE}
This program adds a PHASE list to an event dataset containing
a RAW\_TIMETAG list given the ephemeris of a periodicity.
The user enters the coefficients of the ephemeris. These are:
COEFF1 - Julian date of an occurrence of phase zero (days)
COEFF2 - The periodicity at the time of COEFF1 (days)
COEFF3 - dP/dt, unitless first derivative of period wrt time
\subsection{Julian Date}
The Julian date can be found for a specified calendar date from the
Astronomical Almanac. Modified Julian date (MJD) is defined as
\begin{verbatim}
     Julian date - 2,400,000.5
\end{verbatim}
The MJD at the start of observation can be found using the HEADER
application for any dataset.

The MJD of 1st January 1972 is 41317.0.

\subsection{Input Output Data}
Input: ASTERIX event dataset, with a time list.

Output: Copy of input with phase list added

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Input dataset
    - GLOBAL.EVDS (the current event dataset)
 OUT       2    CHARACTER   Output dataset
    -> GLOBAL.EVDS (the current event dataset)
 COEFF1    3    DOUBLE      Ephemeris coefficient 1
 COEFF2    4    DOUBLE      Ephemeris coefficient 2
 COEFF3    5    DOUBLE      Ephemeris coefficient 3

\end{verbatim}\subsection{Example}
\begin{quote}\begin{verbatim}
>EVPHASE
   EVPHASE Version 1.5-0
   Enter name of input file > EV
   Enter name of output file > EVP
   Enter Ephemeris Coeffs : a(1) + a(2)*N + a(3)*N*N
   Enter first Ephemeris coefficient > 2445296.3065
   Enter second Ephemeris coefficient > 1.98315
   Enter third Ephemeris coefficient > -1.35E-8
\end{verbatim}\end{quote}
\section{FOLDAOV}
A period folding technique which employs the analysis of variances
method. It is especially useful for finding sharp periodic signals
from sparse datasets with few data points.

\subsection{Input \& outpu}
Input file is a 1-d Asterix NDF. Bad pixels should have their quality
set bad.

There are two output files, one contains the periodogram in terms
of power v period and the other contains the data folded at the
best period into a phase file.

The periodogram is displayed on a graphice device during execution.

\subsection{Method}
FOLDAOV uses the analysis of variances technique, a full description
of which is given in Schwarzberg-Czerny, {\em MNRAS 241, 153, 1989}
The Asterix application is an adaption of a code written by David
Holmgren (currently resident at QUVAD).

This technique is commonly available in commercial statistics
packages under the name of ANOVA. A justification for the use
of this algorithm is given in Davies, {\em MNRAS 244, 93, 1990}, where
it is shown that the AOV statistic is more valid than those
commonly used in epoch-folding programs and has the F probability
distribution.

Initially the data is screened and bad quality time bins rejected.
The remaining data points are then passed through to the algorithm.

The technique has the advantage of allowing any number of phase
bins to be used without oversampling the data.

It doesn't weight the data in any way.

When the periodogram has been calculated, it is displayed on a
graphics device of the users choice. The periodogram and folded
data are then saved to two separate NDFs.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      UNIV       Name of the input time series
 PERIOD      2      REAL       Initial period to begin folding
 PINC        3      REAL       Period increment
 NPER        4      INTEGER    Number of periods to try
 NPBIN       5      INTEGER    Number of phase bins
 PER         6      UNIV       Name of output periodogram
 FOLD        7      UNIV       Name of output folded file

\end{verbatim}\subsection{Examples}
To fold a file between 1 and 5 seconds
\begin{quote}\begin{verbatim}
ICL> FOLDAOV
FOLDAOV Version 1.5-1
INP - Input datafile > LTIME
*Variance not present in input datafile*
Using 12 of the 16 input data values
PERIOD - Initial search period /0/ > 1
PINC - Period increment > 0.01
NPER - Number of periods > 400
NPBIN - Number of phase bins > 6
Best fit period 1.276 with amplitude 0.1583877
Graphics device/type (? to see list): VWS
PER - Name of output periodogram file > XPER
FOLD - Name of output phase file > XFOLD
\end{verbatim}\end{quote}
\section{FOLDBIN}
FOLDBIN folds n-D data into phase bins and generates a folded data
set containing the bin means (weighted if required) and standard
deviations, also the chi-squared fit to a constant level.
NOTE: If axis zero is defined then this must be
incorporated in the phase zero epoch.
NOTE: The program will run quickest if the
time axis is the first axis.
\subsection{Input Output Data}
Input: HDS structure containing n-D DATA\_ARRAY

Output: HDS structure containing n-D DATA\_ARRAY

\subsection{Method}
The basic technique is straightforward. Arrays are mapped so
there is no size limitation. Bad quality data are excluded.

Note that if the binning process results in a phase bin with
less than 2 occupants, then that bin is ignored in the chi-
squared calculation. This is because no variance for that
point is available.

The option for weighted or non-weighted mean is included
because whilst a weighted mean will give smaller errors,
it will also tend to drag the value of each phase bin down
if the errors are poissonian because of the way the small
variances are associated with low count rates, somewhat
artificially.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Input data object
    - GLOBAL.BINDS (the current binned dataset)
 OUT       2    CHARACTER   Output data object.
    -> GLOBAL.BINDS (the current binned dataset)
 PERIOD    3    REAL        Folding period.
 EPOCH     4    DOUBLE      Epoch of phase 0. This is the atomic
                            time of phase zero in days.
 BINS      5    INTEGER     Number of phase bins.
 WEIGHT    -    LOGICAL     Whether weighting is
                            required.

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> FOLDBIN
FOLDBIN Version 1.0-5
Enter name of input file > TIME_SERIES
Enter name of output file > FOLDED_DATA
Data is 1 dimensional
Enter period of phase bin >         ! Enter appropriate value
Enter epoch of phase zero /0.0/ >   ! A default value of 0.0 is provided
Enter number of phase bins >        ! Enter appropriate value
Do you want a weighted mean? /NO/ > ! Not recommended to use weighted
\end{verbatim}\end{quote}
\section{FOLDLOTS}
A period search program employing the same standard folding technique
as FOLDBIN. This program produces a periodogram of the chi-squared
fit to a constant for each fold period and a file containing the
data folded at the best period.

\subsection{Input \& outpu}
Input file is a 1-d Asterix NDF. Bad pixels should have their quality
set bad.

There are two output files, one contains the periodogram in terms
of power v period and the other contains the data folded at the
best period into a phase file.

\subsection{Method}
See the help for FOLDBIN for a description of the method employed.

The data may be left unweighted or be weighted by the variance array.
In general it is best to weight the data if there are many counts per
time bin (> 10 ?) but otherwise the data should be left unweighted.
It is worth using both methods to test if features in the resultant
periodogram are real.

The data is folded over a number of periods and the chi-squared
fit to a constant determined for each period. The period giving
the highest chi-squared is deemed to be the best and the
data is folded at this period and written into an output file.
A further file comprising chi-squared v period is also produced.

Each phase bin in the folded file has an associated variance.
This is given by :
\begin{verbatim}
 Unweighted:  Var(bin) = SUM (( Val(i) - mean(bin) ) ** 2 / (N*(N-1)))
\end{verbatim}
Where Val(i) is the value of a given data sample, mean(bin) is the
average value of data samples contributing to this phase bin and
N is the number of data samples in the phase bin.

\begin{verbatim}
 Weighted: Var(bin) = 1.0 / SUM ( 1.0 / Var(i) )
\end{verbatim}
Where Var(i) is the variance of a data sample contributing to this
phase bin.

Note: In both cases the variance of a phase bin is inversely
proportional to the number of data samples in the bin.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Position  Type       Description
 -------  --------  ----       -----------
 INP         1      UNIV       Name of input 1-d datafile
 FOLD_OBJ    2      UNIV       Name of output fold file
 CHI_OBJ     3      UNIV       Name of output periodogram
 PERIOD      4      REAL       Initial period
 PINC        5      REAL       Period increment
 NPER        6      REAL       Number of periods
 EPOCH       7      DOUBLE     The atomic time of a zero in the phase
 NPHASE      8      INTEGER    Number of phase bins
 WEIGHTED           LOGICAL    Weight the data ?

\end{verbatim}\subsubsection{NPHASE}
The number of phase bins. The maximum number of phase bins for
ordinary sampling is given by the minimum period divided by the
time bin size. e.g. if the data is binned in 1 second bins and
the minimum period is 4 seconds then the maximum number of phase
bins for standard sampling is 4. It is valid to oversample datasets,
i.e. set NPHASE to greater than this maximum, if the dataset is long
and contains many counts. However, oversampling may well produce spurious
peaks in the periodogram at integer multiples of the bin width.

\subsection{Examples}
To search for the period in a datafile
\begin{quote}\begin{verbatim}
ICL> FOLDLOTS
FOLDLOTS - V1.5-1
INPUT - Enter name of input datafile > LTIME
*Variance not present in input datafile*
Using 12 of the 16 input data values
FOLD_OBJ - Enter name of output fold file > XFOLD
CHI_OBJ - Enter name of output chi-sq v period file > XPER
PERIOD - Enter start period for folding /1/ > 4
PINC - Period increment > 0.01
NPER - Enter no. of periods to fold at > 400
EPOCH - Enter epoch of phase zero /6743.42/ > ?
Enter in atomic time, The default value is the BASE_TAI in the header
EPOCH - Enter epoch of phase zero /6743.42/ >
BINS - Number of phase bins > 4
WEIGHTED - Are weighted means required? > NO
Best fit period = 7.18 seconds
Chi-sq value of best fit to mean = 307.82
\end{verbatim}\end{quote}
\section{LOMBSCAR}
A LOMB-SCARGLE invocation of the FFT. This is especially useful
for producing a power spectrum of data with gaps.

\subsection{Input \& outpu}
The input file is a 1-d NDF.

The output file is a 1-d power spectrum.p.
The power spectrum is displayed on a graphics device of the users
choice.

\subsection{Method}
This routine uses the PRESS \& RYBICKI implementation of the
LOMB-SCARGLE algorithm, see PRESS and Rybicki, {\em Ap.J., 338, 277}.
It is an adaption of a code written by David Holmgren
(currently resident at QUVAD).

Bad quality data are initially excluded from the algorithm
before processing.

The frequency of the highest peak found is output to the screen,
along with its amplitude and an estimate of the significance of
the peak. A small significance indicates that the peak is likely
to be real, e.g. 'significance 0.01' means there is a 99 percent
chance that the peak is real.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Input dataset
    - GLOBAL.BINDS (the current binned dataset)
 OUT       2    CHARACTER   Output dataset
    -> GLOBAL.BINDS (the current binned dataset)
 OFAC      3    REAL        Oversampling factor
 HIFAC     4    REAL        Factor to extend the high frequency
                            range to e.g. HIFAC=2 goes to twice
                            the Nyquist frequency
 WFREQ     5    REAL        Frequency of window function - hidden
                            default is 0.0 which means don't use a
                            window function

\end{verbatim}\subsubsection{OFAC}
The oversampling factor. This has the effect of extending the low
frequency end of the power spectrum. A value of 4 or larger is
typical.

\subsubsection{HIFAC}
This factor extends the spectrum to higher frequencies. By default
the spectrum is calculated out to the Nyquist frequency. If HIFAC
is set to 2 then the spectrum is extrapolated out to twice the
Nyquist frequency.

\subsubsection{WFREQ}
The hidden default is 0.0. In later invocations of the program,
it will be possible to produce the window function of the data at
the input frequency, WFREQ, at present setting WFREQ will have
no effect.

\subsection{Example}
To produce a power spectrum of a datafile and extend the low frequencies
by a factor of four and the high freqs. to twice Nyquist.
\begin{quote}\begin{verbatim}
ICL> LOMBSCAR
LOMBSCAR Version 1.5-1
INPUT - Input datafile > LTIME
*Variance not present in input datafile*
Using 12 of the 16 input data values
OFAC - Oversampling factor > 4
HIFAC - High frequency factor > 2
Peak found at 0.23 of amplitude 3.224 and error 0.623
Graphics device type (? to see list): VWS
OUTPUT - Output datafile > POW
\end{verbatim}\end{quote}
\section{PHASE}
This program will convert a time series into a "phase series"
given the ephemeris of the periodicity. Note, that when plotted
this will produce a linear phase axis, and not a time series
labeled with phase values.

The user enters the coefficients of the ephemeris. These are:


\begin{description}
\item[COEFF1]
Julian date of an occurrence of phase zero (days)
\item[COEFF2]
The periodicity at the time of COEFF1 (days)
\item[COEFF3]
54$dP/dt$, unitless first derivative of period wrt time
\end{description}
\subsection{Input Output Data}
Input: ASTERIX binned dataset, with a time axis.
Output: ASTERIX binned dataset, with the time axis
replaced by a phase axis.
\subsection{Julian Date}
The Julian date can be found for a specified calendar date from the
Astronomical Almanac. Modified Julian date (MJD) is defined as
\begin{quote}\begin{verbatim}
MJD = Julian date - 2,400,000.5
\end{verbatim}\end{quote}
The MJD at the start of observation can be found using the HEADER
application for any dataset.

The MJD of 1st January 1972 is 41317.0

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Input dataset
    - GLOBAL.BINDS (the current binned dataset)
 OUT       2    CHARACTER   Output dataset
    -> GLOBAL.BINDS (the current binned dataset)
 COEFF1    3    DOUBLE      Ephemeris coefficient 1
 COEFF2    4    DOUBLE      Ephemeris coefficient 2
 COEFF3    5    DOUBLE      Ephemeris coefficient 3

\end{verbatim}\subsection{Example}
\begin{quote}\begin{verbatim}
ICL> phase
PHASE Version 1.0-0
Enter name of input file > time_series
Enter name of output file > phase_series
Enter Ephemeris Coeffs : a(1) + a(2)*N + a(3)*N*N
Enter first Ephemeris coefficient > 2445296.3065
Enter second Ephemeris coefficient > 1.98315
Enter third Ephemeris coefficient > -1.35E-8
\end{verbatim}\end{quote}
\section{POWER}
Computes the power spectrum of a 1D data set using an FFT for
speed. Data are assumed to be regularly spaced, and no weighting
of data is performed (use SINFIT instead if weights are very
important). The data set can be of any length, but the programme
will run most quickly if there are 2**n points. If the number
contains a large prime factor then it can be very slow, the
TRUNCATE option allows truncation (with loss of no more than 10\%
of the data) to a less awkward number. All data are assumed to be
of good quality.

Normalization of power spectrum is:-
power = (wave amplitude / 2)**2
The data can be tapered to eliminate spurious power arising from
a jump between the first and last data values (since the data
are effectively assumed to be periodic). If this option is
selected (answer Y) then a cosine bell is applied to 10\% of the
data at each end, after removing the data mean.
The data mean contributes only to the zero frequency power, but
it may be desirable to remove it to avoid it dominating an output
plot.
A data object containing the power at each frequency is created.
\subsection{Deficiencies}
The data are assumed to be regularly spaced. Data errors and data
quality are ignored.
\subsection{Input Output Data}
Input: 1D dataset
Output: 1D dataset (power spectrum)
\subsection{Method}
The basic technique is standard. The data must not be modified by
the programme, so a temporary data object is used for scratch
storage if mean removal or tapering is requested. Arrays are
mapped so there is no size limitation.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type        Description
 -------   ----  ----        -----------
 INP         1   CHARACTER   Input dataset
 OUT         2   CHARACTER   Output object.
    -> GLOBAL.BINDS (the current binned dataset)
 TRUNCATE    -   LOGICAL     Truncate data?
 TAPER       -   LOGICAL     Taper data?
 REMOVE_MEAN -   LOGICAL     Remove data mean?

\end{verbatim}\subsection{Example}
\begin{quote}\begin{verbatim}
  $POWER                       ! Invocation of command
    POWER Version 1.0-1
   Enter name of input dataset > TIME_SERIES
   Data object is of type TIME_SERIES
   82 data points entered
   Do you wish to truncate the data? (Y/N) > y

   First and last data values are : 4.050  2.576
   Do you wish to taper the data? (Y/N) > y
   Data mean 1.94365 subtracted
   Enter name of output dataset > POWER_SPECTRUM
\end{verbatim}\end{quote}
\section{SINFIT}
Computes an approximate periodogram of 1D data which may be
irregularly spaced, by weighted least squares fitting of sine
waves. Bad quality data are omitted from the calculation. The
program calculates:-
fit(frequency) = amplitude * SIN(frequency*angle + phase)
- mean value of data
The normalization used for the power is:-
power = (amplitude / 2)**2
Since the fast Fourier transform is not used, the program can be
slow for large data sets, and so should be run in batch.
A data object containing the power at each frequency is created.
NOTE: if SINFIT is run on regularly spaced data, it
will not give a sensible result at the Nyquist
frequency (= 0.5 * sampling frequency) since the
fitted sine wave amplitude is indeterminate.
\subsection{Deficiencies}
Slow compared to FFT methods so use latter (POWER) for regularly
spaced data, unless variable data weights are important.

\subsection{Input Output Data}
Input: 1D dataset - Either a structure containing 1D DATA\_ARRAY,
or ANY 1D primitive.
Output: 1D dataset
\subsection{Method}
See {\em MNRAS 196, p583.}
\subsection{Phase Information}
Phase information may also be stored in the output dataset. This
is written as as NDF inside the MORE.ASTERIX.PHASE\_SPECTRUM
component of the output dataset.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Name of input object
 OUT       2    CHARACTER   Name of output object.
    -> GLOBAL.BINDS (the current binned dataset)
 BASE      3    REAL        Base frequency
 INC       4    REAL        Frequency increment
 NUM       5    INTEGER     Number of frequencies.
 PHASE     -    LOGICAL     Store phase information?
    Hidden default value = NO

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> sinfit
SINFIT Version 1.0-0
Enter filename of input dataset > time_series
82 data points entered
Base frequency > 10
Frequency increment > 100
Number of frequencies to be fitted to the data > 1000
Data mean 3.345787 has been subtracted
Enter name of output dataset >
\end{verbatim}\end{quote}
\section{STREAMLINE}
Reduces the size of a one-dimensional datafile by removing
bad quality data points from the file. This is especially useful
for time series produced by low earth orbit satellites. Most
of the time series applications will work sensibly on datafiles with
bad quality pixels, however, reducing the filesize using this
program will increase the speed of each application.

\subsection{Input Output Data}
Input: 1 dimensional dataset with some bad quality pixels
Output: 1 dimensional dataset
\subsection{Method}
The quality of each datapoint is checked and those with GOOD
quality are stored in the second file along with their axis
value. The axis in the output file consists of an array of numbers.
Variances are also stored in the output file if present in the
input.

The output file does not contain a QUALITY array since by definition
all pixels have good quality.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP       1    CHARACTER   Input dataset
   - GLOBAL.BINDS the current binned dataset
 OUT       2    CHARACTER   Output dataset
   -> GLOBAL.BINDS the current binned dataset

\end{verbatim}\subsection{Examples}
To reduce the size of a time series with 50 percent bad data.
\begin{quote}\begin{verbatim}
ICL> streamline inp=big_file out=small_file
Using 40000 of the 80000 data values
\end{verbatim}\end{quote}
\section{TIMSIM}
TIMSIM creates an artificial time-series in which the time axis is
either taken from another data file or created by the user. The data
is created by choosing components which are added together and then,
if required, scattered using Poisson statistics. The options available
are, constant level, ramp, sine(s), square wave(s), saw tooth wave(s),
flare(s) and Poisson scatter.
\subsection{Input Output Data}
Input: Possibly axis 1 data from an NDF
Output: Asterix 1 dimensional NDF
\subsection{Method}
The program starts by establishing the time axis of the output
file, for which it prompts for the name. This time axis can
either be a time axis taken from another standard ASTERIX file
(it must be axis number 1) or by stating the duration,initial
time and bin width and creating a regular axis. The program
then creates and maps the data and variance components and
prompts the user for which of the options displayed on the
option menu are to be used. Each required subroutine is in
turn called and the particular components added to the data.
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 OUT       1    _UNIV       Output file name
 AXIS      2    _LOGICAL    Is the time axis to be from another file
 INP            _UNIV       Input file name
 DUR            _REAL       Observation length
 WID            _REAL       Width of bins
 INIT           _REAL       Initial time value
 OPT            _CHAR       List of required options
 LEV1           _REAL       Background level
 RAM1           _REAL       Initial level for ramp
 RAM2           _REAL       Final level for ramp
 NUMSIN         _INTEGER    Number of sine waves
 PER1           _REAL       Period of sine wave
 PHA1           _REAL       Initial phase of sine wave
 AMP1           _REAL       Amplitude of sine wave
 NUMSQU         _INTEGER    Number of square waves
 PER2           _REAL       Period of square wave
 PHA2           _REAL       Initial phase of square wave
 AMP2           _REAL       Amplitude of square wave
 NUMSAW         _INTEGER    Number of saw tooth waves
 PER3           _REAL       Period of saw tooth wave
 PHA3           _REAL       Initial phase of saw tooth wave
 AMP3           _REAL       Amplitude of saw tooth wave
 NUMFLA         _INTEGER    Number of flares
 RIS            _REAL       Rise time of flares
 DEC            _REAL       Decay time of flare
 AMP4           _REAL       Amplitude of flare
 TIM            _REAL       Time of flare
 DLAB           _CHARACTER  Data label
 DUNI           _CHARACTER  Data units
 AXLAB          _CHARACTER  Axis label
 AXUNI          _CHARACTER  Axis units
 BSF            _INTEGER    Bin splitting factor

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
   >>TIMSIM
      OUTPUT FILE - Enter name of output file /@EXAMPLE/ >
      AXIS - Do you wish to use a time axis from another file? /FALSE/ >
      DURATION - Enter duration of time series in seconds /1000/ >
      BIN WIDTH - Enter required bin width /10/ >
      INITIAL - Enter time of centre of first bin /0/ >
       1) Background
       2) Ramp
       3) Sine(s)
       4) Square Wave(s)
       5) Saw Tooth Wave(s)
       6) Flare(s)
       7) Poisson Scatter
      OPTION - Enter options separated by spaces > 1 3 6 7
      CONSTANT LEVEL - Enter level of background in count/s /10/ >
      SINE(S) - Enter number of sine waves required /1/ >
      PERIOD - Enter period of sine wave in secs /100/ >
      PHASE - Enter phase at initial time value /0/ >
      AMPLITUDE - Enter amplitude of sine wave /10/ >
      FLARE(S) - Enter number of flares required /1/ >
      RISE TIME - Enter rise time in secs > 30
      DECAY-TIME - Enter decay-time in secs > 50
      AMPLITUDE - Enter amplitude of flare > 10
      TIME - Enter time of maximum of flare in secs > 200
\end{verbatim}\end{quote}
\section{TONOFF}
This command is actually part of the time series editing system,
but may be run standalone. It inspects the QUALITY array on the
time series and writes a file of on-off times for periods of
good QUALITY. The times are in MJD and are suitable to be read
by sorting programs such as XSORT.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type  Description
 -------  ----  ----  -----------
   INP     1    CHAR  Input dataset
   FILE    2    CHAR  Text file of on-off times

\end{verbatim}\section{VARTEST}
Vartest tests a 1-D timeseries for variability. Using a maximum
likelihood method to find the source count rate, it takes as an
input the source box time series and background box time series,
corrected as if they were at the centre of the field of view. It
calculates the probability that the least likely background
subtracted point, occured due to chance, assuming a Poissonian
distribution about the calculated count rate. Hence the
probability statistic output is approximately the probability
that the time series is from a constant source.

N.B. The output statistic is not strictly a probability
and be more than one in some cases of very low
variability.

\subsection{Method}
The program tries several different values of the source count rate
ranging from the maximum value of the background subtacted count rate
to the minimum. For each of these it finds the most significant
background subtracted point and calculates the probability for it.
This is done by summing the probabilities over values of the number
of background counts, such that the the number of source counts and
background counts give the required background subtracted count rate.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Type        Description
 -------  ----        -----------
 INP      UNIV        Input source time series.
 BACK     UNIV        Input background time series.
 AREA     REAL        Ratio of source box area to background
                      box area.
 PFUNC    REAL        Probability statistic.
\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> VARTEST                        ! Invocation of command
VARTEST Version 1.1-0
INP - Source time series > SOURCE
BACK - Background time series > BACK
AREA - Ratio of source to background areas /1/ > 16
Probability statistic is 0.1
\end{verbatim}\end{quote}
The statistic implies 90\% confidence that the source is variable.
\chapter{Editing time series}
These commands allow time series to be displayed and edited interactively.
The time series is displayed on a suitable graphic device and sections
can be selected for discarding, the remaining data points being saved
to a new dataset. Where the data is fragmented the sections with data
missing (indicated by QUALITY value) can be ommitted from the plotting
and the remaining sections with data can be selected individually.

Example:
\begin{quote}\begin{verbatim}
 TLOAD dataset XWINDOW           - load dataset and open XWINDOW
 TPLOT                           - display all data
 TCHOP                           - display only segments with data
 TSELECT                         - select segment for display
 TSLICE                          - remove part
 TSAVE new data                  - save edited data
\end{verbatim}\end{quote}
Some plotting attributes are set internally by the applications
(principally the positions of the different segments of the plot),
but GSET may also be used in conjunction with these commands.

\section{TCHOP}
Chops up the time series into segments of contiguous data and
redisplays. Missing data between segments is indicated by
QUALITY=MISSING. All segments are plotted with the same scaling
in x and y.

\section{TCLOSE}
Closes down the system by releasing the dataset and device.
If no other sub-system is using the graphics device this will
also be closed.

\section{TONOFF}
Writes a file of on-off times for periods with good QUALITY.
The times are in MJD and are in a format suitable to be read
by sorting programs such as XSORT. This command can also
be run standalone, in which case an input file will be
prompted for.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type  Description
 -------  ----  ----  -----------
   INP     1    CHAR  Input dataset (standalone mode)
   FILE    2    CHAR  Text file to write on-off times to

\end{verbatim}\section{TPLOT}
Plots the data in a form dependent on current selections.
Most commands that change these selections cause automatic
plotting.

\section{TSAVE}
Save edited data to a new dataset. The data that are saved
and the form of the dataset they are saved in will depend on
current selections:

\begin{itemize}
\item Not chopped. single dataset including gaps
\item Chopped with 1 segment selected. Single dataset of just that segment
\item Chopped with more than 1 segment selected. Multiple dataset of
selected segments
\end{itemize}
In the last case information is written into the Graphics
Control Block such that GDRAW will plot them in the same
way (nearly).

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type  Description
 -------  ----  ----  -----------
   OUT     1    CHAR  Output dataset

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
ICL> TSAVE newdataset
\end{verbatim}\end{quote}
\section{TSELECT}
Selects one or more segments for display. To select a segment
the cursor is moved onto the segment and the left-hand mouse
button or any keyboard button (other than \verb+<RETURN>+) pressed.
The right-hand mouse button or \verb+<RETURN>+ will terminate the
selection. To select all segments including those not currently
displayed use the ALL option on the command line.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type  Description
 -------  ----  ----  -----------
  ALL      -    LOG   Select all segments

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> TSELECT ALL       - select all segments
\end{verbatim}\end{quote}
\section{TSLICE}
Define a slice of the displayed data using the cursor. By
default this section will be removed (by setting QUALITY to IGNORE),
but by using the KEEP option with the command it may be kept and
the data either side discarded.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type  Description
 -------  ----  ----  -----------
  KEEP      -   LOG   keep selected range and discard rest
     (Hidden default = N)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> TSLICE KEEP
\end{verbatim}\end{quote}
\section{TLOAD}
This starts up the interactive system by loading the time series.
If none is already open then the given plotting device will be opened
with one or more plotting zones.

\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type  Description
 -------  ----  ----  -----------
   INP     1    CHAR  Time series dataset
   DEV     2    CHAR  Graphic device
   NX      3    INT   Plotting zones in x-dir
       (Hidden default = 1)
   NY      4    INT   Plotting zones in y-dir
       (Hidden default = 1)

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> TLOAD dataset                      - load dataset

> TSTART dataset XWINDOW 1 2         - same but opening XWINDOW
                                       with two plotting zones
\end{verbatim}\end{quote}
\section{TWHOLE}
Selects the whole dataset for display, ie. plot one continous
time series including the empty sections. This command reverses
the effect of TCHOP.

\section{TZAP}
Removes individual points from displayed time series by setting
quality to bad.

\chapter{Mathematical commands}
These are applications which perform mathematical operations. on
various types of input.

\section{ARITHMETIC}
ARITHMETIC performs a specified arithmetic operation (+,-,*,/) on
two 'data objects'. These 'data objects' may be either binned
datasets, primitive HDS objects, or scalar values typed in
response to the prompts.

The result is stored in a binned dataset, which may overwrite
the first input dataset (if it is indeed a dataset) if the OVER
keyword is added to the command line.

Errors and quality in the input objects are taken into account
(see error\_functions). If the input has no errors but you want to
add some enter the keyword ERRORS on the command lines. This will
cause errors to be prompted for. If errors are prompted for but
no errors are required then respond with a '!'.

\subsection{Error functions}
For the operation
\begin{verbatim}
        C +/- CE = A +/- AE o B +/- BE
\end{verbatim}
the error functions used are:-
\begin{verbatim}
       Operator       Error Function
       --------    ----------------------
       + or - :    CE=SQRT(AE**2+BE**2)
            * :    CE=SQRT((B*AE)**2+(A*BE)**2)
            / :    CE=SQRT((AE/B)**2+(A*BE/B**2)**2)
\end{verbatim}
\subsection{Floating Point Divide by 0}
Where division by zero is detected, the result is set to 0.0
and the corresponding quality byte to 00000100.

\subsection{Input output data}
Input1: Binned dataset, primitive data object.

Input2: Binned dataset, primitive data object
or scalar number.

Output: modified input or binned dataset

\subsection{Shortforms}
\begin{verbatim}
         ADD        invokes in + mode
         SUBTRACT   invokes in - mode
         MULTIPLY   invokes in * mode
         DIVIDE     invokes in / mode
\end{verbatim}
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------
 INP1      1    CHARACTER   First input data object.
 INP2      2    CHARACTER   Second input data object.
 OUT       3    CHARACTER   Output data object.
   -> GLOBAL.BINDS (the current binned dataset)
 OPER      4    CHARACTER   Type of operation (+,-,*,/).
 ERROR1    5    CHARACTER   Error estimate for first primitive
                            data object.
 ERROR2    6    CHARACTER   Error estimate for second primitive
                            data object.
 OVER      -    LOGICAL     If set then the INPUT1 dataset
                            becomes the output dataset too.
 ERRORS    -    LOGICAL     If set then the user is prompted for
                            input dataset errors if these are not
                            yet present.

\end{verbatim}\subsection{Examples}
\begin{quote}\begin{verbatim}
> ARITHMETIC
  First input data object > IMAGE_DATA
  Second input data object > 10
  Output dataset > PRODUCT
  Operator (+,-,*,/) > *
  Copying ancillary information

> ARITHMETIC
  First input data object > IMAGE_DATA
  Second input data object > IMAGE_DATA
  Output dataset > QUOTIENT
  Operator (+,-,*,/) > /
  Copying ancillary data

> ARITHMETIC ERRORS
  First input data object > 1,2,3,4,5
  Second input data object > 10,20,30,40,50
  Output dataset > PRODUCT
  Existing data object will be overwritten
  Operator (+,-,*,/) > *
  Error value(s) for first object > !
  Error value(s) for second object > !
\end{verbatim}\end{quote}
\section{ADD+}
Invokes ARITHMETIC in add mode
\section{SUBTRACT+}
Invokes ARITHMETIC in subtract mode
\section{MULTIPLY+}
Invokes ARITHMETIC in multiply mode
\section{DIVIDE+}
Invokes ARITHMETIC in divide mode
\section{OPERATE}
OPERATE operates (eg. with LOG10) on either binned datasets or
primitive or structured input/output, including scalar numbers.

Errors (if present) are amended using the local linear
approximation:

\begin{quote}\begin{verbatim}
          sigma[f(x)] = sigma[x] * df/dx .
\end{verbatim}\end{quote}
The input object may be overwritten (using the OVER parameter) or
a new output object created.

If input is primitive user will be asked to give error estimate.
If none desired, type "!" and not \verb+<RETURN>+ which would give
error of 0.0 If data is negative and LOG10 or LOG is used, then
data is set to 1.E-38 and error such that when added to data it
will have value plus 1 sigma. If data is negative and SQRT is
used, then data is set to zero and error modified.

If EXP or 10** are used and data is such that Arithmetic Overflow
would occur, data values and errors are replaced to indicate that
this is the case. The data label (if present) is amended to
reflect the operation performed, and all other components are
unchanged.

\subsection{Operations Available}
Currently supported operations are:
\begin{verbatim}
        -         Change sign of data values
        LOG       Take natural logarithm of data values
        LOG10     Take base 10 logarithm of data values
        EXP       Exponentiate data values
        10**      10 to the power of data values
        SQRT      Square root of data values
\end{verbatim}
\subsection{Parameters}
\begin{verbatim}
 Keyword  Posn  Type        Description
 -------  ----  ----        -----------

 INP        1   CHARACTER   Input data object.
   - GLOBAL.BINDS (the current binned dataset)
 OUT        2   CHARACTER   Output data object
   -> GLOBAL.BINDS (the current binned dataset)
 OPER       3   CHARACTER   Operation to be performed
 ERROR      -   CHARACTER   Error for input data object
 OVER       -   LOGICAL     If set then the INPUT1 dataset
                            becomes the output dataset too.

\end{verbatim}\chapter{Menu Commands}
The menu system in Asterix allows a user to fire off a task by "pressing"
a "button" on the screen with the cursor. A menu consists of a set of
up to 10 buttons, and may be set up interactively using the DEFMENU
command or by calling up a pre-defined menu file with the command, LOADMENU.
Up to 5 menus may be defined simultaneously. A previously defined menu
may be activated by the command "RUNMENU $n$", where
$n$ is the menu number.

To get a feel for how the menu system works, start up the image processing
system on an interactive graphics device and try out the default image
processing menu, using the following sequence.

\begin{quote}\begin{verbatim}
ICL> IDISPLAY image_file
ICL> LOADMENU $AST_DAT_MENU/img.mnu
ICL> RUNMENU 1
\end{verbatim}\end{quote}
Menu files may be written by the user, see the help entry on
2\_File\_Format.

A single button may be defined to do several tasks, see the help entry
on 1\_Buttons.

\section{Buttons}
A button may be defined either when setting up a menu, with DEFMENU or
LOADMENU, or separately using the BUTSET command.

A button consists of a name, of up to 8 characters, and a command which
may contain up to 80 characters. The command string may contain several
operations which will be executed in sequence. Multi-operations should be
separated by the pipe "|" character.

e.g. a button set to the command string:
\begin{quote}\begin{verbatim}
ICENTROID|IMODE KEY|ICIRCLE RAD=0.1 \|ISTATS|IWHOLE|IMODE CUR
\end{verbatim}\end{quote}
Calculates the counts within a circle, of radius 0.1 axis units, about the
centre of a source selected by the user. This system makes the writing of
trivial ICL procedures unnecessary.

\section{File format}
A menu file is a simple text file which may describe up to 5 menus.
The first word in any line of the file should be one of the two keywords -
MENU or BUTTON. Alternatively the semi-colon (;) character may be used,
as the FIRST character in the line, to denote a comment line. Any other
first word will produce a "Keyword not understood" message.

The MENU keyword simply means start another menu and the BUTTON keyword
tells the system that a button within the current menu is being defined.
e.g. the following

\begin{quote}\begin{verbatim}
;Image proc menu
  MENU
;
    BUTTON SLICE ISLICE
    BUTTON STATS ICENTROID|IMODE KEY|ICIRCLE RAD=0.1|ISTATS|IMODE CUR
    BUTTON RADIAL IRADIAL
    BUTTON EXTRAS USEMENU 2
    BUTTON QUIT QUIT

;
; Second menu of additional options
   MENU
;
    BUTTON "SET DEF" DEFAULT DISK$SCRATCH:[RDS]
    BUTTON SPOKES XSPOKES
    BUTTON QUIT USEMENU 1
\end{verbatim}\end{quote}
The above is a simple two menu file which defines a number of image
processing operations in the first menu and some extra functions in a
second menu. The second menu may be called from the first by pressing
the "EXTRAS" button.

Note the use of the pipe character (|) in the STATS command to
allow several operations to be performed sequentially. The QUIT
button in the first menu, has been set up to issue the QUIT command.
This is a special command which is recognised by the RUNMENU procedure as
a means of exiting the menu system. A top level menu should include one
button which executes this command so that a neat exit from the menu
system can be achieved (otherwise CNTL C works).

The format for the BUTTON command is 'BUTTON name command'. If a button
name is required with an embedded space then the name should be enclosed
in quotes, e.g. the 'SET DEF' command.

A menu file may be loaded with the LOADMENU command. It is then executed
by typing "RUNMENU 1".

Example menus exist in the \$AST\_DAT\_MENU directory.

\section{Method}
The menu system works by storing the commands associated with each
button as ADAM global parameters. When a button is pressed the command
associated with it is written into a temporary file IMENUCOM.ICL .
This is then loaded and executed.

\section{BUTSET}
Sets a button in a pre-defined menu. This can be used to define an
already existing button or to extend the number of buttons in a menu.
e.g. if a menu exists with three buttons and you attempt to set the fifth
button then the fourth will be created as well but will be blank. This
is one method of spacing the buttons out.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 MENUNUM    1    INTEGER    Number of the menu containing the button
 BUTNUM     2    INTEGER    The button number
 BUTNAM     3    CHARACTER  The name for the button
 BUTCOM     4    CHARACTER  The button command
 MORE            LOGICAL    Modify another button ?

\end{verbatim}\section{CHKMENU}
This is the command which polls the menu and checks if a button has
been pressed. It shouldn't be called directly by the user, but is used
within the RUNMENU procedure. User written ICL procedures which use
the menu system may need to use this command.

When a button is pressed, CHKMENU gets the command associated with it
from a GLOBAL parameter and writes it into a file
SYS\$SCRATCH:IMENUCOM.ICL.

\section{DEFMENU}
This is the common way to set up a menu. Button names and commands
are prompted for.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 NMENU      1    INTEGER    Number of menus wanted
 MENU1_NBUT 2    INTEGER    Number of buttons in the first menu
 MENU2_NBUT      INTEGER    Number of buttons in the second menu
    .
 MENU5_NBUT      INTEGER    Number of buttons in the fifth menu
 BUTNAM1         CHARACTER  Name of first button
    .
    .
 BUTNAM10        CHARACTER  Name of tenth button
 BUTCOM1         CHARACTER  Command for first button
    .
    .
 BUTCOM10        CHARACTER  Command for tenth button

\end{verbatim}\subsection{Examples}
To set up a single menu with three buttons
\begin{quote}\begin{verbatim}
ICL> DEFMENU 1
Number of buttons in menu 1 > 3
Name for button > SLICE
Command for this button > ISLICE
Name for button > STATS
Command for this button > ICIRCLE|ISTATS|IWHOLE
Name for button > QUIT
Command for this button > QUIT
\end{verbatim}\end{quote}
\section{LOADMENU}
Loads a set of menus from a menu file (see FILE\_FORMAT for details
of the format). The number of menus which may be active at any one
time is limited to five, however, a button may be set to load a
further set of menus and activate them. So, using this technique,
the number of menus which may be interlinked becomes unlimited.

Standard Asterix menus are kept in the \$AST\_DAT\_MENU directory.
Take a copy of the {\tt img.mnu} file and edit it to your tastes.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 MFILENAME  1    INTEGER    Name of menu file

\end{verbatim}\section{RUNMENU*}
This is the ICL procedure which actually activates the menu. The
sequence of commands would typically be:
Set up a menu with DEFMENU or LOADMENU and then type "RUNMENU $n$"
where "$n$" is the number of the menu which you wish to use.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 MENUNUM    1    INTEGER    Menu to use

\end{verbatim}\section{USEMENU}
This sets the current menu number and displays the menu on the current
graphics device. This is of little use interactively. However a button
may be set to call a menu by setting its command to "USEMENU n". In this
way menus can be interlinked.

\subsection{Parameters}
\begin{verbatim}
 Keyword   Posn  Type       Description
 -------   ----  ----       -----------
 MENUNUM    1    INTEGER    Menu to use

\end{verbatim}\chapter{UNIX Differences}
Although the UNIX and VMS implementations of ASTERIX are similar,
there are differences, and these are described in the various
subtopics below.

\section{Running Applications}
To start you must issue the usual ASTSTART command, but note that
it must be in lowercase.

\begin{quote}\begin{verbatim}
csh> aststart
\end{verbatim}\end{quote}
The program development system can be made available using
\begin{quote}\begin{verbatim}
csh> astdev
\end{verbatim}\end{quote}
You can of course put these two commands in your login procedure
.login in your home directory. If you want to run ASTERIX in
script or batch mode under UNIX, you should also put these
commands in your script files.

All commands must be given in full and in lowercase from the shell.
Commands issued from ICL may be abbreviated as on VMS.

Since commands are invoked from the shell some command syntax is
interpreted as shell metacharacters. These include parenthesis,
bracket, quote, double quote and backslash. You can usually get
around this by including the offending string in single quotes, eg.

\begin{quote}\begin{verbatim}
csh> statistix 'file.data_array(1:100,2:20)'
\end{verbatim}\end{quote}
When responding to prompts neither the \verb+<TAB>+ function for editing
the default nor command-line recall is yet available.

Parameter values are case insensitive except for graphics devices
and filenames. No file extension is assumed for ASCII files. (On
VMS ".DAT" is assumed if no extension is given.)

\section{HDS Files}
Although HDS files are portable you are recommended to copy them to
the host machine, and run the KAPPA application 'native' on them for
significant efficiency gains.

\section{Missing Applications}
The following applications are not available on UNIX:
\begin{verbatim}
   Application       Reason
   ~~~~~~~~~~~       ~~~~~~
   EXOCORR           Raw data files are not portable
   SCANEXO            "
   PACKEXO            "
   SHOWEXO            "
   EXOLESORT          "
   EXOLESUB           "
   EXOLEPH            "
   EXOMESORT          "
   EXORESP            "
   EXOSUBPH           "
\end{verbatim}
\section{Name Clashes}
Some ASTERIX applications clash with UNIX commands. These are
restore and history. The former can be invoked using,

\begin{quote}\begin{verbatim}
csh> quality restore over
\end{verbatim}\end{quote}
and the latter application has had its name shortened to simply
'hist'.

\end{document}
